{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.255863539445629,
  "eval_steps": 500,
  "global_step": 2700,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0004047149289219406,
      "grad_norm": 30.525068283081055,
      "learning_rate": 0.0003,
      "loss": 7.8136,
      "step": 1
    },
    {
      "epoch": 0.0008094298578438812,
      "grad_norm": 14.420040130615234,
      "learning_rate": 0.0002999392958316471,
      "loss": 6.7409,
      "step": 2
    },
    {
      "epoch": 0.0012141447867658219,
      "grad_norm": 11.38471794128418,
      "learning_rate": 0.0002998785916632942,
      "loss": 5.4653,
      "step": 3
    },
    {
      "epoch": 0.0016188597156877624,
      "grad_norm": 14.392556190490723,
      "learning_rate": 0.00029981788749494127,
      "loss": 4.34,
      "step": 4
    },
    {
      "epoch": 0.002023574644609703,
      "grad_norm": 7.117629051208496,
      "learning_rate": 0.0002997571833265884,
      "loss": 3.7756,
      "step": 5
    },
    {
      "epoch": 0.0024282895735316438,
      "grad_norm": 6.1199259757995605,
      "learning_rate": 0.0002996964791582355,
      "loss": 3.4608,
      "step": 6
    },
    {
      "epoch": 0.0028330045024535845,
      "grad_norm": 6.467629432678223,
      "learning_rate": 0.0002996357749898826,
      "loss": 3.3161,
      "step": 7
    },
    {
      "epoch": 0.0032377194313755247,
      "grad_norm": 7.567018508911133,
      "learning_rate": 0.00029957507082152974,
      "loss": 2.9592,
      "step": 8
    },
    {
      "epoch": 0.0036424343602974654,
      "grad_norm": 9.55405330657959,
      "learning_rate": 0.00029951436665317686,
      "loss": 2.44,
      "step": 9
    },
    {
      "epoch": 0.004047149289219406,
      "grad_norm": 10.994133949279785,
      "learning_rate": 0.0002994536624848239,
      "loss": 2.9392,
      "step": 10
    },
    {
      "epoch": 0.004451864218141347,
      "grad_norm": 8.373197555541992,
      "learning_rate": 0.00029939295831647104,
      "loss": 2.2308,
      "step": 11
    },
    {
      "epoch": 0.0048565791470632875,
      "grad_norm": 7.289917469024658,
      "learning_rate": 0.00029933225414811816,
      "loss": 2.3583,
      "step": 12
    },
    {
      "epoch": 0.005261294075985228,
      "grad_norm": 7.796220302581787,
      "learning_rate": 0.0002992715499797653,
      "loss": 2.835,
      "step": 13
    },
    {
      "epoch": 0.005666009004907169,
      "grad_norm": 5.056464672088623,
      "learning_rate": 0.0002992108458114124,
      "loss": 2.072,
      "step": 14
    },
    {
      "epoch": 0.006070723933829109,
      "grad_norm": 5.2291059494018555,
      "learning_rate": 0.00029915014164305946,
      "loss": 2.1192,
      "step": 15
    },
    {
      "epoch": 0.0064754388627510494,
      "grad_norm": 4.9805192947387695,
      "learning_rate": 0.0002990894374747066,
      "loss": 1.7761,
      "step": 16
    },
    {
      "epoch": 0.00688015379167299,
      "grad_norm": 5.765310764312744,
      "learning_rate": 0.0002990287333063537,
      "loss": 1.7041,
      "step": 17
    },
    {
      "epoch": 0.007284868720594931,
      "grad_norm": 8.130989074707031,
      "learning_rate": 0.0002989680291380008,
      "loss": 1.4854,
      "step": 18
    },
    {
      "epoch": 0.0076895836495168715,
      "grad_norm": 6.6567583084106445,
      "learning_rate": 0.0002989073249696479,
      "loss": 1.7721,
      "step": 19
    },
    {
      "epoch": 0.008094298578438812,
      "grad_norm": 5.525673866271973,
      "learning_rate": 0.000298846620801295,
      "loss": 0.9901,
      "step": 20
    },
    {
      "epoch": 0.008499013507360752,
      "grad_norm": 6.780470848083496,
      "learning_rate": 0.0002987859166329421,
      "loss": 1.1232,
      "step": 21
    },
    {
      "epoch": 0.008903728436282694,
      "grad_norm": 6.268028736114502,
      "learning_rate": 0.0002987252124645892,
      "loss": 1.2482,
      "step": 22
    },
    {
      "epoch": 0.009308443365204633,
      "grad_norm": 6.701329708099365,
      "learning_rate": 0.0002986645082962363,
      "loss": 1.2162,
      "step": 23
    },
    {
      "epoch": 0.009713158294126575,
      "grad_norm": 8.657279968261719,
      "learning_rate": 0.0002986038041278834,
      "loss": 1.0709,
      "step": 24
    },
    {
      "epoch": 0.010117873223048515,
      "grad_norm": 6.080664157867432,
      "learning_rate": 0.0002985430999595305,
      "loss": 1.0031,
      "step": 25
    },
    {
      "epoch": 0.010522588151970456,
      "grad_norm": 5.30901575088501,
      "learning_rate": 0.00029848239579117764,
      "loss": 0.7886,
      "step": 26
    },
    {
      "epoch": 0.010927303080892396,
      "grad_norm": 5.92638635635376,
      "learning_rate": 0.0002984216916228247,
      "loss": 1.0719,
      "step": 27
    },
    {
      "epoch": 0.011332018009814338,
      "grad_norm": 4.682679653167725,
      "learning_rate": 0.0002983609874544718,
      "loss": 1.0106,
      "step": 28
    },
    {
      "epoch": 0.011736732938736278,
      "grad_norm": 4.426610946655273,
      "learning_rate": 0.00029830028328611894,
      "loss": 0.6834,
      "step": 29
    },
    {
      "epoch": 0.012141447867658217,
      "grad_norm": 6.17793083190918,
      "learning_rate": 0.00029823957911776606,
      "loss": 0.866,
      "step": 30
    },
    {
      "epoch": 0.012546162796580159,
      "grad_norm": 4.089677810668945,
      "learning_rate": 0.0002981788749494132,
      "loss": 0.7683,
      "step": 31
    },
    {
      "epoch": 0.012950877725502099,
      "grad_norm": 3.8830010890960693,
      "learning_rate": 0.00029811817078106024,
      "loss": 0.7544,
      "step": 32
    },
    {
      "epoch": 0.01335559265442404,
      "grad_norm": 5.4102349281311035,
      "learning_rate": 0.00029805746661270736,
      "loss": 0.6234,
      "step": 33
    },
    {
      "epoch": 0.01376030758334598,
      "grad_norm": 4.775615692138672,
      "learning_rate": 0.0002979967624443545,
      "loss": 0.7935,
      "step": 34
    },
    {
      "epoch": 0.014165022512267922,
      "grad_norm": 4.716794967651367,
      "learning_rate": 0.0002979360582760016,
      "loss": 0.408,
      "step": 35
    },
    {
      "epoch": 0.014569737441189862,
      "grad_norm": 4.206177711486816,
      "learning_rate": 0.0002978753541076487,
      "loss": 0.8608,
      "step": 36
    },
    {
      "epoch": 0.014974452370111803,
      "grad_norm": 3.9510111808776855,
      "learning_rate": 0.0002978146499392958,
      "loss": 0.6151,
      "step": 37
    },
    {
      "epoch": 0.015379167299033743,
      "grad_norm": 2.6683919429779053,
      "learning_rate": 0.0002977539457709429,
      "loss": 0.6568,
      "step": 38
    },
    {
      "epoch": 0.015783882227955685,
      "grad_norm": 4.212109565734863,
      "learning_rate": 0.00029769324160259,
      "loss": 0.5811,
      "step": 39
    },
    {
      "epoch": 0.016188597156877624,
      "grad_norm": 3.9082746505737305,
      "learning_rate": 0.00029763253743423713,
      "loss": 0.7018,
      "step": 40
    },
    {
      "epoch": 0.016593312085799564,
      "grad_norm": 5.583312511444092,
      "learning_rate": 0.00029757183326588424,
      "loss": 0.4952,
      "step": 41
    },
    {
      "epoch": 0.016998027014721504,
      "grad_norm": 3.9099504947662354,
      "learning_rate": 0.00029751112909753136,
      "loss": 0.5925,
      "step": 42
    },
    {
      "epoch": 0.017402741943643447,
      "grad_norm": 3.5821115970611572,
      "learning_rate": 0.0002974504249291784,
      "loss": 0.5396,
      "step": 43
    },
    {
      "epoch": 0.017807456872565387,
      "grad_norm": 5.847021579742432,
      "learning_rate": 0.00029738972076082554,
      "loss": 0.7917,
      "step": 44
    },
    {
      "epoch": 0.018212171801487327,
      "grad_norm": 4.8538079261779785,
      "learning_rate": 0.00029732901659247266,
      "loss": 0.8357,
      "step": 45
    },
    {
      "epoch": 0.018616886730409267,
      "grad_norm": 4.825687885284424,
      "learning_rate": 0.0002972683124241198,
      "loss": 0.533,
      "step": 46
    },
    {
      "epoch": 0.01902160165933121,
      "grad_norm": 3.206975221633911,
      "learning_rate": 0.0002972076082557669,
      "loss": 0.3943,
      "step": 47
    },
    {
      "epoch": 0.01942631658825315,
      "grad_norm": 4.978316307067871,
      "learning_rate": 0.00029714690408741396,
      "loss": 0.6462,
      "step": 48
    },
    {
      "epoch": 0.01983103151717509,
      "grad_norm": 4.49243688583374,
      "learning_rate": 0.0002970861999190611,
      "loss": 0.485,
      "step": 49
    },
    {
      "epoch": 0.02023574644609703,
      "grad_norm": 2.287867546081543,
      "learning_rate": 0.0002970254957507082,
      "loss": 0.4017,
      "step": 50
    },
    {
      "epoch": 0.02064046137501897,
      "grad_norm": 4.404902935028076,
      "learning_rate": 0.0002969647915823553,
      "loss": 0.3091,
      "step": 51
    },
    {
      "epoch": 0.021045176303940913,
      "grad_norm": 4.470913887023926,
      "learning_rate": 0.00029690408741400243,
      "loss": 0.5311,
      "step": 52
    },
    {
      "epoch": 0.021449891232862853,
      "grad_norm": 4.427117824554443,
      "learning_rate": 0.00029684338324564955,
      "loss": 0.4438,
      "step": 53
    },
    {
      "epoch": 0.021854606161784793,
      "grad_norm": 7.0150532722473145,
      "learning_rate": 0.0002967826790772966,
      "loss": 0.6954,
      "step": 54
    },
    {
      "epoch": 0.022259321090706732,
      "grad_norm": 4.513857841491699,
      "learning_rate": 0.00029672197490894373,
      "loss": 0.4479,
      "step": 55
    },
    {
      "epoch": 0.022664036019628676,
      "grad_norm": 5.234785556793213,
      "learning_rate": 0.00029666127074059085,
      "loss": 0.6148,
      "step": 56
    },
    {
      "epoch": 0.023068750948550616,
      "grad_norm": 4.430375576019287,
      "learning_rate": 0.00029660056657223796,
      "loss": 0.4178,
      "step": 57
    },
    {
      "epoch": 0.023473465877472555,
      "grad_norm": 5.720733642578125,
      "learning_rate": 0.00029653986240388503,
      "loss": 0.4895,
      "step": 58
    },
    {
      "epoch": 0.023878180806394495,
      "grad_norm": 4.497159004211426,
      "learning_rate": 0.00029647915823553215,
      "loss": 0.3981,
      "step": 59
    },
    {
      "epoch": 0.024282895735316435,
      "grad_norm": 3.485389232635498,
      "learning_rate": 0.00029641845406717926,
      "loss": 0.3581,
      "step": 60
    },
    {
      "epoch": 0.02468761066423838,
      "grad_norm": 3.7137811183929443,
      "learning_rate": 0.0002963577498988264,
      "loss": 0.334,
      "step": 61
    },
    {
      "epoch": 0.025092325593160318,
      "grad_norm": 4.494969367980957,
      "learning_rate": 0.00029629704573047344,
      "loss": 0.3206,
      "step": 62
    },
    {
      "epoch": 0.025497040522082258,
      "grad_norm": 4.5634942054748535,
      "learning_rate": 0.00029623634156212056,
      "loss": 0.3433,
      "step": 63
    },
    {
      "epoch": 0.025901755451004198,
      "grad_norm": 4.859524250030518,
      "learning_rate": 0.0002961756373937677,
      "loss": 0.4226,
      "step": 64
    },
    {
      "epoch": 0.02630647037992614,
      "grad_norm": 4.462199687957764,
      "learning_rate": 0.0002961149332254148,
      "loss": 0.4224,
      "step": 65
    },
    {
      "epoch": 0.02671118530884808,
      "grad_norm": 3.4427902698516846,
      "learning_rate": 0.00029605422905706186,
      "loss": 0.2948,
      "step": 66
    },
    {
      "epoch": 0.02711590023777002,
      "grad_norm": 3.7195658683776855,
      "learning_rate": 0.000295993524888709,
      "loss": 0.3149,
      "step": 67
    },
    {
      "epoch": 0.02752061516669196,
      "grad_norm": 3.595891237258911,
      "learning_rate": 0.0002959328207203561,
      "loss": 0.3725,
      "step": 68
    },
    {
      "epoch": 0.0279253300956139,
      "grad_norm": 4.916220188140869,
      "learning_rate": 0.0002958721165520032,
      "loss": 0.3589,
      "step": 69
    },
    {
      "epoch": 0.028330045024535844,
      "grad_norm": 3.549741506576538,
      "learning_rate": 0.00029581141238365033,
      "loss": 0.3814,
      "step": 70
    },
    {
      "epoch": 0.028734759953457784,
      "grad_norm": 3.136277437210083,
      "learning_rate": 0.0002957507082152974,
      "loss": 0.2819,
      "step": 71
    },
    {
      "epoch": 0.029139474882379723,
      "grad_norm": 3.005974531173706,
      "learning_rate": 0.0002956900040469445,
      "loss": 0.2549,
      "step": 72
    },
    {
      "epoch": 0.029544189811301663,
      "grad_norm": 3.4031307697296143,
      "learning_rate": 0.00029562929987859163,
      "loss": 0.243,
      "step": 73
    },
    {
      "epoch": 0.029948904740223607,
      "grad_norm": 6.619162082672119,
      "learning_rate": 0.00029556859571023875,
      "loss": 0.3935,
      "step": 74
    },
    {
      "epoch": 0.030353619669145546,
      "grad_norm": 3.1795220375061035,
      "learning_rate": 0.00029550789154188586,
      "loss": 0.2591,
      "step": 75
    },
    {
      "epoch": 0.030758334598067486,
      "grad_norm": 4.9926252365112305,
      "learning_rate": 0.00029544718737353293,
      "loss": 0.295,
      "step": 76
    },
    {
      "epoch": 0.031163049526989426,
      "grad_norm": 5.02488899230957,
      "learning_rate": 0.00029538648320518005,
      "loss": 0.3292,
      "step": 77
    },
    {
      "epoch": 0.03156776445591137,
      "grad_norm": 6.8157267570495605,
      "learning_rate": 0.00029532577903682716,
      "loss": 0.4761,
      "step": 78
    },
    {
      "epoch": 0.03197247938483331,
      "grad_norm": 6.042709827423096,
      "learning_rate": 0.0002952650748684743,
      "loss": 0.2592,
      "step": 79
    },
    {
      "epoch": 0.03237719431375525,
      "grad_norm": 4.012299537658691,
      "learning_rate": 0.0002952043707001214,
      "loss": 0.2355,
      "step": 80
    },
    {
      "epoch": 0.03278190924267719,
      "grad_norm": 2.3690452575683594,
      "learning_rate": 0.00029514366653176846,
      "loss": 0.2309,
      "step": 81
    },
    {
      "epoch": 0.03318662417159913,
      "grad_norm": 4.004385471343994,
      "learning_rate": 0.0002950829623634156,
      "loss": 0.3326,
      "step": 82
    },
    {
      "epoch": 0.03359133910052107,
      "grad_norm": 6.8191351890563965,
      "learning_rate": 0.0002950222581950627,
      "loss": 0.5206,
      "step": 83
    },
    {
      "epoch": 0.03399605402944301,
      "grad_norm": 7.003422260284424,
      "learning_rate": 0.0002949615540267098,
      "loss": 0.4742,
      "step": 84
    },
    {
      "epoch": 0.034400768958364955,
      "grad_norm": 5.945672988891602,
      "learning_rate": 0.00029490084985835693,
      "loss": 0.3622,
      "step": 85
    },
    {
      "epoch": 0.034805483887286895,
      "grad_norm": 4.978864669799805,
      "learning_rate": 0.00029484014569000405,
      "loss": 0.3179,
      "step": 86
    },
    {
      "epoch": 0.035210198816208835,
      "grad_norm": 4.106935024261475,
      "learning_rate": 0.0002947794415216511,
      "loss": 0.3192,
      "step": 87
    },
    {
      "epoch": 0.035614913745130775,
      "grad_norm": 4.740048885345459,
      "learning_rate": 0.00029471873735329823,
      "loss": 0.351,
      "step": 88
    },
    {
      "epoch": 0.036019628674052714,
      "grad_norm": 4.530243396759033,
      "learning_rate": 0.00029465803318494535,
      "loss": 0.2298,
      "step": 89
    },
    {
      "epoch": 0.036424343602974654,
      "grad_norm": 2.7203309535980225,
      "learning_rate": 0.00029459732901659247,
      "loss": 0.254,
      "step": 90
    },
    {
      "epoch": 0.036829058531896594,
      "grad_norm": 3.5485100746154785,
      "learning_rate": 0.0002945366248482396,
      "loss": 0.2511,
      "step": 91
    },
    {
      "epoch": 0.037233773460818534,
      "grad_norm": 2.906890392303467,
      "learning_rate": 0.00029447592067988665,
      "loss": 0.2502,
      "step": 92
    },
    {
      "epoch": 0.037638488389740474,
      "grad_norm": 3.6623785495758057,
      "learning_rate": 0.00029441521651153376,
      "loss": 0.2426,
      "step": 93
    },
    {
      "epoch": 0.03804320331866242,
      "grad_norm": 4.832069396972656,
      "learning_rate": 0.0002943545123431809,
      "loss": 0.4142,
      "step": 94
    },
    {
      "epoch": 0.03844791824758436,
      "grad_norm": 3.790980100631714,
      "learning_rate": 0.000294293808174828,
      "loss": 0.2524,
      "step": 95
    },
    {
      "epoch": 0.0388526331765063,
      "grad_norm": 8.107574462890625,
      "learning_rate": 0.0002942331040064751,
      "loss": 0.311,
      "step": 96
    },
    {
      "epoch": 0.03925734810542824,
      "grad_norm": 3.440554141998291,
      "learning_rate": 0.00029417239983812224,
      "loss": 0.3812,
      "step": 97
    },
    {
      "epoch": 0.03966206303435018,
      "grad_norm": 2.6167221069335938,
      "learning_rate": 0.0002941116956697693,
      "loss": 0.207,
      "step": 98
    },
    {
      "epoch": 0.04006677796327212,
      "grad_norm": 4.706789970397949,
      "learning_rate": 0.0002940509915014164,
      "loss": 0.3396,
      "step": 99
    },
    {
      "epoch": 0.04047149289219406,
      "grad_norm": 2.8598265647888184,
      "learning_rate": 0.00029399028733306353,
      "loss": 0.2521,
      "step": 100
    },
    {
      "epoch": 0.040876207821116,
      "grad_norm": 2.7079875469207764,
      "learning_rate": 0.00029392958316471065,
      "loss": 0.2632,
      "step": 101
    },
    {
      "epoch": 0.04128092275003794,
      "grad_norm": 3.975212812423706,
      "learning_rate": 0.0002938688789963577,
      "loss": 0.129,
      "step": 102
    },
    {
      "epoch": 0.041685637678959886,
      "grad_norm": 4.1952691078186035,
      "learning_rate": 0.00029380817482800483,
      "loss": 0.1921,
      "step": 103
    },
    {
      "epoch": 0.042090352607881826,
      "grad_norm": 4.80797004699707,
      "learning_rate": 0.00029374747065965195,
      "loss": 0.3101,
      "step": 104
    },
    {
      "epoch": 0.042495067536803766,
      "grad_norm": 7.791005611419678,
      "learning_rate": 0.00029368676649129907,
      "loss": 0.3499,
      "step": 105
    },
    {
      "epoch": 0.042899782465725705,
      "grad_norm": 5.848782539367676,
      "learning_rate": 0.00029362606232294613,
      "loss": 0.1858,
      "step": 106
    },
    {
      "epoch": 0.043304497394647645,
      "grad_norm": 4.356809616088867,
      "learning_rate": 0.00029356535815459325,
      "loss": 0.1611,
      "step": 107
    },
    {
      "epoch": 0.043709212323569585,
      "grad_norm": 2.9253950119018555,
      "learning_rate": 0.00029350465398624037,
      "loss": 0.1383,
      "step": 108
    },
    {
      "epoch": 0.044113927252491525,
      "grad_norm": 4.332372665405273,
      "learning_rate": 0.0002934439498178875,
      "loss": 0.2181,
      "step": 109
    },
    {
      "epoch": 0.044518642181413465,
      "grad_norm": 5.3375563621521,
      "learning_rate": 0.00029338324564953455,
      "loss": 0.3166,
      "step": 110
    },
    {
      "epoch": 0.044923357110335405,
      "grad_norm": 3.8372910022735596,
      "learning_rate": 0.00029332254148118166,
      "loss": 0.3079,
      "step": 111
    },
    {
      "epoch": 0.04532807203925735,
      "grad_norm": 5.797091960906982,
      "learning_rate": 0.0002932618373128288,
      "loss": 0.2762,
      "step": 112
    },
    {
      "epoch": 0.04573278696817929,
      "grad_norm": 4.213591575622559,
      "learning_rate": 0.0002932011331444759,
      "loss": 0.3032,
      "step": 113
    },
    {
      "epoch": 0.04613750189710123,
      "grad_norm": 4.972136974334717,
      "learning_rate": 0.00029314042897612296,
      "loss": 0.3123,
      "step": 114
    },
    {
      "epoch": 0.04654221682602317,
      "grad_norm": 2.641500949859619,
      "learning_rate": 0.0002930797248077701,
      "loss": 0.0693,
      "step": 115
    },
    {
      "epoch": 0.04694693175494511,
      "grad_norm": 7.130342960357666,
      "learning_rate": 0.0002930190206394172,
      "loss": 0.291,
      "step": 116
    },
    {
      "epoch": 0.04735164668386705,
      "grad_norm": 4.330962657928467,
      "learning_rate": 0.0002929583164710643,
      "loss": 0.1966,
      "step": 117
    },
    {
      "epoch": 0.04775636161278899,
      "grad_norm": 4.164599418640137,
      "learning_rate": 0.00029289761230271143,
      "loss": 0.1696,
      "step": 118
    },
    {
      "epoch": 0.04816107654171093,
      "grad_norm": 4.543781757354736,
      "learning_rate": 0.00029283690813435855,
      "loss": 0.2922,
      "step": 119
    },
    {
      "epoch": 0.04856579147063287,
      "grad_norm": 3.310497522354126,
      "learning_rate": 0.0002927762039660056,
      "loss": 0.2162,
      "step": 120
    },
    {
      "epoch": 0.04897050639955482,
      "grad_norm": 3.3368589878082275,
      "learning_rate": 0.00029271549979765273,
      "loss": 0.2009,
      "step": 121
    },
    {
      "epoch": 0.04937522132847676,
      "grad_norm": 4.5496602058410645,
      "learning_rate": 0.00029265479562929985,
      "loss": 0.2332,
      "step": 122
    },
    {
      "epoch": 0.049779936257398696,
      "grad_norm": 3.366652250289917,
      "learning_rate": 0.00029259409146094697,
      "loss": 0.1941,
      "step": 123
    },
    {
      "epoch": 0.050184651186320636,
      "grad_norm": 3.692636013031006,
      "learning_rate": 0.0002925333872925941,
      "loss": 0.247,
      "step": 124
    },
    {
      "epoch": 0.050589366115242576,
      "grad_norm": 5.216883182525635,
      "learning_rate": 0.00029247268312424115,
      "loss": 0.3623,
      "step": 125
    },
    {
      "epoch": 0.050994081044164516,
      "grad_norm": 2.996424913406372,
      "learning_rate": 0.00029241197895588827,
      "loss": 0.2184,
      "step": 126
    },
    {
      "epoch": 0.051398795973086456,
      "grad_norm": 4.81725549697876,
      "learning_rate": 0.0002923512747875354,
      "loss": 0.2621,
      "step": 127
    },
    {
      "epoch": 0.051803510902008396,
      "grad_norm": 5.6742024421691895,
      "learning_rate": 0.0002922905706191825,
      "loss": 0.3893,
      "step": 128
    },
    {
      "epoch": 0.052208225830930335,
      "grad_norm": 6.898286819458008,
      "learning_rate": 0.0002922298664508296,
      "loss": 0.4438,
      "step": 129
    },
    {
      "epoch": 0.05261294075985228,
      "grad_norm": 3.5456011295318604,
      "learning_rate": 0.00029216916228247674,
      "loss": 0.2697,
      "step": 130
    },
    {
      "epoch": 0.05301765568877422,
      "grad_norm": 4.826069355010986,
      "learning_rate": 0.0002921084581141238,
      "loss": 0.2299,
      "step": 131
    },
    {
      "epoch": 0.05342237061769616,
      "grad_norm": 4.644303321838379,
      "learning_rate": 0.0002920477539457709,
      "loss": 0.2176,
      "step": 132
    },
    {
      "epoch": 0.0538270855466181,
      "grad_norm": 4.198410987854004,
      "learning_rate": 0.00029198704977741804,
      "loss": 0.1382,
      "step": 133
    },
    {
      "epoch": 0.05423180047554004,
      "grad_norm": 3.7301502227783203,
      "learning_rate": 0.00029192634560906515,
      "loss": 0.1338,
      "step": 134
    },
    {
      "epoch": 0.05463651540446198,
      "grad_norm": 4.470071315765381,
      "learning_rate": 0.00029186564144071227,
      "loss": 0.2401,
      "step": 135
    },
    {
      "epoch": 0.05504123033338392,
      "grad_norm": 2.959949493408203,
      "learning_rate": 0.00029180493727235933,
      "loss": 0.1762,
      "step": 136
    },
    {
      "epoch": 0.05544594526230586,
      "grad_norm": 4.361060619354248,
      "learning_rate": 0.00029174423310400645,
      "loss": 0.2275,
      "step": 137
    },
    {
      "epoch": 0.0558506601912278,
      "grad_norm": 2.3785412311553955,
      "learning_rate": 0.00029168352893565357,
      "loss": 0.1518,
      "step": 138
    },
    {
      "epoch": 0.05625537512014975,
      "grad_norm": 4.4628496170043945,
      "learning_rate": 0.0002916228247673007,
      "loss": 0.3272,
      "step": 139
    },
    {
      "epoch": 0.05666009004907169,
      "grad_norm": 4.200509548187256,
      "learning_rate": 0.0002915621205989478,
      "loss": 0.1722,
      "step": 140
    },
    {
      "epoch": 0.05706480497799363,
      "grad_norm": 3.837481737136841,
      "learning_rate": 0.00029150141643059487,
      "loss": 0.1334,
      "step": 141
    },
    {
      "epoch": 0.05746951990691557,
      "grad_norm": 3.512986421585083,
      "learning_rate": 0.000291440712262242,
      "loss": 0.2537,
      "step": 142
    },
    {
      "epoch": 0.05787423483583751,
      "grad_norm": 4.710855484008789,
      "learning_rate": 0.0002913800080938891,
      "loss": 0.1997,
      "step": 143
    },
    {
      "epoch": 0.05827894976475945,
      "grad_norm": 1.9994524717330933,
      "learning_rate": 0.0002913193039255362,
      "loss": 0.129,
      "step": 144
    },
    {
      "epoch": 0.05868366469368139,
      "grad_norm": 3.6724538803100586,
      "learning_rate": 0.0002912585997571833,
      "loss": 0.189,
      "step": 145
    },
    {
      "epoch": 0.059088379622603326,
      "grad_norm": 4.911861419677734,
      "learning_rate": 0.0002911978955888304,
      "loss": 0.1953,
      "step": 146
    },
    {
      "epoch": 0.059493094551525266,
      "grad_norm": 1.9087368249893188,
      "learning_rate": 0.0002911371914204775,
      "loss": 0.1539,
      "step": 147
    },
    {
      "epoch": 0.05989780948044721,
      "grad_norm": 4.3609538078308105,
      "learning_rate": 0.00029107648725212464,
      "loss": 0.1122,
      "step": 148
    },
    {
      "epoch": 0.06030252440936915,
      "grad_norm": 4.732774257659912,
      "learning_rate": 0.0002910157830837717,
      "loss": 0.3068,
      "step": 149
    },
    {
      "epoch": 0.06070723933829109,
      "grad_norm": 4.191019058227539,
      "learning_rate": 0.0002909550789154188,
      "loss": 0.2371,
      "step": 150
    },
    {
      "epoch": 0.06111195426721303,
      "grad_norm": 5.4464240074157715,
      "learning_rate": 0.00029089437474706594,
      "loss": 0.227,
      "step": 151
    },
    {
      "epoch": 0.06151666919613497,
      "grad_norm": 5.408900260925293,
      "learning_rate": 0.00029083367057871305,
      "loss": 0.2602,
      "step": 152
    },
    {
      "epoch": 0.06192138412505691,
      "grad_norm": 3.8972887992858887,
      "learning_rate": 0.0002907729664103601,
      "loss": 0.1918,
      "step": 153
    },
    {
      "epoch": 0.06232609905397885,
      "grad_norm": 3.5640664100646973,
      "learning_rate": 0.00029071226224200723,
      "loss": 0.1576,
      "step": 154
    },
    {
      "epoch": 0.0627308139829008,
      "grad_norm": 6.481161117553711,
      "learning_rate": 0.00029065155807365435,
      "loss": 0.3348,
      "step": 155
    },
    {
      "epoch": 0.06313552891182274,
      "grad_norm": 5.416360855102539,
      "learning_rate": 0.00029059085390530147,
      "loss": 0.1933,
      "step": 156
    },
    {
      "epoch": 0.06354024384074468,
      "grad_norm": 4.661542892456055,
      "learning_rate": 0.0002905301497369486,
      "loss": 0.2715,
      "step": 157
    },
    {
      "epoch": 0.06394495876966662,
      "grad_norm": 3.5946474075317383,
      "learning_rate": 0.00029046944556859565,
      "loss": 0.2308,
      "step": 158
    },
    {
      "epoch": 0.06434967369858856,
      "grad_norm": 2.977748155593872,
      "learning_rate": 0.00029040874140024277,
      "loss": 0.1489,
      "step": 159
    },
    {
      "epoch": 0.0647543886275105,
      "grad_norm": 4.346152305603027,
      "learning_rate": 0.0002903480372318899,
      "loss": 0.1647,
      "step": 160
    },
    {
      "epoch": 0.06515910355643244,
      "grad_norm": 4.270928382873535,
      "learning_rate": 0.000290287333063537,
      "loss": 0.1593,
      "step": 161
    },
    {
      "epoch": 0.06556381848535438,
      "grad_norm": 4.498690128326416,
      "learning_rate": 0.0002902266288951841,
      "loss": 0.2448,
      "step": 162
    },
    {
      "epoch": 0.06596853341427632,
      "grad_norm": 6.748908519744873,
      "learning_rate": 0.0002901659247268312,
      "loss": 0.1179,
      "step": 163
    },
    {
      "epoch": 0.06637324834319826,
      "grad_norm": 5.131928443908691,
      "learning_rate": 0.0002901052205584783,
      "loss": 0.1635,
      "step": 164
    },
    {
      "epoch": 0.0667779632721202,
      "grad_norm": 5.0977277755737305,
      "learning_rate": 0.0002900445163901254,
      "loss": 0.19,
      "step": 165
    },
    {
      "epoch": 0.06718267820104214,
      "grad_norm": 4.538437843322754,
      "learning_rate": 0.00028998381222177254,
      "loss": 0.2494,
      "step": 166
    },
    {
      "epoch": 0.06758739312996408,
      "grad_norm": 2.6854028701782227,
      "learning_rate": 0.00028992310805341966,
      "loss": 0.2126,
      "step": 167
    },
    {
      "epoch": 0.06799210805888602,
      "grad_norm": 2.84639835357666,
      "learning_rate": 0.00028986240388506677,
      "loss": 0.1371,
      "step": 168
    },
    {
      "epoch": 0.06839682298780796,
      "grad_norm": 2.2624151706695557,
      "learning_rate": 0.00028980169971671384,
      "loss": 0.1136,
      "step": 169
    },
    {
      "epoch": 0.06880153791672991,
      "grad_norm": 4.343297958374023,
      "learning_rate": 0.00028974099554836095,
      "loss": 0.2758,
      "step": 170
    },
    {
      "epoch": 0.06920625284565185,
      "grad_norm": 4.103126525878906,
      "learning_rate": 0.00028968029138000807,
      "loss": 0.2311,
      "step": 171
    },
    {
      "epoch": 0.06961096777457379,
      "grad_norm": 7.208852767944336,
      "learning_rate": 0.0002896195872116552,
      "loss": 0.3335,
      "step": 172
    },
    {
      "epoch": 0.07001568270349573,
      "grad_norm": 5.525347709655762,
      "learning_rate": 0.0002895588830433023,
      "loss": 0.2398,
      "step": 173
    },
    {
      "epoch": 0.07042039763241767,
      "grad_norm": 5.022634506225586,
      "learning_rate": 0.00028949817887494937,
      "loss": 0.2659,
      "step": 174
    },
    {
      "epoch": 0.07082511256133961,
      "grad_norm": 2.5018320083618164,
      "learning_rate": 0.0002894374747065965,
      "loss": 0.1245,
      "step": 175
    },
    {
      "epoch": 0.07122982749026155,
      "grad_norm": 3.143778085708618,
      "learning_rate": 0.0002893767705382436,
      "loss": 0.1271,
      "step": 176
    },
    {
      "epoch": 0.07163454241918349,
      "grad_norm": 3.0323314666748047,
      "learning_rate": 0.0002893160663698907,
      "loss": 0.1971,
      "step": 177
    },
    {
      "epoch": 0.07203925734810543,
      "grad_norm": 4.181677341461182,
      "learning_rate": 0.00028925536220153784,
      "loss": 0.2151,
      "step": 178
    },
    {
      "epoch": 0.07244397227702737,
      "grad_norm": 6.521783351898193,
      "learning_rate": 0.00028919465803318496,
      "loss": 0.1733,
      "step": 179
    },
    {
      "epoch": 0.07284868720594931,
      "grad_norm": 4.524399280548096,
      "learning_rate": 0.000289133953864832,
      "loss": 0.1161,
      "step": 180
    },
    {
      "epoch": 0.07325340213487125,
      "grad_norm": 4.2222981452941895,
      "learning_rate": 0.00028907324969647914,
      "loss": 0.1426,
      "step": 181
    },
    {
      "epoch": 0.07365811706379319,
      "grad_norm": 4.193050384521484,
      "learning_rate": 0.00028901254552812626,
      "loss": 0.1458,
      "step": 182
    },
    {
      "epoch": 0.07406283199271513,
      "grad_norm": 3.858262062072754,
      "learning_rate": 0.0002889518413597734,
      "loss": 0.1659,
      "step": 183
    },
    {
      "epoch": 0.07446754692163707,
      "grad_norm": 3.7937846183776855,
      "learning_rate": 0.00028889113719142044,
      "loss": 0.1257,
      "step": 184
    },
    {
      "epoch": 0.07487226185055901,
      "grad_norm": 2.8088021278381348,
      "learning_rate": 0.00028883043302306756,
      "loss": 0.1581,
      "step": 185
    },
    {
      "epoch": 0.07527697677948095,
      "grad_norm": 3.7814037799835205,
      "learning_rate": 0.0002887697288547147,
      "loss": 0.1161,
      "step": 186
    },
    {
      "epoch": 0.07568169170840289,
      "grad_norm": 4.740095138549805,
      "learning_rate": 0.0002887090246863618,
      "loss": 0.1873,
      "step": 187
    },
    {
      "epoch": 0.07608640663732484,
      "grad_norm": 4.068313121795654,
      "learning_rate": 0.00028864832051800885,
      "loss": 0.1271,
      "step": 188
    },
    {
      "epoch": 0.07649112156624678,
      "grad_norm": 4.611112117767334,
      "learning_rate": 0.00028858761634965597,
      "loss": 0.2179,
      "step": 189
    },
    {
      "epoch": 0.07689583649516872,
      "grad_norm": 7.800652503967285,
      "learning_rate": 0.0002885269121813031,
      "loss": 0.2944,
      "step": 190
    },
    {
      "epoch": 0.07730055142409066,
      "grad_norm": 5.870997905731201,
      "learning_rate": 0.0002884662080129502,
      "loss": 0.1534,
      "step": 191
    },
    {
      "epoch": 0.0777052663530126,
      "grad_norm": 1.8697011470794678,
      "learning_rate": 0.00028840550384459727,
      "loss": 0.1428,
      "step": 192
    },
    {
      "epoch": 0.07810998128193454,
      "grad_norm": 5.954970359802246,
      "learning_rate": 0.0002883447996762444,
      "loss": 0.2334,
      "step": 193
    },
    {
      "epoch": 0.07851469621085648,
      "grad_norm": 5.16180419921875,
      "learning_rate": 0.0002882840955078915,
      "loss": 0.1635,
      "step": 194
    },
    {
      "epoch": 0.07891941113977842,
      "grad_norm": 3.383253335952759,
      "learning_rate": 0.0002882233913395386,
      "loss": 0.1677,
      "step": 195
    },
    {
      "epoch": 0.07932412606870036,
      "grad_norm": 2.5685951709747314,
      "learning_rate": 0.0002881626871711857,
      "loss": 0.0859,
      "step": 196
    },
    {
      "epoch": 0.0797288409976223,
      "grad_norm": 5.524072170257568,
      "learning_rate": 0.0002881019830028328,
      "loss": 0.1044,
      "step": 197
    },
    {
      "epoch": 0.08013355592654424,
      "grad_norm": 4.235884189605713,
      "learning_rate": 0.0002880412788344799,
      "loss": 0.1627,
      "step": 198
    },
    {
      "epoch": 0.08053827085546618,
      "grad_norm": 4.031894207000732,
      "learning_rate": 0.00028798057466612704,
      "loss": 0.094,
      "step": 199
    },
    {
      "epoch": 0.08094298578438812,
      "grad_norm": 5.19568395614624,
      "learning_rate": 0.00028791987049777416,
      "loss": 0.1507,
      "step": 200
    },
    {
      "epoch": 0.08134770071331006,
      "grad_norm": 4.622841835021973,
      "learning_rate": 0.0002878591663294213,
      "loss": 0.2048,
      "step": 201
    },
    {
      "epoch": 0.081752415642232,
      "grad_norm": 7.031576633453369,
      "learning_rate": 0.00028779846216106834,
      "loss": 0.2062,
      "step": 202
    },
    {
      "epoch": 0.08215713057115394,
      "grad_norm": 7.018929958343506,
      "learning_rate": 0.00028773775799271546,
      "loss": 0.2808,
      "step": 203
    },
    {
      "epoch": 0.08256184550007588,
      "grad_norm": 6.3805718421936035,
      "learning_rate": 0.0002876770538243626,
      "loss": 0.1583,
      "step": 204
    },
    {
      "epoch": 0.08296656042899782,
      "grad_norm": 3.9775640964508057,
      "learning_rate": 0.0002876163496560097,
      "loss": 0.214,
      "step": 205
    },
    {
      "epoch": 0.08337127535791977,
      "grad_norm": 3.2399673461914062,
      "learning_rate": 0.0002875556454876568,
      "loss": 0.1376,
      "step": 206
    },
    {
      "epoch": 0.08377599028684171,
      "grad_norm": 5.175667762756348,
      "learning_rate": 0.00028749494131930387,
      "loss": 0.1911,
      "step": 207
    },
    {
      "epoch": 0.08418070521576365,
      "grad_norm": 6.1704325675964355,
      "learning_rate": 0.000287434237150951,
      "loss": 0.3692,
      "step": 208
    },
    {
      "epoch": 0.08458542014468559,
      "grad_norm": 4.997256278991699,
      "learning_rate": 0.0002873735329825981,
      "loss": 0.1529,
      "step": 209
    },
    {
      "epoch": 0.08499013507360753,
      "grad_norm": 3.696410894393921,
      "learning_rate": 0.0002873128288142452,
      "loss": 0.12,
      "step": 210
    },
    {
      "epoch": 0.08539485000252947,
      "grad_norm": 2.7030749320983887,
      "learning_rate": 0.00028725212464589234,
      "loss": 0.0906,
      "step": 211
    },
    {
      "epoch": 0.08579956493145141,
      "grad_norm": 4.033035755157471,
      "learning_rate": 0.00028719142047753946,
      "loss": 0.2157,
      "step": 212
    },
    {
      "epoch": 0.08620427986037335,
      "grad_norm": 4.608082294464111,
      "learning_rate": 0.0002871307163091865,
      "loss": 0.1153,
      "step": 213
    },
    {
      "epoch": 0.08660899478929529,
      "grad_norm": 3.860154867172241,
      "learning_rate": 0.00028707001214083364,
      "loss": 0.162,
      "step": 214
    },
    {
      "epoch": 0.08701370971821723,
      "grad_norm": 3.7828385829925537,
      "learning_rate": 0.00028700930797248076,
      "loss": 0.1123,
      "step": 215
    },
    {
      "epoch": 0.08741842464713917,
      "grad_norm": 2.7201027870178223,
      "learning_rate": 0.0002869486038041279,
      "loss": 0.132,
      "step": 216
    },
    {
      "epoch": 0.08782313957606111,
      "grad_norm": 3.308485984802246,
      "learning_rate": 0.000286887899635775,
      "loss": 0.1334,
      "step": 217
    },
    {
      "epoch": 0.08822785450498305,
      "grad_norm": 3.1447298526763916,
      "learning_rate": 0.00028682719546742206,
      "loss": 0.1238,
      "step": 218
    },
    {
      "epoch": 0.08863256943390499,
      "grad_norm": 4.473772048950195,
      "learning_rate": 0.0002867664912990692,
      "loss": 0.2255,
      "step": 219
    },
    {
      "epoch": 0.08903728436282693,
      "grad_norm": 3.8530187606811523,
      "learning_rate": 0.0002867057871307163,
      "loss": 0.1953,
      "step": 220
    },
    {
      "epoch": 0.08944199929174887,
      "grad_norm": 4.5437846183776855,
      "learning_rate": 0.0002866450829623634,
      "loss": 0.1976,
      "step": 221
    },
    {
      "epoch": 0.08984671422067081,
      "grad_norm": 2.4733827114105225,
      "learning_rate": 0.00028658437879401053,
      "loss": 0.1613,
      "step": 222
    },
    {
      "epoch": 0.09025142914959275,
      "grad_norm": 2.906649112701416,
      "learning_rate": 0.0002865236746256576,
      "loss": 0.1627,
      "step": 223
    },
    {
      "epoch": 0.0906561440785147,
      "grad_norm": 5.924722671508789,
      "learning_rate": 0.0002864629704573047,
      "loss": 0.2466,
      "step": 224
    },
    {
      "epoch": 0.09106085900743664,
      "grad_norm": 4.013983726501465,
      "learning_rate": 0.0002864022662889518,
      "loss": 0.2076,
      "step": 225
    },
    {
      "epoch": 0.09146557393635858,
      "grad_norm": 2.6354007720947266,
      "learning_rate": 0.00028634156212059894,
      "loss": 0.1476,
      "step": 226
    },
    {
      "epoch": 0.09187028886528052,
      "grad_norm": 10.013409614562988,
      "learning_rate": 0.000286280857952246,
      "loss": 0.3654,
      "step": 227
    },
    {
      "epoch": 0.09227500379420246,
      "grad_norm": 2.464038848876953,
      "learning_rate": 0.0002862201537838931,
      "loss": 0.1012,
      "step": 228
    },
    {
      "epoch": 0.0926797187231244,
      "grad_norm": 1.9376822710037231,
      "learning_rate": 0.00028615944961554024,
      "loss": 0.1065,
      "step": 229
    },
    {
      "epoch": 0.09308443365204634,
      "grad_norm": 1.7318363189697266,
      "learning_rate": 0.00028609874544718736,
      "loss": 0.0441,
      "step": 230
    },
    {
      "epoch": 0.09348914858096828,
      "grad_norm": 1.753899097442627,
      "learning_rate": 0.0002860380412788345,
      "loss": 0.0562,
      "step": 231
    },
    {
      "epoch": 0.09389386350989022,
      "grad_norm": 3.4776861667633057,
      "learning_rate": 0.00028597733711048154,
      "loss": 0.1141,
      "step": 232
    },
    {
      "epoch": 0.09429857843881216,
      "grad_norm": 3.364966869354248,
      "learning_rate": 0.00028591663294212866,
      "loss": 0.1233,
      "step": 233
    },
    {
      "epoch": 0.0947032933677341,
      "grad_norm": 4.179869651794434,
      "learning_rate": 0.0002858559287737758,
      "loss": 0.3334,
      "step": 234
    },
    {
      "epoch": 0.09510800829665604,
      "grad_norm": 2.5550014972686768,
      "learning_rate": 0.0002857952246054229,
      "loss": 0.0606,
      "step": 235
    },
    {
      "epoch": 0.09551272322557798,
      "grad_norm": 5.203214645385742,
      "learning_rate": 0.00028573452043706996,
      "loss": 0.1861,
      "step": 236
    },
    {
      "epoch": 0.09591743815449992,
      "grad_norm": 5.147572040557861,
      "learning_rate": 0.0002856738162687171,
      "loss": 0.1927,
      "step": 237
    },
    {
      "epoch": 0.09632215308342186,
      "grad_norm": 2.9220786094665527,
      "learning_rate": 0.0002856131121003642,
      "loss": 0.0601,
      "step": 238
    },
    {
      "epoch": 0.0967268680123438,
      "grad_norm": 4.404560089111328,
      "learning_rate": 0.0002855524079320113,
      "loss": 0.1178,
      "step": 239
    },
    {
      "epoch": 0.09713158294126574,
      "grad_norm": 4.318569183349609,
      "learning_rate": 0.0002854917037636584,
      "loss": 0.1267,
      "step": 240
    },
    {
      "epoch": 0.09753629787018768,
      "grad_norm": 3.6259095668792725,
      "learning_rate": 0.0002854309995953055,
      "loss": 0.2198,
      "step": 241
    },
    {
      "epoch": 0.09794101279910963,
      "grad_norm": 5.819733619689941,
      "learning_rate": 0.0002853702954269526,
      "loss": 0.1543,
      "step": 242
    },
    {
      "epoch": 0.09834572772803157,
      "grad_norm": 3.0062143802642822,
      "learning_rate": 0.0002853095912585997,
      "loss": 0.1079,
      "step": 243
    },
    {
      "epoch": 0.09875044265695351,
      "grad_norm": 4.939717769622803,
      "learning_rate": 0.00028524888709024684,
      "loss": 0.1684,
      "step": 244
    },
    {
      "epoch": 0.09915515758587545,
      "grad_norm": 7.136099338531494,
      "learning_rate": 0.00028518818292189396,
      "loss": 0.1821,
      "step": 245
    },
    {
      "epoch": 0.09955987251479739,
      "grad_norm": 6.274852275848389,
      "learning_rate": 0.000285127478753541,
      "loss": 0.1406,
      "step": 246
    },
    {
      "epoch": 0.09996458744371933,
      "grad_norm": 5.223964214324951,
      "learning_rate": 0.00028506677458518814,
      "loss": 0.1568,
      "step": 247
    },
    {
      "epoch": 0.10036930237264127,
      "grad_norm": 3.9115772247314453,
      "learning_rate": 0.00028500607041683526,
      "loss": 0.0708,
      "step": 248
    },
    {
      "epoch": 0.10077401730156321,
      "grad_norm": 3.5711164474487305,
      "learning_rate": 0.0002849453662484824,
      "loss": 0.1589,
      "step": 249
    },
    {
      "epoch": 0.10117873223048515,
      "grad_norm": 1.1663529872894287,
      "learning_rate": 0.0002848846620801295,
      "loss": 0.0566,
      "step": 250
    },
    {
      "epoch": 0.10158344715940709,
      "grad_norm": 2.616361141204834,
      "learning_rate": 0.00028482395791177656,
      "loss": 0.1518,
      "step": 251
    },
    {
      "epoch": 0.10198816208832903,
      "grad_norm": 3.2067713737487793,
      "learning_rate": 0.0002847632537434237,
      "loss": 0.1099,
      "step": 252
    },
    {
      "epoch": 0.10239287701725097,
      "grad_norm": 2.173792839050293,
      "learning_rate": 0.0002847025495750708,
      "loss": 0.101,
      "step": 253
    },
    {
      "epoch": 0.10279759194617291,
      "grad_norm": 3.592874765396118,
      "learning_rate": 0.0002846418454067179,
      "loss": 0.0979,
      "step": 254
    },
    {
      "epoch": 0.10320230687509485,
      "grad_norm": 4.28223180770874,
      "learning_rate": 0.00028458114123836503,
      "loss": 0.141,
      "step": 255
    },
    {
      "epoch": 0.10360702180401679,
      "grad_norm": 2.932933807373047,
      "learning_rate": 0.00028452043707001215,
      "loss": 0.0673,
      "step": 256
    },
    {
      "epoch": 0.10401173673293873,
      "grad_norm": 1.3963236808776855,
      "learning_rate": 0.0002844597329016592,
      "loss": 0.0471,
      "step": 257
    },
    {
      "epoch": 0.10441645166186067,
      "grad_norm": 6.9858574867248535,
      "learning_rate": 0.00028439902873330633,
      "loss": 0.1828,
      "step": 258
    },
    {
      "epoch": 0.10482116659078261,
      "grad_norm": 3.568052053451538,
      "learning_rate": 0.00028433832456495345,
      "loss": 0.119,
      "step": 259
    },
    {
      "epoch": 0.10522588151970456,
      "grad_norm": 2.695509910583496,
      "learning_rate": 0.00028427762039660056,
      "loss": 0.0474,
      "step": 260
    },
    {
      "epoch": 0.1056305964486265,
      "grad_norm": 6.284395694732666,
      "learning_rate": 0.0002842169162282477,
      "loss": 0.1902,
      "step": 261
    },
    {
      "epoch": 0.10603531137754844,
      "grad_norm": 3.4106881618499756,
      "learning_rate": 0.00028415621205989474,
      "loss": 0.0821,
      "step": 262
    },
    {
      "epoch": 0.10644002630647038,
      "grad_norm": 2.450223445892334,
      "learning_rate": 0.00028409550789154186,
      "loss": 0.1471,
      "step": 263
    },
    {
      "epoch": 0.10684474123539232,
      "grad_norm": 7.65010929107666,
      "learning_rate": 0.000284034803723189,
      "loss": 0.3986,
      "step": 264
    },
    {
      "epoch": 0.10724945616431426,
      "grad_norm": 4.3031840324401855,
      "learning_rate": 0.0002839740995548361,
      "loss": 0.0829,
      "step": 265
    },
    {
      "epoch": 0.1076541710932362,
      "grad_norm": 1.6360859870910645,
      "learning_rate": 0.0002839133953864832,
      "loss": 0.085,
      "step": 266
    },
    {
      "epoch": 0.10805888602215814,
      "grad_norm": 4.352919101715088,
      "learning_rate": 0.0002838526912181303,
      "loss": 0.1922,
      "step": 267
    },
    {
      "epoch": 0.10846360095108008,
      "grad_norm": 4.616546154022217,
      "learning_rate": 0.0002837919870497774,
      "loss": 0.1728,
      "step": 268
    },
    {
      "epoch": 0.10886831588000202,
      "grad_norm": 4.126352310180664,
      "learning_rate": 0.0002837312828814245,
      "loss": 0.1621,
      "step": 269
    },
    {
      "epoch": 0.10927303080892396,
      "grad_norm": 3.9414901733398438,
      "learning_rate": 0.00028367057871307163,
      "loss": 0.1452,
      "step": 270
    },
    {
      "epoch": 0.1096777457378459,
      "grad_norm": 2.366060256958008,
      "learning_rate": 0.0002836098745447187,
      "loss": 0.113,
      "step": 271
    },
    {
      "epoch": 0.11008246066676784,
      "grad_norm": 3.8414251804351807,
      "learning_rate": 0.0002835491703763658,
      "loss": 0.2451,
      "step": 272
    },
    {
      "epoch": 0.11048717559568978,
      "grad_norm": 3.970635175704956,
      "learning_rate": 0.00028348846620801293,
      "loss": 0.1503,
      "step": 273
    },
    {
      "epoch": 0.11089189052461172,
      "grad_norm": 2.0134143829345703,
      "learning_rate": 0.00028342776203966005,
      "loss": 0.1522,
      "step": 274
    },
    {
      "epoch": 0.11129660545353366,
      "grad_norm": 2.0021731853485107,
      "learning_rate": 0.0002833670578713071,
      "loss": 0.1024,
      "step": 275
    },
    {
      "epoch": 0.1117013203824556,
      "grad_norm": 3.5149693489074707,
      "learning_rate": 0.00028330635370295423,
      "loss": 0.0782,
      "step": 276
    },
    {
      "epoch": 0.11210603531137754,
      "grad_norm": 3.5892744064331055,
      "learning_rate": 0.00028324564953460135,
      "loss": 0.1824,
      "step": 277
    },
    {
      "epoch": 0.1125107502402995,
      "grad_norm": 8.44450569152832,
      "learning_rate": 0.00028318494536624846,
      "loss": 0.2606,
      "step": 278
    },
    {
      "epoch": 0.11291546516922144,
      "grad_norm": 3.297325611114502,
      "learning_rate": 0.00028312424119789553,
      "loss": 0.2056,
      "step": 279
    },
    {
      "epoch": 0.11332018009814337,
      "grad_norm": 4.146347522735596,
      "learning_rate": 0.00028306353702954264,
      "loss": 0.1385,
      "step": 280
    },
    {
      "epoch": 0.11372489502706531,
      "grad_norm": 5.15305233001709,
      "learning_rate": 0.00028300283286118976,
      "loss": 0.2133,
      "step": 281
    },
    {
      "epoch": 0.11412960995598725,
      "grad_norm": 1.967445731163025,
      "learning_rate": 0.0002829421286928369,
      "loss": 0.0711,
      "step": 282
    },
    {
      "epoch": 0.1145343248849092,
      "grad_norm": 4.554964065551758,
      "learning_rate": 0.000282881424524484,
      "loss": 0.1549,
      "step": 283
    },
    {
      "epoch": 0.11493903981383113,
      "grad_norm": 1.5941534042358398,
      "learning_rate": 0.00028282072035613106,
      "loss": 0.1283,
      "step": 284
    },
    {
      "epoch": 0.11534375474275307,
      "grad_norm": 3.787686347961426,
      "learning_rate": 0.0002827600161877782,
      "loss": 0.2584,
      "step": 285
    },
    {
      "epoch": 0.11574846967167501,
      "grad_norm": 2.6941659450531006,
      "learning_rate": 0.0002826993120194253,
      "loss": 0.0949,
      "step": 286
    },
    {
      "epoch": 0.11615318460059695,
      "grad_norm": 4.302460670471191,
      "learning_rate": 0.0002826386078510724,
      "loss": 0.154,
      "step": 287
    },
    {
      "epoch": 0.1165578995295189,
      "grad_norm": 2.972332715988159,
      "learning_rate": 0.00028257790368271953,
      "loss": 0.1176,
      "step": 288
    },
    {
      "epoch": 0.11696261445844083,
      "grad_norm": 1.9537014961242676,
      "learning_rate": 0.00028251719951436665,
      "loss": 0.1675,
      "step": 289
    },
    {
      "epoch": 0.11736732938736277,
      "grad_norm": 2.6581339836120605,
      "learning_rate": 0.0002824564953460137,
      "loss": 0.1438,
      "step": 290
    },
    {
      "epoch": 0.11777204431628471,
      "grad_norm": 4.444108486175537,
      "learning_rate": 0.00028239579117766083,
      "loss": 0.1537,
      "step": 291
    },
    {
      "epoch": 0.11817675924520665,
      "grad_norm": 3.130025863647461,
      "learning_rate": 0.00028233508700930795,
      "loss": 0.2315,
      "step": 292
    },
    {
      "epoch": 0.11858147417412859,
      "grad_norm": 2.2596068382263184,
      "learning_rate": 0.00028227438284095507,
      "loss": 0.1469,
      "step": 293
    },
    {
      "epoch": 0.11898618910305053,
      "grad_norm": 2.533735990524292,
      "learning_rate": 0.0002822136786726022,
      "loss": 0.1259,
      "step": 294
    },
    {
      "epoch": 0.11939090403197249,
      "grad_norm": 6.929020404815674,
      "learning_rate": 0.00028215297450424925,
      "loss": 0.3074,
      "step": 295
    },
    {
      "epoch": 0.11979561896089443,
      "grad_norm": 3.3630781173706055,
      "learning_rate": 0.00028209227033589636,
      "loss": 0.1266,
      "step": 296
    },
    {
      "epoch": 0.12020033388981637,
      "grad_norm": 4.5739521980285645,
      "learning_rate": 0.0002820315661675435,
      "loss": 0.2234,
      "step": 297
    },
    {
      "epoch": 0.1206050488187383,
      "grad_norm": 3.581928014755249,
      "learning_rate": 0.0002819708619991906,
      "loss": 0.0873,
      "step": 298
    },
    {
      "epoch": 0.12100976374766025,
      "grad_norm": 2.9118902683258057,
      "learning_rate": 0.0002819101578308377,
      "loss": 0.094,
      "step": 299
    },
    {
      "epoch": 0.12141447867658219,
      "grad_norm": 1.9627609252929688,
      "learning_rate": 0.00028184945366248483,
      "loss": 0.0779,
      "step": 300
    },
    {
      "epoch": 0.12181919360550413,
      "grad_norm": 6.505875110626221,
      "learning_rate": 0.0002817887494941319,
      "loss": 0.2103,
      "step": 301
    },
    {
      "epoch": 0.12222390853442607,
      "grad_norm": 0.8674436807632446,
      "learning_rate": 0.000281728045325779,
      "loss": 0.067,
      "step": 302
    },
    {
      "epoch": 0.122628623463348,
      "grad_norm": 5.019002437591553,
      "learning_rate": 0.00028166734115742613,
      "loss": 0.2108,
      "step": 303
    },
    {
      "epoch": 0.12303333839226994,
      "grad_norm": 7.589794158935547,
      "learning_rate": 0.00028160663698907325,
      "loss": 0.2404,
      "step": 304
    },
    {
      "epoch": 0.12343805332119188,
      "grad_norm": 3.2354612350463867,
      "learning_rate": 0.00028154593282072037,
      "loss": 0.1284,
      "step": 305
    },
    {
      "epoch": 0.12384276825011382,
      "grad_norm": 4.9794392585754395,
      "learning_rate": 0.00028148522865236743,
      "loss": 0.2225,
      "step": 306
    },
    {
      "epoch": 0.12424748317903576,
      "grad_norm": 3.4997947216033936,
      "learning_rate": 0.00028142452448401455,
      "loss": 0.1598,
      "step": 307
    },
    {
      "epoch": 0.1246521981079577,
      "grad_norm": 4.860116958618164,
      "learning_rate": 0.00028136382031566167,
      "loss": 0.2598,
      "step": 308
    },
    {
      "epoch": 0.12505691303687966,
      "grad_norm": 3.248856782913208,
      "learning_rate": 0.0002813031161473088,
      "loss": 0.1029,
      "step": 309
    },
    {
      "epoch": 0.1254616279658016,
      "grad_norm": 5.241997241973877,
      "learning_rate": 0.00028124241197895585,
      "loss": 0.1642,
      "step": 310
    },
    {
      "epoch": 0.12586634289472354,
      "grad_norm": 4.6705498695373535,
      "learning_rate": 0.00028118170781060297,
      "loss": 0.1244,
      "step": 311
    },
    {
      "epoch": 0.12627105782364548,
      "grad_norm": 2.1659231185913086,
      "learning_rate": 0.0002811210036422501,
      "loss": 0.0955,
      "step": 312
    },
    {
      "epoch": 0.12667577275256742,
      "grad_norm": 4.9492950439453125,
      "learning_rate": 0.0002810602994738972,
      "loss": 0.2834,
      "step": 313
    },
    {
      "epoch": 0.12708048768148936,
      "grad_norm": 2.500284194946289,
      "learning_rate": 0.00028099959530554426,
      "loss": 0.1452,
      "step": 314
    },
    {
      "epoch": 0.1274852026104113,
      "grad_norm": 5.693544864654541,
      "learning_rate": 0.0002809388911371914,
      "loss": 0.1627,
      "step": 315
    },
    {
      "epoch": 0.12788991753933324,
      "grad_norm": 1.982316017150879,
      "learning_rate": 0.0002808781869688385,
      "loss": 0.0634,
      "step": 316
    },
    {
      "epoch": 0.12829463246825518,
      "grad_norm": 4.99901008605957,
      "learning_rate": 0.0002808174828004856,
      "loss": 0.1572,
      "step": 317
    },
    {
      "epoch": 0.12869934739717712,
      "grad_norm": 3.720524787902832,
      "learning_rate": 0.0002807567786321327,
      "loss": 0.1079,
      "step": 318
    },
    {
      "epoch": 0.12910406232609906,
      "grad_norm": 2.4771127700805664,
      "learning_rate": 0.0002806960744637798,
      "loss": 0.0607,
      "step": 319
    },
    {
      "epoch": 0.129508777255021,
      "grad_norm": 2.0596377849578857,
      "learning_rate": 0.0002806353702954269,
      "loss": 0.0914,
      "step": 320
    },
    {
      "epoch": 0.12991349218394294,
      "grad_norm": 3.249055862426758,
      "learning_rate": 0.00028057466612707403,
      "loss": 0.1713,
      "step": 321
    },
    {
      "epoch": 0.13031820711286488,
      "grad_norm": 3.166719913482666,
      "learning_rate": 0.00028051396195872115,
      "loss": 0.1331,
      "step": 322
    },
    {
      "epoch": 0.13072292204178682,
      "grad_norm": 1.5116783380508423,
      "learning_rate": 0.0002804532577903682,
      "loss": 0.0755,
      "step": 323
    },
    {
      "epoch": 0.13112763697070876,
      "grad_norm": 2.4522571563720703,
      "learning_rate": 0.00028039255362201533,
      "loss": 0.1389,
      "step": 324
    },
    {
      "epoch": 0.1315323518996307,
      "grad_norm": 2.7383999824523926,
      "learning_rate": 0.00028033184945366245,
      "loss": 0.0725,
      "step": 325
    },
    {
      "epoch": 0.13193706682855263,
      "grad_norm": 2.521043300628662,
      "learning_rate": 0.00028027114528530957,
      "loss": 0.0936,
      "step": 326
    },
    {
      "epoch": 0.13234178175747457,
      "grad_norm": 3.752385139465332,
      "learning_rate": 0.0002802104411169567,
      "loss": 0.2433,
      "step": 327
    },
    {
      "epoch": 0.13274649668639651,
      "grad_norm": 1.9226700067520142,
      "learning_rate": 0.00028014973694860375,
      "loss": 0.0241,
      "step": 328
    },
    {
      "epoch": 0.13315121161531845,
      "grad_norm": 1.9911634922027588,
      "learning_rate": 0.00028008903278025087,
      "loss": 0.1026,
      "step": 329
    },
    {
      "epoch": 0.1335559265442404,
      "grad_norm": 1.4710558652877808,
      "learning_rate": 0.000280028328611898,
      "loss": 0.042,
      "step": 330
    },
    {
      "epoch": 0.13396064147316233,
      "grad_norm": 2.489983320236206,
      "learning_rate": 0.0002799676244435451,
      "loss": 0.0817,
      "step": 331
    },
    {
      "epoch": 0.13436535640208427,
      "grad_norm": 2.3402390480041504,
      "learning_rate": 0.0002799069202751922,
      "loss": 0.0685,
      "step": 332
    },
    {
      "epoch": 0.1347700713310062,
      "grad_norm": 2.9636166095733643,
      "learning_rate": 0.00027984621610683934,
      "loss": 0.0894,
      "step": 333
    },
    {
      "epoch": 0.13517478625992815,
      "grad_norm": 2.530822992324829,
      "learning_rate": 0.0002797855119384864,
      "loss": 0.0836,
      "step": 334
    },
    {
      "epoch": 0.1355795011888501,
      "grad_norm": 3.139371633529663,
      "learning_rate": 0.0002797248077701335,
      "loss": 0.147,
      "step": 335
    },
    {
      "epoch": 0.13598421611777203,
      "grad_norm": 6.313640117645264,
      "learning_rate": 0.00027966410360178064,
      "loss": 0.2508,
      "step": 336
    },
    {
      "epoch": 0.13638893104669397,
      "grad_norm": 3.934973955154419,
      "learning_rate": 0.00027960339943342775,
      "loss": 0.1424,
      "step": 337
    },
    {
      "epoch": 0.1367936459756159,
      "grad_norm": 4.124831676483154,
      "learning_rate": 0.00027954269526507487,
      "loss": 0.096,
      "step": 338
    },
    {
      "epoch": 0.13719836090453785,
      "grad_norm": 4.237881183624268,
      "learning_rate": 0.00027948199109672193,
      "loss": 0.0925,
      "step": 339
    },
    {
      "epoch": 0.13760307583345982,
      "grad_norm": 4.83750057220459,
      "learning_rate": 0.00027942128692836905,
      "loss": 0.1266,
      "step": 340
    },
    {
      "epoch": 0.13800779076238176,
      "grad_norm": 2.890538215637207,
      "learning_rate": 0.00027936058276001617,
      "loss": 0.0605,
      "step": 341
    },
    {
      "epoch": 0.1384125056913037,
      "grad_norm": 4.63137149810791,
      "learning_rate": 0.0002792998785916633,
      "loss": 0.1271,
      "step": 342
    },
    {
      "epoch": 0.13881722062022564,
      "grad_norm": 5.854705810546875,
      "learning_rate": 0.0002792391744233104,
      "loss": 0.1371,
      "step": 343
    },
    {
      "epoch": 0.13922193554914758,
      "grad_norm": 2.4090335369110107,
      "learning_rate": 0.0002791784702549575,
      "loss": 0.0664,
      "step": 344
    },
    {
      "epoch": 0.13962665047806952,
      "grad_norm": 4.011292457580566,
      "learning_rate": 0.0002791177660866046,
      "loss": 0.162,
      "step": 345
    },
    {
      "epoch": 0.14003136540699146,
      "grad_norm": 3.60577654838562,
      "learning_rate": 0.0002790570619182517,
      "loss": 0.1197,
      "step": 346
    },
    {
      "epoch": 0.1404360803359134,
      "grad_norm": 1.438032865524292,
      "learning_rate": 0.0002789963577498988,
      "loss": 0.0824,
      "step": 347
    },
    {
      "epoch": 0.14084079526483534,
      "grad_norm": 5.385595798492432,
      "learning_rate": 0.00027893565358154594,
      "loss": 0.1893,
      "step": 348
    },
    {
      "epoch": 0.14124551019375728,
      "grad_norm": 2.1300671100616455,
      "learning_rate": 0.000278874949413193,
      "loss": 0.0521,
      "step": 349
    },
    {
      "epoch": 0.14165022512267922,
      "grad_norm": 4.512085437774658,
      "learning_rate": 0.0002788142452448401,
      "loss": 0.191,
      "step": 350
    },
    {
      "epoch": 0.14205494005160116,
      "grad_norm": 3.2441046237945557,
      "learning_rate": 0.00027875354107648724,
      "loss": 0.0926,
      "step": 351
    },
    {
      "epoch": 0.1424596549805231,
      "grad_norm": 3.658827543258667,
      "learning_rate": 0.00027869283690813435,
      "loss": 0.1259,
      "step": 352
    },
    {
      "epoch": 0.14286436990944504,
      "grad_norm": 2.6578879356384277,
      "learning_rate": 0.0002786321327397814,
      "loss": 0.0806,
      "step": 353
    },
    {
      "epoch": 0.14326908483836698,
      "grad_norm": 4.049111843109131,
      "learning_rate": 0.00027857142857142854,
      "loss": 0.1403,
      "step": 354
    },
    {
      "epoch": 0.14367379976728892,
      "grad_norm": 1.0954108238220215,
      "learning_rate": 0.00027851072440307565,
      "loss": 0.0688,
      "step": 355
    },
    {
      "epoch": 0.14407851469621086,
      "grad_norm": 2.197160243988037,
      "learning_rate": 0.00027845002023472277,
      "loss": 0.0407,
      "step": 356
    },
    {
      "epoch": 0.1444832296251328,
      "grad_norm": 3.5012710094451904,
      "learning_rate": 0.00027838931606636983,
      "loss": 0.0976,
      "step": 357
    },
    {
      "epoch": 0.14488794455405474,
      "grad_norm": 2.682953357696533,
      "learning_rate": 0.00027832861189801695,
      "loss": 0.0795,
      "step": 358
    },
    {
      "epoch": 0.14529265948297668,
      "grad_norm": 3.0914454460144043,
      "learning_rate": 0.00027826790772966407,
      "loss": 0.2154,
      "step": 359
    },
    {
      "epoch": 0.14569737441189862,
      "grad_norm": 2.6187703609466553,
      "learning_rate": 0.0002782072035613112,
      "loss": 0.0771,
      "step": 360
    },
    {
      "epoch": 0.14610208934082056,
      "grad_norm": 3.5185611248016357,
      "learning_rate": 0.0002781464993929583,
      "loss": 0.1236,
      "step": 361
    },
    {
      "epoch": 0.1465068042697425,
      "grad_norm": 2.529086112976074,
      "learning_rate": 0.00027808579522460537,
      "loss": 0.0821,
      "step": 362
    },
    {
      "epoch": 0.14691151919866444,
      "grad_norm": 5.932543754577637,
      "learning_rate": 0.0002780250910562525,
      "loss": 0.107,
      "step": 363
    },
    {
      "epoch": 0.14731623412758638,
      "grad_norm": 2.596850633621216,
      "learning_rate": 0.0002779643868878996,
      "loss": 0.0762,
      "step": 364
    },
    {
      "epoch": 0.14772094905650832,
      "grad_norm": 3.15626859664917,
      "learning_rate": 0.0002779036827195467,
      "loss": 0.059,
      "step": 365
    },
    {
      "epoch": 0.14812566398543026,
      "grad_norm": 6.449612617492676,
      "learning_rate": 0.00027784297855119384,
      "loss": 0.2244,
      "step": 366
    },
    {
      "epoch": 0.1485303789143522,
      "grad_norm": 3.2189977169036865,
      "learning_rate": 0.0002777822743828409,
      "loss": 0.1483,
      "step": 367
    },
    {
      "epoch": 0.14893509384327414,
      "grad_norm": 2.202695369720459,
      "learning_rate": 0.000277721570214488,
      "loss": 0.0746,
      "step": 368
    },
    {
      "epoch": 0.14933980877219608,
      "grad_norm": 3.9910550117492676,
      "learning_rate": 0.00027766086604613514,
      "loss": 0.1923,
      "step": 369
    },
    {
      "epoch": 0.14974452370111802,
      "grad_norm": 2.6243598461151123,
      "learning_rate": 0.00027760016187778225,
      "loss": 0.0963,
      "step": 370
    },
    {
      "epoch": 0.15014923863003996,
      "grad_norm": 5.5005059242248535,
      "learning_rate": 0.00027753945770942937,
      "loss": 0.1148,
      "step": 371
    },
    {
      "epoch": 0.1505539535589619,
      "grad_norm": 5.686526775360107,
      "learning_rate": 0.00027747875354107644,
      "loss": 0.111,
      "step": 372
    },
    {
      "epoch": 0.15095866848788383,
      "grad_norm": 2.0172317028045654,
      "learning_rate": 0.00027741804937272355,
      "loss": 0.1047,
      "step": 373
    },
    {
      "epoch": 0.15136338341680577,
      "grad_norm": 4.811121940612793,
      "learning_rate": 0.00027735734520437067,
      "loss": 0.1408,
      "step": 374
    },
    {
      "epoch": 0.15176809834572771,
      "grad_norm": 6.0056562423706055,
      "learning_rate": 0.0002772966410360178,
      "loss": 0.1913,
      "step": 375
    },
    {
      "epoch": 0.15217281327464968,
      "grad_norm": 5.8603105545043945,
      "learning_rate": 0.0002772359368676649,
      "loss": 0.1104,
      "step": 376
    },
    {
      "epoch": 0.15257752820357162,
      "grad_norm": 2.8822240829467773,
      "learning_rate": 0.000277175232699312,
      "loss": 0.1026,
      "step": 377
    },
    {
      "epoch": 0.15298224313249356,
      "grad_norm": 3.7826108932495117,
      "learning_rate": 0.0002771145285309591,
      "loss": 0.1676,
      "step": 378
    },
    {
      "epoch": 0.1533869580614155,
      "grad_norm": 4.296358108520508,
      "learning_rate": 0.0002770538243626062,
      "loss": 0.113,
      "step": 379
    },
    {
      "epoch": 0.15379167299033744,
      "grad_norm": 3.6633360385894775,
      "learning_rate": 0.0002769931201942533,
      "loss": 0.1273,
      "step": 380
    },
    {
      "epoch": 0.15419638791925938,
      "grad_norm": 3.3765106201171875,
      "learning_rate": 0.00027693241602590044,
      "loss": 0.0513,
      "step": 381
    },
    {
      "epoch": 0.15460110284818132,
      "grad_norm": 5.547400951385498,
      "learning_rate": 0.00027687171185754756,
      "loss": 0.134,
      "step": 382
    },
    {
      "epoch": 0.15500581777710326,
      "grad_norm": 3.0699799060821533,
      "learning_rate": 0.0002768110076891946,
      "loss": 0.0772,
      "step": 383
    },
    {
      "epoch": 0.1554105327060252,
      "grad_norm": 3.8581759929656982,
      "learning_rate": 0.00027675030352084174,
      "loss": 0.0703,
      "step": 384
    },
    {
      "epoch": 0.15581524763494714,
      "grad_norm": 1.396852970123291,
      "learning_rate": 0.00027668959935248886,
      "loss": 0.0523,
      "step": 385
    },
    {
      "epoch": 0.15621996256386908,
      "grad_norm": 2.8765923976898193,
      "learning_rate": 0.000276628895184136,
      "loss": 0.104,
      "step": 386
    },
    {
      "epoch": 0.15662467749279102,
      "grad_norm": 3.3078901767730713,
      "learning_rate": 0.0002765681910157831,
      "loss": 0.0422,
      "step": 387
    },
    {
      "epoch": 0.15702939242171296,
      "grad_norm": 3.2828822135925293,
      "learning_rate": 0.00027650748684743015,
      "loss": 0.0962,
      "step": 388
    },
    {
      "epoch": 0.1574341073506349,
      "grad_norm": 2.7981252670288086,
      "learning_rate": 0.00027644678267907727,
      "loss": 0.1336,
      "step": 389
    },
    {
      "epoch": 0.15783882227955684,
      "grad_norm": 4.080057144165039,
      "learning_rate": 0.0002763860785107244,
      "loss": 0.1706,
      "step": 390
    },
    {
      "epoch": 0.15824353720847878,
      "grad_norm": 3.550365924835205,
      "learning_rate": 0.0002763253743423715,
      "loss": 0.1029,
      "step": 391
    },
    {
      "epoch": 0.15864825213740072,
      "grad_norm": 3.0231423377990723,
      "learning_rate": 0.0002762646701740186,
      "loss": 0.0781,
      "step": 392
    },
    {
      "epoch": 0.15905296706632266,
      "grad_norm": 3.1159932613372803,
      "learning_rate": 0.0002762039660056657,
      "loss": 0.159,
      "step": 393
    },
    {
      "epoch": 0.1594576819952446,
      "grad_norm": 1.7134205102920532,
      "learning_rate": 0.0002761432618373128,
      "loss": 0.0552,
      "step": 394
    },
    {
      "epoch": 0.15986239692416654,
      "grad_norm": 3.7963364124298096,
      "learning_rate": 0.0002760825576689599,
      "loss": 0.2427,
      "step": 395
    },
    {
      "epoch": 0.16026711185308848,
      "grad_norm": 2.6211318969726562,
      "learning_rate": 0.00027602185350060704,
      "loss": 0.1639,
      "step": 396
    },
    {
      "epoch": 0.16067182678201042,
      "grad_norm": 3.5421719551086426,
      "learning_rate": 0.0002759611493322541,
      "loss": 0.151,
      "step": 397
    },
    {
      "epoch": 0.16107654171093236,
      "grad_norm": 2.5676448345184326,
      "learning_rate": 0.0002759004451639012,
      "loss": 0.1605,
      "step": 398
    },
    {
      "epoch": 0.1614812566398543,
      "grad_norm": 2.272326707839966,
      "learning_rate": 0.00027583974099554834,
      "loss": 0.1267,
      "step": 399
    },
    {
      "epoch": 0.16188597156877624,
      "grad_norm": 1.5537323951721191,
      "learning_rate": 0.00027577903682719546,
      "loss": 0.0688,
      "step": 400
    },
    {
      "epoch": 0.16229068649769818,
      "grad_norm": 3.861201524734497,
      "learning_rate": 0.0002757183326588425,
      "loss": 0.1398,
      "step": 401
    },
    {
      "epoch": 0.16269540142662012,
      "grad_norm": 3.0121212005615234,
      "learning_rate": 0.00027565762849048964,
      "loss": 0.1773,
      "step": 402
    },
    {
      "epoch": 0.16310011635554206,
      "grad_norm": 1.521546721458435,
      "learning_rate": 0.00027559692432213676,
      "loss": 0.0609,
      "step": 403
    },
    {
      "epoch": 0.163504831284464,
      "grad_norm": 2.0153939723968506,
      "learning_rate": 0.0002755362201537839,
      "loss": 0.1163,
      "step": 404
    },
    {
      "epoch": 0.16390954621338594,
      "grad_norm": 2.912531614303589,
      "learning_rate": 0.00027547551598543094,
      "loss": 0.0834,
      "step": 405
    },
    {
      "epoch": 0.16431426114230788,
      "grad_norm": 1.550134539604187,
      "learning_rate": 0.00027541481181707806,
      "loss": 0.0533,
      "step": 406
    },
    {
      "epoch": 0.16471897607122982,
      "grad_norm": 3.3269362449645996,
      "learning_rate": 0.00027535410764872517,
      "loss": 0.1458,
      "step": 407
    },
    {
      "epoch": 0.16512369100015176,
      "grad_norm": 3.346137285232544,
      "learning_rate": 0.0002752934034803723,
      "loss": 0.1496,
      "step": 408
    },
    {
      "epoch": 0.1655284059290737,
      "grad_norm": 3.9966962337493896,
      "learning_rate": 0.0002752326993120194,
      "loss": 0.0474,
      "step": 409
    },
    {
      "epoch": 0.16593312085799564,
      "grad_norm": 4.20927619934082,
      "learning_rate": 0.0002751719951436665,
      "loss": 0.1097,
      "step": 410
    },
    {
      "epoch": 0.16633783578691758,
      "grad_norm": 1.8240983486175537,
      "learning_rate": 0.0002751112909753136,
      "loss": 0.0735,
      "step": 411
    },
    {
      "epoch": 0.16674255071583954,
      "grad_norm": 1.077996850013733,
      "learning_rate": 0.0002750505868069607,
      "loss": 0.0813,
      "step": 412
    },
    {
      "epoch": 0.16714726564476148,
      "grad_norm": 1.6110378503799438,
      "learning_rate": 0.0002749898826386078,
      "loss": 0.0601,
      "step": 413
    },
    {
      "epoch": 0.16755198057368342,
      "grad_norm": 0.8898118138313293,
      "learning_rate": 0.00027492917847025494,
      "loss": 0.0508,
      "step": 414
    },
    {
      "epoch": 0.16795669550260536,
      "grad_norm": 2.6752827167510986,
      "learning_rate": 0.00027486847430190206,
      "loss": 0.093,
      "step": 415
    },
    {
      "epoch": 0.1683614104315273,
      "grad_norm": 3.439565896987915,
      "learning_rate": 0.0002748077701335491,
      "loss": 0.0749,
      "step": 416
    },
    {
      "epoch": 0.16876612536044924,
      "grad_norm": 2.5943398475646973,
      "learning_rate": 0.00027474706596519624,
      "loss": 0.0869,
      "step": 417
    },
    {
      "epoch": 0.16917084028937118,
      "grad_norm": 1.8742817640304565,
      "learning_rate": 0.00027468636179684336,
      "loss": 0.0488,
      "step": 418
    },
    {
      "epoch": 0.16957555521829312,
      "grad_norm": 5.809864521026611,
      "learning_rate": 0.0002746256576284905,
      "loss": 0.1058,
      "step": 419
    },
    {
      "epoch": 0.16998027014721506,
      "grad_norm": 3.4914708137512207,
      "learning_rate": 0.0002745649534601376,
      "loss": 0.1135,
      "step": 420
    },
    {
      "epoch": 0.170384985076137,
      "grad_norm": 4.4492621421813965,
      "learning_rate": 0.0002745042492917847,
      "loss": 0.1664,
      "step": 421
    },
    {
      "epoch": 0.17078970000505894,
      "grad_norm": 5.632392406463623,
      "learning_rate": 0.0002744435451234318,
      "loss": 0.2339,
      "step": 422
    },
    {
      "epoch": 0.17119441493398088,
      "grad_norm": 2.902240514755249,
      "learning_rate": 0.0002743828409550789,
      "loss": 0.095,
      "step": 423
    },
    {
      "epoch": 0.17159912986290282,
      "grad_norm": 4.838000774383545,
      "learning_rate": 0.000274322136786726,
      "loss": 0.1371,
      "step": 424
    },
    {
      "epoch": 0.17200384479182476,
      "grad_norm": 3.8460381031036377,
      "learning_rate": 0.00027426143261837313,
      "loss": 0.2259,
      "step": 425
    },
    {
      "epoch": 0.1724085597207467,
      "grad_norm": 7.997128009796143,
      "learning_rate": 0.00027420072845002024,
      "loss": 0.2595,
      "step": 426
    },
    {
      "epoch": 0.17281327464966864,
      "grad_norm": 7.859138011932373,
      "learning_rate": 0.0002741400242816673,
      "loss": 0.1982,
      "step": 427
    },
    {
      "epoch": 0.17321798957859058,
      "grad_norm": 2.619802474975586,
      "learning_rate": 0.0002740793201133144,
      "loss": 0.1602,
      "step": 428
    },
    {
      "epoch": 0.17362270450751252,
      "grad_norm": 4.771603107452393,
      "learning_rate": 0.00027401861594496154,
      "loss": 0.177,
      "step": 429
    },
    {
      "epoch": 0.17402741943643446,
      "grad_norm": 3.9424209594726562,
      "learning_rate": 0.00027395791177660866,
      "loss": 0.158,
      "step": 430
    },
    {
      "epoch": 0.1744321343653564,
      "grad_norm": 1.7082382440567017,
      "learning_rate": 0.0002738972076082558,
      "loss": 0.0807,
      "step": 431
    },
    {
      "epoch": 0.17483684929427834,
      "grad_norm": 4.719090461730957,
      "learning_rate": 0.00027383650343990284,
      "loss": 0.1238,
      "step": 432
    },
    {
      "epoch": 0.17524156422320028,
      "grad_norm": 1.2783983945846558,
      "learning_rate": 0.00027377579927154996,
      "loss": 0.0456,
      "step": 433
    },
    {
      "epoch": 0.17564627915212222,
      "grad_norm": 2.512037754058838,
      "learning_rate": 0.0002737150951031971,
      "loss": 0.1484,
      "step": 434
    },
    {
      "epoch": 0.17605099408104416,
      "grad_norm": 2.5887913703918457,
      "learning_rate": 0.0002736543909348442,
      "loss": 0.2429,
      "step": 435
    },
    {
      "epoch": 0.1764557090099661,
      "grad_norm": 1.0190361738204956,
      "learning_rate": 0.00027359368676649126,
      "loss": 0.0446,
      "step": 436
    },
    {
      "epoch": 0.17686042393888804,
      "grad_norm": 3.3471906185150146,
      "learning_rate": 0.0002735329825981384,
      "loss": 0.1205,
      "step": 437
    },
    {
      "epoch": 0.17726513886780998,
      "grad_norm": 1.2631862163543701,
      "learning_rate": 0.0002734722784297855,
      "loss": 0.0365,
      "step": 438
    },
    {
      "epoch": 0.17766985379673192,
      "grad_norm": 1.2540736198425293,
      "learning_rate": 0.0002734115742614326,
      "loss": 0.0975,
      "step": 439
    },
    {
      "epoch": 0.17807456872565386,
      "grad_norm": 7.340885639190674,
      "learning_rate": 0.0002733508700930797,
      "loss": 0.1972,
      "step": 440
    },
    {
      "epoch": 0.1784792836545758,
      "grad_norm": 5.344380855560303,
      "learning_rate": 0.0002732901659247268,
      "loss": 0.1781,
      "step": 441
    },
    {
      "epoch": 0.17888399858349774,
      "grad_norm": 3.576605796813965,
      "learning_rate": 0.0002732294617563739,
      "loss": 0.0713,
      "step": 442
    },
    {
      "epoch": 0.17928871351241968,
      "grad_norm": 2.525420904159546,
      "learning_rate": 0.00027316875758802103,
      "loss": 0.1274,
      "step": 443
    },
    {
      "epoch": 0.17969342844134162,
      "grad_norm": 1.7501996755599976,
      "learning_rate": 0.0002731080534196681,
      "loss": 0.0688,
      "step": 444
    },
    {
      "epoch": 0.18009814337026356,
      "grad_norm": 0.9865562319755554,
      "learning_rate": 0.0002730473492513152,
      "loss": 0.0233,
      "step": 445
    },
    {
      "epoch": 0.1805028582991855,
      "grad_norm": 3.0894370079040527,
      "learning_rate": 0.0002729866450829623,
      "loss": 0.1267,
      "step": 446
    },
    {
      "epoch": 0.18090757322810747,
      "grad_norm": 3.014370918273926,
      "learning_rate": 0.00027292594091460944,
      "loss": 0.0761,
      "step": 447
    },
    {
      "epoch": 0.1813122881570294,
      "grad_norm": 4.124777317047119,
      "learning_rate": 0.00027286523674625656,
      "loss": 0.1645,
      "step": 448
    },
    {
      "epoch": 0.18171700308595135,
      "grad_norm": 4.623742580413818,
      "learning_rate": 0.0002728045325779036,
      "loss": 0.0879,
      "step": 449
    },
    {
      "epoch": 0.18212171801487329,
      "grad_norm": 1.951249599456787,
      "learning_rate": 0.00027274382840955074,
      "loss": 0.0533,
      "step": 450
    },
    {
      "epoch": 0.18252643294379522,
      "grad_norm": 1.8991986513137817,
      "learning_rate": 0.00027268312424119786,
      "loss": 0.1064,
      "step": 451
    },
    {
      "epoch": 0.18293114787271716,
      "grad_norm": 3.189387798309326,
      "learning_rate": 0.000272622420072845,
      "loss": 0.1221,
      "step": 452
    },
    {
      "epoch": 0.1833358628016391,
      "grad_norm": 1.316253900527954,
      "learning_rate": 0.0002725617159044921,
      "loss": 0.0245,
      "step": 453
    },
    {
      "epoch": 0.18374057773056104,
      "grad_norm": 2.606898784637451,
      "learning_rate": 0.0002725010117361392,
      "loss": 0.0803,
      "step": 454
    },
    {
      "epoch": 0.18414529265948298,
      "grad_norm": 1.5452674627304077,
      "learning_rate": 0.0002724403075677863,
      "loss": 0.0949,
      "step": 455
    },
    {
      "epoch": 0.18455000758840492,
      "grad_norm": 3.9350178241729736,
      "learning_rate": 0.0002723796033994334,
      "loss": 0.115,
      "step": 456
    },
    {
      "epoch": 0.18495472251732686,
      "grad_norm": 2.435410499572754,
      "learning_rate": 0.0002723188992310805,
      "loss": 0.0866,
      "step": 457
    },
    {
      "epoch": 0.1853594374462488,
      "grad_norm": 2.578739643096924,
      "learning_rate": 0.00027225819506272763,
      "loss": 0.1049,
      "step": 458
    },
    {
      "epoch": 0.18576415237517074,
      "grad_norm": 2.21305513381958,
      "learning_rate": 0.00027219749089437475,
      "loss": 0.1446,
      "step": 459
    },
    {
      "epoch": 0.18616886730409268,
      "grad_norm": 1.87655770778656,
      "learning_rate": 0.0002721367867260218,
      "loss": 0.107,
      "step": 460
    },
    {
      "epoch": 0.18657358223301462,
      "grad_norm": 2.297621965408325,
      "learning_rate": 0.00027207608255766893,
      "loss": 0.0713,
      "step": 461
    },
    {
      "epoch": 0.18697829716193656,
      "grad_norm": 2.4779229164123535,
      "learning_rate": 0.00027201537838931605,
      "loss": 0.1349,
      "step": 462
    },
    {
      "epoch": 0.1873830120908585,
      "grad_norm": 6.737192630767822,
      "learning_rate": 0.00027195467422096316,
      "loss": 0.1276,
      "step": 463
    },
    {
      "epoch": 0.18778772701978044,
      "grad_norm": 3.802053928375244,
      "learning_rate": 0.0002718939700526103,
      "loss": 0.1005,
      "step": 464
    },
    {
      "epoch": 0.18819244194870238,
      "grad_norm": 2.985938310623169,
      "learning_rate": 0.00027183326588425734,
      "loss": 0.124,
      "step": 465
    },
    {
      "epoch": 0.18859715687762432,
      "grad_norm": 3.0849454402923584,
      "learning_rate": 0.00027177256171590446,
      "loss": 0.0931,
      "step": 466
    },
    {
      "epoch": 0.18900187180654626,
      "grad_norm": 3.039863109588623,
      "learning_rate": 0.0002717118575475516,
      "loss": 0.0928,
      "step": 467
    },
    {
      "epoch": 0.1894065867354682,
      "grad_norm": 1.1530342102050781,
      "learning_rate": 0.0002716511533791987,
      "loss": 0.062,
      "step": 468
    },
    {
      "epoch": 0.18981130166439014,
      "grad_norm": 4.7942352294921875,
      "learning_rate": 0.0002715904492108458,
      "loss": 0.1475,
      "step": 469
    },
    {
      "epoch": 0.19021601659331208,
      "grad_norm": 2.747978687286377,
      "learning_rate": 0.00027152974504249293,
      "loss": 0.1443,
      "step": 470
    },
    {
      "epoch": 0.19062073152223402,
      "grad_norm": 0.6733910441398621,
      "learning_rate": 0.00027146904087414,
      "loss": 0.0637,
      "step": 471
    },
    {
      "epoch": 0.19102544645115596,
      "grad_norm": 1.0628196001052856,
      "learning_rate": 0.0002714083367057871,
      "loss": 0.0441,
      "step": 472
    },
    {
      "epoch": 0.1914301613800779,
      "grad_norm": 1.0703516006469727,
      "learning_rate": 0.00027134763253743423,
      "loss": 0.0513,
      "step": 473
    },
    {
      "epoch": 0.19183487630899984,
      "grad_norm": 6.874579906463623,
      "learning_rate": 0.00027128692836908135,
      "loss": 0.1997,
      "step": 474
    },
    {
      "epoch": 0.19223959123792178,
      "grad_norm": 3.013037919998169,
      "learning_rate": 0.0002712262242007284,
      "loss": 0.0876,
      "step": 475
    },
    {
      "epoch": 0.19264430616684372,
      "grad_norm": 4.3804755210876465,
      "learning_rate": 0.00027116552003237553,
      "loss": 0.2049,
      "step": 476
    },
    {
      "epoch": 0.19304902109576566,
      "grad_norm": 1.9619086980819702,
      "learning_rate": 0.00027110481586402265,
      "loss": 0.0859,
      "step": 477
    },
    {
      "epoch": 0.1934537360246876,
      "grad_norm": 5.261214733123779,
      "learning_rate": 0.00027104411169566976,
      "loss": 0.1629,
      "step": 478
    },
    {
      "epoch": 0.19385845095360954,
      "grad_norm": 3.518733024597168,
      "learning_rate": 0.00027098340752731683,
      "loss": 0.1174,
      "step": 479
    },
    {
      "epoch": 0.19426316588253148,
      "grad_norm": 3.21321439743042,
      "learning_rate": 0.00027092270335896395,
      "loss": 0.0854,
      "step": 480
    },
    {
      "epoch": 0.19466788081145342,
      "grad_norm": 3.278394937515259,
      "learning_rate": 0.00027086199919061106,
      "loss": 0.0959,
      "step": 481
    },
    {
      "epoch": 0.19507259574037536,
      "grad_norm": 2.993218183517456,
      "learning_rate": 0.0002708012950222582,
      "loss": 0.087,
      "step": 482
    },
    {
      "epoch": 0.19547731066929733,
      "grad_norm": 6.297508239746094,
      "learning_rate": 0.00027074059085390524,
      "loss": 0.1187,
      "step": 483
    },
    {
      "epoch": 0.19588202559821927,
      "grad_norm": 5.257776737213135,
      "learning_rate": 0.00027067988668555236,
      "loss": 0.1683,
      "step": 484
    },
    {
      "epoch": 0.1962867405271412,
      "grad_norm": 4.182867527008057,
      "learning_rate": 0.0002706191825171995,
      "loss": 0.1157,
      "step": 485
    },
    {
      "epoch": 0.19669145545606315,
      "grad_norm": 2.830204963684082,
      "learning_rate": 0.0002705584783488466,
      "loss": 0.0738,
      "step": 486
    },
    {
      "epoch": 0.1970961703849851,
      "grad_norm": 2.3022170066833496,
      "learning_rate": 0.00027049777418049366,
      "loss": 0.0626,
      "step": 487
    },
    {
      "epoch": 0.19750088531390703,
      "grad_norm": 4.317059516906738,
      "learning_rate": 0.0002704370700121408,
      "loss": 0.1238,
      "step": 488
    },
    {
      "epoch": 0.19790560024282897,
      "grad_norm": 0.9580963850021362,
      "learning_rate": 0.0002703763658437879,
      "loss": 0.0293,
      "step": 489
    },
    {
      "epoch": 0.1983103151717509,
      "grad_norm": 2.298433542251587,
      "learning_rate": 0.000270315661675435,
      "loss": 0.0546,
      "step": 490
    },
    {
      "epoch": 0.19871503010067285,
      "grad_norm": 5.170107364654541,
      "learning_rate": 0.00027025495750708213,
      "loss": 0.1818,
      "step": 491
    },
    {
      "epoch": 0.19911974502959479,
      "grad_norm": 3.716085195541382,
      "learning_rate": 0.00027019425333872925,
      "loss": 0.1674,
      "step": 492
    },
    {
      "epoch": 0.19952445995851673,
      "grad_norm": 1.6659691333770752,
      "learning_rate": 0.0002701335491703763,
      "loss": 0.0518,
      "step": 493
    },
    {
      "epoch": 0.19992917488743867,
      "grad_norm": 5.620941638946533,
      "learning_rate": 0.00027007284500202343,
      "loss": 0.298,
      "step": 494
    },
    {
      "epoch": 0.2003338898163606,
      "grad_norm": 5.703207015991211,
      "learning_rate": 0.00027001214083367055,
      "loss": 0.1696,
      "step": 495
    },
    {
      "epoch": 0.20073860474528255,
      "grad_norm": 3.978991985321045,
      "learning_rate": 0.00026995143666531767,
      "loss": 0.1421,
      "step": 496
    },
    {
      "epoch": 0.20114331967420448,
      "grad_norm": 4.56838846206665,
      "learning_rate": 0.0002698907324969648,
      "loss": 0.1525,
      "step": 497
    },
    {
      "epoch": 0.20154803460312642,
      "grad_norm": 4.289068698883057,
      "learning_rate": 0.00026983002832861185,
      "loss": 0.1373,
      "step": 498
    },
    {
      "epoch": 0.20195274953204836,
      "grad_norm": 5.457456111907959,
      "learning_rate": 0.00026976932416025896,
      "loss": 0.1859,
      "step": 499
    },
    {
      "epoch": 0.2023574644609703,
      "grad_norm": 1.7716326713562012,
      "learning_rate": 0.0002697086199919061,
      "loss": 0.0999,
      "step": 500
    },
    {
      "epoch": 0.20276217938989224,
      "grad_norm": 2.600027561187744,
      "learning_rate": 0.0002696479158235532,
      "loss": 0.0592,
      "step": 501
    },
    {
      "epoch": 0.20316689431881418,
      "grad_norm": 2.299621820449829,
      "learning_rate": 0.0002695872116552003,
      "loss": 0.1205,
      "step": 502
    },
    {
      "epoch": 0.20357160924773612,
      "grad_norm": 5.770894527435303,
      "learning_rate": 0.00026952650748684743,
      "loss": 0.152,
      "step": 503
    },
    {
      "epoch": 0.20397632417665806,
      "grad_norm": 2.362837314605713,
      "learning_rate": 0.0002694658033184945,
      "loss": 0.0595,
      "step": 504
    },
    {
      "epoch": 0.20438103910558,
      "grad_norm": 0.5495297908782959,
      "learning_rate": 0.0002694050991501416,
      "loss": 0.0391,
      "step": 505
    },
    {
      "epoch": 0.20478575403450194,
      "grad_norm": 1.7252655029296875,
      "learning_rate": 0.00026934439498178873,
      "loss": 0.0547,
      "step": 506
    },
    {
      "epoch": 0.20519046896342388,
      "grad_norm": 0.9693864583969116,
      "learning_rate": 0.00026928369081343585,
      "loss": 0.0587,
      "step": 507
    },
    {
      "epoch": 0.20559518389234582,
      "grad_norm": 3.745774507522583,
      "learning_rate": 0.00026922298664508297,
      "loss": 0.0989,
      "step": 508
    },
    {
      "epoch": 0.20599989882126776,
      "grad_norm": 4.574748516082764,
      "learning_rate": 0.00026916228247673003,
      "loss": 0.2439,
      "step": 509
    },
    {
      "epoch": 0.2064046137501897,
      "grad_norm": 3.3530004024505615,
      "learning_rate": 0.00026910157830837715,
      "loss": 0.1417,
      "step": 510
    },
    {
      "epoch": 0.20680932867911164,
      "grad_norm": 1.5346707105636597,
      "learning_rate": 0.00026904087414002427,
      "loss": 0.0772,
      "step": 511
    },
    {
      "epoch": 0.20721404360803358,
      "grad_norm": 3.9384138584136963,
      "learning_rate": 0.0002689801699716714,
      "loss": 0.1365,
      "step": 512
    },
    {
      "epoch": 0.20761875853695552,
      "grad_norm": 2.272223949432373,
      "learning_rate": 0.0002689194658033185,
      "loss": 0.1138,
      "step": 513
    },
    {
      "epoch": 0.20802347346587746,
      "grad_norm": 2.0808680057525635,
      "learning_rate": 0.00026885876163496557,
      "loss": 0.0629,
      "step": 514
    },
    {
      "epoch": 0.2084281883947994,
      "grad_norm": 3.2383484840393066,
      "learning_rate": 0.0002687980574666127,
      "loss": 0.1735,
      "step": 515
    },
    {
      "epoch": 0.20883290332372134,
      "grad_norm": 2.857283592224121,
      "learning_rate": 0.0002687373532982598,
      "loss": 0.0847,
      "step": 516
    },
    {
      "epoch": 0.20923761825264328,
      "grad_norm": 3.5934221744537354,
      "learning_rate": 0.0002686766491299069,
      "loss": 0.0848,
      "step": 517
    },
    {
      "epoch": 0.20964233318156522,
      "grad_norm": 2.28646183013916,
      "learning_rate": 0.000268615944961554,
      "loss": 0.0903,
      "step": 518
    },
    {
      "epoch": 0.2100470481104872,
      "grad_norm": 2.5548105239868164,
      "learning_rate": 0.0002685552407932011,
      "loss": 0.1094,
      "step": 519
    },
    {
      "epoch": 0.21045176303940913,
      "grad_norm": 2.696115016937256,
      "learning_rate": 0.0002684945366248482,
      "loss": 0.0795,
      "step": 520
    },
    {
      "epoch": 0.21085647796833107,
      "grad_norm": 1.4219273328781128,
      "learning_rate": 0.00026843383245649533,
      "loss": 0.0819,
      "step": 521
    },
    {
      "epoch": 0.211261192897253,
      "grad_norm": 1.4120545387268066,
      "learning_rate": 0.00026837312828814245,
      "loss": 0.0819,
      "step": 522
    },
    {
      "epoch": 0.21166590782617495,
      "grad_norm": 2.899829149246216,
      "learning_rate": 0.0002683124241197895,
      "loss": 0.1728,
      "step": 523
    },
    {
      "epoch": 0.2120706227550969,
      "grad_norm": 1.093893051147461,
      "learning_rate": 0.00026825171995143663,
      "loss": 0.0879,
      "step": 524
    },
    {
      "epoch": 0.21247533768401883,
      "grad_norm": 3.6094655990600586,
      "learning_rate": 0.00026819101578308375,
      "loss": 0.1131,
      "step": 525
    },
    {
      "epoch": 0.21288005261294077,
      "grad_norm": 2.431572198867798,
      "learning_rate": 0.00026813031161473087,
      "loss": 0.0823,
      "step": 526
    },
    {
      "epoch": 0.2132847675418627,
      "grad_norm": 3.718907356262207,
      "learning_rate": 0.00026806960744637793,
      "loss": 0.1217,
      "step": 527
    },
    {
      "epoch": 0.21368948247078465,
      "grad_norm": 7.821251392364502,
      "learning_rate": 0.00026800890327802505,
      "loss": 0.1753,
      "step": 528
    },
    {
      "epoch": 0.2140941973997066,
      "grad_norm": 3.2628462314605713,
      "learning_rate": 0.00026794819910967217,
      "loss": 0.1148,
      "step": 529
    },
    {
      "epoch": 0.21449891232862853,
      "grad_norm": 1.543513536453247,
      "learning_rate": 0.0002678874949413193,
      "loss": 0.0363,
      "step": 530
    },
    {
      "epoch": 0.21490362725755047,
      "grad_norm": 1.7848228216171265,
      "learning_rate": 0.00026782679077296635,
      "loss": 0.0659,
      "step": 531
    },
    {
      "epoch": 0.2153083421864724,
      "grad_norm": 1.048367977142334,
      "learning_rate": 0.00026776608660461347,
      "loss": 0.0935,
      "step": 532
    },
    {
      "epoch": 0.21571305711539435,
      "grad_norm": 4.096813678741455,
      "learning_rate": 0.0002677053824362606,
      "loss": 0.1313,
      "step": 533
    },
    {
      "epoch": 0.2161177720443163,
      "grad_norm": 1.9734413623809814,
      "learning_rate": 0.0002676446782679077,
      "loss": 0.0597,
      "step": 534
    },
    {
      "epoch": 0.21652248697323823,
      "grad_norm": 3.501389265060425,
      "learning_rate": 0.0002675839740995548,
      "loss": 0.1125,
      "step": 535
    },
    {
      "epoch": 0.21692720190216017,
      "grad_norm": 4.881041049957275,
      "learning_rate": 0.00026752326993120194,
      "loss": 0.1644,
      "step": 536
    },
    {
      "epoch": 0.2173319168310821,
      "grad_norm": 3.6683545112609863,
      "learning_rate": 0.000267462565762849,
      "loss": 0.1432,
      "step": 537
    },
    {
      "epoch": 0.21773663176000405,
      "grad_norm": 2.944054365158081,
      "learning_rate": 0.0002674018615944961,
      "loss": 0.1959,
      "step": 538
    },
    {
      "epoch": 0.21814134668892599,
      "grad_norm": 2.8736281394958496,
      "learning_rate": 0.00026734115742614323,
      "loss": 0.1942,
      "step": 539
    },
    {
      "epoch": 0.21854606161784793,
      "grad_norm": 2.163905143737793,
      "learning_rate": 0.00026728045325779035,
      "loss": 0.0557,
      "step": 540
    },
    {
      "epoch": 0.21895077654676987,
      "grad_norm": 0.8499112129211426,
      "learning_rate": 0.00026721974908943747,
      "loss": 0.0507,
      "step": 541
    },
    {
      "epoch": 0.2193554914756918,
      "grad_norm": 3.1613166332244873,
      "learning_rate": 0.00026715904492108453,
      "loss": 0.1271,
      "step": 542
    },
    {
      "epoch": 0.21976020640461374,
      "grad_norm": 3.948817014694214,
      "learning_rate": 0.00026709834075273165,
      "loss": 0.1515,
      "step": 543
    },
    {
      "epoch": 0.22016492133353568,
      "grad_norm": 4.599306106567383,
      "learning_rate": 0.00026703763658437877,
      "loss": 0.131,
      "step": 544
    },
    {
      "epoch": 0.22056963626245762,
      "grad_norm": 3.8763973712921143,
      "learning_rate": 0.0002669769324160259,
      "loss": 0.151,
      "step": 545
    },
    {
      "epoch": 0.22097435119137956,
      "grad_norm": 1.5048625469207764,
      "learning_rate": 0.000266916228247673,
      "loss": 0.0959,
      "step": 546
    },
    {
      "epoch": 0.2213790661203015,
      "grad_norm": 1.6409722566604614,
      "learning_rate": 0.0002668555240793201,
      "loss": 0.1267,
      "step": 547
    },
    {
      "epoch": 0.22178378104922344,
      "grad_norm": 3.1427226066589355,
      "learning_rate": 0.0002667948199109672,
      "loss": 0.1149,
      "step": 548
    },
    {
      "epoch": 0.22218849597814538,
      "grad_norm": 1.368154764175415,
      "learning_rate": 0.0002667341157426143,
      "loss": 0.0739,
      "step": 549
    },
    {
      "epoch": 0.22259321090706732,
      "grad_norm": 2.076725482940674,
      "learning_rate": 0.0002666734115742614,
      "loss": 0.0595,
      "step": 550
    },
    {
      "epoch": 0.22299792583598926,
      "grad_norm": 4.479791641235352,
      "learning_rate": 0.00026661270740590854,
      "loss": 0.1686,
      "step": 551
    },
    {
      "epoch": 0.2234026407649112,
      "grad_norm": 1.1213568449020386,
      "learning_rate": 0.00026655200323755566,
      "loss": 0.0702,
      "step": 552
    },
    {
      "epoch": 0.22380735569383314,
      "grad_norm": 0.7592081427574158,
      "learning_rate": 0.0002664912990692027,
      "loss": 0.0511,
      "step": 553
    },
    {
      "epoch": 0.22421207062275508,
      "grad_norm": 2.2880985736846924,
      "learning_rate": 0.00026643059490084984,
      "loss": 0.068,
      "step": 554
    },
    {
      "epoch": 0.22461678555167705,
      "grad_norm": 0.9724892973899841,
      "learning_rate": 0.00026636989073249695,
      "loss": 0.0369,
      "step": 555
    },
    {
      "epoch": 0.225021500480599,
      "grad_norm": 2.511178970336914,
      "learning_rate": 0.00026630918656414407,
      "loss": 0.1233,
      "step": 556
    },
    {
      "epoch": 0.22542621540952093,
      "grad_norm": 4.88578462600708,
      "learning_rate": 0.0002662484823957912,
      "loss": 0.1132,
      "step": 557
    },
    {
      "epoch": 0.22583093033844287,
      "grad_norm": 0.7240115404129028,
      "learning_rate": 0.00026618777822743825,
      "loss": 0.0196,
      "step": 558
    },
    {
      "epoch": 0.2262356452673648,
      "grad_norm": 2.8223230838775635,
      "learning_rate": 0.00026612707405908537,
      "loss": 0.069,
      "step": 559
    },
    {
      "epoch": 0.22664036019628675,
      "grad_norm": 1.9789209365844727,
      "learning_rate": 0.0002660663698907325,
      "loss": 0.0766,
      "step": 560
    },
    {
      "epoch": 0.2270450751252087,
      "grad_norm": 3.0024890899658203,
      "learning_rate": 0.0002660056657223796,
      "loss": 0.0768,
      "step": 561
    },
    {
      "epoch": 0.22744979005413063,
      "grad_norm": 5.6888251304626465,
      "learning_rate": 0.00026594496155402667,
      "loss": 0.1812,
      "step": 562
    },
    {
      "epoch": 0.22785450498305257,
      "grad_norm": 2.5803794860839844,
      "learning_rate": 0.0002658842573856738,
      "loss": 0.0448,
      "step": 563
    },
    {
      "epoch": 0.2282592199119745,
      "grad_norm": 5.5587897300720215,
      "learning_rate": 0.0002658235532173209,
      "loss": 0.2426,
      "step": 564
    },
    {
      "epoch": 0.22866393484089645,
      "grad_norm": 4.043572902679443,
      "learning_rate": 0.000265762849048968,
      "loss": 0.0657,
      "step": 565
    },
    {
      "epoch": 0.2290686497698184,
      "grad_norm": 3.547858238220215,
      "learning_rate": 0.0002657021448806151,
      "loss": 0.1103,
      "step": 566
    },
    {
      "epoch": 0.22947336469874033,
      "grad_norm": 2.1000587940216064,
      "learning_rate": 0.0002656414407122622,
      "loss": 0.0364,
      "step": 567
    },
    {
      "epoch": 0.22987807962766227,
      "grad_norm": 3.18182635307312,
      "learning_rate": 0.0002655807365439093,
      "loss": 0.0847,
      "step": 568
    },
    {
      "epoch": 0.2302827945565842,
      "grad_norm": 3.042832374572754,
      "learning_rate": 0.00026552003237555644,
      "loss": 0.0881,
      "step": 569
    },
    {
      "epoch": 0.23068750948550615,
      "grad_norm": 7.196845054626465,
      "learning_rate": 0.0002654593282072035,
      "loss": 0.1982,
      "step": 570
    },
    {
      "epoch": 0.2310922244144281,
      "grad_norm": 3.6799511909484863,
      "learning_rate": 0.0002653986240388506,
      "loss": 0.0777,
      "step": 571
    },
    {
      "epoch": 0.23149693934335003,
      "grad_norm": 5.029986381530762,
      "learning_rate": 0.00026533791987049774,
      "loss": 0.1128,
      "step": 572
    },
    {
      "epoch": 0.23190165427227197,
      "grad_norm": 2.9395854473114014,
      "learning_rate": 0.00026527721570214485,
      "loss": 0.1095,
      "step": 573
    },
    {
      "epoch": 0.2323063692011939,
      "grad_norm": 2.3306407928466797,
      "learning_rate": 0.00026521651153379197,
      "loss": 0.1054,
      "step": 574
    },
    {
      "epoch": 0.23271108413011585,
      "grad_norm": 2.1407694816589355,
      "learning_rate": 0.00026515580736543904,
      "loss": 0.0752,
      "step": 575
    },
    {
      "epoch": 0.2331157990590378,
      "grad_norm": 2.367918014526367,
      "learning_rate": 0.00026509510319708615,
      "loss": 0.0982,
      "step": 576
    },
    {
      "epoch": 0.23352051398795973,
      "grad_norm": 0.6847609281539917,
      "learning_rate": 0.00026503439902873327,
      "loss": 0.022,
      "step": 577
    },
    {
      "epoch": 0.23392522891688167,
      "grad_norm": 1.8529480695724487,
      "learning_rate": 0.0002649736948603804,
      "loss": 0.0803,
      "step": 578
    },
    {
      "epoch": 0.2343299438458036,
      "grad_norm": 5.136591911315918,
      "learning_rate": 0.0002649129906920275,
      "loss": 0.18,
      "step": 579
    },
    {
      "epoch": 0.23473465877472555,
      "grad_norm": 4.384200096130371,
      "learning_rate": 0.0002648522865236746,
      "loss": 0.1196,
      "step": 580
    },
    {
      "epoch": 0.2351393737036475,
      "grad_norm": 2.0189671516418457,
      "learning_rate": 0.0002647915823553217,
      "loss": 0.055,
      "step": 581
    },
    {
      "epoch": 0.23554408863256943,
      "grad_norm": 2.4813427925109863,
      "learning_rate": 0.0002647308781869688,
      "loss": 0.1249,
      "step": 582
    },
    {
      "epoch": 0.23594880356149137,
      "grad_norm": 2.8510894775390625,
      "learning_rate": 0.0002646701740186159,
      "loss": 0.0625,
      "step": 583
    },
    {
      "epoch": 0.2363535184904133,
      "grad_norm": 3.6175355911254883,
      "learning_rate": 0.00026460946985026304,
      "loss": 0.2094,
      "step": 584
    },
    {
      "epoch": 0.23675823341933525,
      "grad_norm": 1.2290058135986328,
      "learning_rate": 0.00026454876568191016,
      "loss": 0.028,
      "step": 585
    },
    {
      "epoch": 0.23716294834825719,
      "grad_norm": 1.711179494857788,
      "learning_rate": 0.0002644880615135572,
      "loss": 0.1209,
      "step": 586
    },
    {
      "epoch": 0.23756766327717913,
      "grad_norm": 3.212615489959717,
      "learning_rate": 0.00026442735734520434,
      "loss": 0.1066,
      "step": 587
    },
    {
      "epoch": 0.23797237820610107,
      "grad_norm": 5.0382866859436035,
      "learning_rate": 0.00026436665317685146,
      "loss": 0.1713,
      "step": 588
    },
    {
      "epoch": 0.238377093135023,
      "grad_norm": 2.146634340286255,
      "learning_rate": 0.0002643059490084986,
      "loss": 0.0748,
      "step": 589
    },
    {
      "epoch": 0.23878180806394497,
      "grad_norm": 3.63802170753479,
      "learning_rate": 0.0002642452448401457,
      "loss": 0.1893,
      "step": 590
    },
    {
      "epoch": 0.2391865229928669,
      "grad_norm": 2.404250144958496,
      "learning_rate": 0.0002641845406717928,
      "loss": 0.0935,
      "step": 591
    },
    {
      "epoch": 0.23959123792178885,
      "grad_norm": 3.4540584087371826,
      "learning_rate": 0.00026412383650343987,
      "loss": 0.0903,
      "step": 592
    },
    {
      "epoch": 0.2399959528507108,
      "grad_norm": 2.9647743701934814,
      "learning_rate": 0.000264063132335087,
      "loss": 0.1008,
      "step": 593
    },
    {
      "epoch": 0.24040066777963273,
      "grad_norm": 5.8474931716918945,
      "learning_rate": 0.0002640024281667341,
      "loss": 0.0901,
      "step": 594
    },
    {
      "epoch": 0.24080538270855467,
      "grad_norm": 6.215998649597168,
      "learning_rate": 0.0002639417239983812,
      "loss": 0.1557,
      "step": 595
    },
    {
      "epoch": 0.2412100976374766,
      "grad_norm": 0.3361993134021759,
      "learning_rate": 0.00026388101983002834,
      "loss": 0.0324,
      "step": 596
    },
    {
      "epoch": 0.24161481256639855,
      "grad_norm": 4.59174108505249,
      "learning_rate": 0.0002638203156616754,
      "loss": 0.1036,
      "step": 597
    },
    {
      "epoch": 0.2420195274953205,
      "grad_norm": 7.750960350036621,
      "learning_rate": 0.0002637596114933225,
      "loss": 0.2325,
      "step": 598
    },
    {
      "epoch": 0.24242424242424243,
      "grad_norm": 6.0140814781188965,
      "learning_rate": 0.00026369890732496964,
      "loss": 0.1193,
      "step": 599
    },
    {
      "epoch": 0.24282895735316437,
      "grad_norm": 5.399631977081299,
      "learning_rate": 0.00026363820315661676,
      "loss": 0.1559,
      "step": 600
    },
    {
      "epoch": 0.2432336722820863,
      "grad_norm": 4.0109639167785645,
      "learning_rate": 0.0002635774989882638,
      "loss": 0.15,
      "step": 601
    },
    {
      "epoch": 0.24363838721100825,
      "grad_norm": 1.754660964012146,
      "learning_rate": 0.00026351679481991094,
      "loss": 0.0642,
      "step": 602
    },
    {
      "epoch": 0.2440431021399302,
      "grad_norm": 2.67401385307312,
      "learning_rate": 0.00026345609065155806,
      "loss": 0.1339,
      "step": 603
    },
    {
      "epoch": 0.24444781706885213,
      "grad_norm": 3.430460214614868,
      "learning_rate": 0.0002633953864832052,
      "loss": 0.0694,
      "step": 604
    },
    {
      "epoch": 0.24485253199777407,
      "grad_norm": 3.1631410121917725,
      "learning_rate": 0.00026333468231485224,
      "loss": 0.0817,
      "step": 605
    },
    {
      "epoch": 0.245257246926696,
      "grad_norm": 1.5680716037750244,
      "learning_rate": 0.00026327397814649936,
      "loss": 0.0869,
      "step": 606
    },
    {
      "epoch": 0.24566196185561795,
      "grad_norm": 1.8133474588394165,
      "learning_rate": 0.0002632132739781465,
      "loss": 0.0366,
      "step": 607
    },
    {
      "epoch": 0.2460666767845399,
      "grad_norm": 4.139424800872803,
      "learning_rate": 0.0002631525698097936,
      "loss": 0.1957,
      "step": 608
    },
    {
      "epoch": 0.24647139171346183,
      "grad_norm": 3.5786728858947754,
      "learning_rate": 0.00026309186564144065,
      "loss": 0.1375,
      "step": 609
    },
    {
      "epoch": 0.24687610664238377,
      "grad_norm": 2.3526601791381836,
      "learning_rate": 0.00026303116147308777,
      "loss": 0.0424,
      "step": 610
    },
    {
      "epoch": 0.2472808215713057,
      "grad_norm": 7.991869926452637,
      "learning_rate": 0.0002629704573047349,
      "loss": 0.1317,
      "step": 611
    },
    {
      "epoch": 0.24768553650022765,
      "grad_norm": 0.41563835740089417,
      "learning_rate": 0.000262909753136382,
      "loss": 0.0208,
      "step": 612
    },
    {
      "epoch": 0.2480902514291496,
      "grad_norm": 2.14486026763916,
      "learning_rate": 0.0002628490489680291,
      "loss": 0.0282,
      "step": 613
    },
    {
      "epoch": 0.24849496635807153,
      "grad_norm": 1.5336214303970337,
      "learning_rate": 0.0002627883447996762,
      "loss": 0.0441,
      "step": 614
    },
    {
      "epoch": 0.24889968128699347,
      "grad_norm": 4.668149948120117,
      "learning_rate": 0.0002627276406313233,
      "loss": 0.1049,
      "step": 615
    },
    {
      "epoch": 0.2493043962159154,
      "grad_norm": 3.330862522125244,
      "learning_rate": 0.0002626669364629704,
      "loss": 0.0929,
      "step": 616
    },
    {
      "epoch": 0.24970911114483735,
      "grad_norm": 1.0565226078033447,
      "learning_rate": 0.00026260623229461754,
      "loss": 0.0315,
      "step": 617
    },
    {
      "epoch": 0.2501138260737593,
      "grad_norm": 0.5940179228782654,
      "learning_rate": 0.00026254552812626466,
      "loss": 0.0604,
      "step": 618
    },
    {
      "epoch": 0.2505185410026812,
      "grad_norm": 2.863109827041626,
      "learning_rate": 0.0002624848239579117,
      "loss": 0.1425,
      "step": 619
    },
    {
      "epoch": 0.2509232559316032,
      "grad_norm": 2.621108293533325,
      "learning_rate": 0.00026242411978955884,
      "loss": 0.0562,
      "step": 620
    },
    {
      "epoch": 0.2513279708605251,
      "grad_norm": 1.3990925550460815,
      "learning_rate": 0.00026236341562120596,
      "loss": 0.0255,
      "step": 621
    },
    {
      "epoch": 0.2517326857894471,
      "grad_norm": 2.156027317047119,
      "learning_rate": 0.0002623027114528531,
      "loss": 0.0415,
      "step": 622
    },
    {
      "epoch": 0.252137400718369,
      "grad_norm": 2.068535327911377,
      "learning_rate": 0.0002622420072845002,
      "loss": 0.0498,
      "step": 623
    },
    {
      "epoch": 0.25254211564729095,
      "grad_norm": 2.4445457458496094,
      "learning_rate": 0.0002621813031161473,
      "loss": 0.0451,
      "step": 624
    },
    {
      "epoch": 0.25294683057621287,
      "grad_norm": 3.511944532394409,
      "learning_rate": 0.0002621205989477944,
      "loss": 0.0794,
      "step": 625
    },
    {
      "epoch": 0.25335154550513483,
      "grad_norm": 1.5723129510879517,
      "learning_rate": 0.0002620598947794415,
      "loss": 0.0177,
      "step": 626
    },
    {
      "epoch": 0.25375626043405675,
      "grad_norm": 2.7816264629364014,
      "learning_rate": 0.0002619991906110886,
      "loss": 0.105,
      "step": 627
    },
    {
      "epoch": 0.2541609753629787,
      "grad_norm": 2.2478444576263428,
      "learning_rate": 0.0002619384864427357,
      "loss": 0.1102,
      "step": 628
    },
    {
      "epoch": 0.2545656902919006,
      "grad_norm": 2.9563100337982178,
      "learning_rate": 0.00026187778227438284,
      "loss": 0.0798,
      "step": 629
    },
    {
      "epoch": 0.2549704052208226,
      "grad_norm": 3.2568602561950684,
      "learning_rate": 0.0002618170781060299,
      "loss": 0.0592,
      "step": 630
    },
    {
      "epoch": 0.2553751201497445,
      "grad_norm": 2.207505464553833,
      "learning_rate": 0.000261756373937677,
      "loss": 0.0697,
      "step": 631
    },
    {
      "epoch": 0.2557798350786665,
      "grad_norm": 2.9244742393493652,
      "learning_rate": 0.00026169566976932414,
      "loss": 0.1306,
      "step": 632
    },
    {
      "epoch": 0.2561845500075884,
      "grad_norm": 2.441668748855591,
      "learning_rate": 0.00026163496560097126,
      "loss": 0.0813,
      "step": 633
    },
    {
      "epoch": 0.25658926493651035,
      "grad_norm": 1.1639984846115112,
      "learning_rate": 0.0002615742614326184,
      "loss": 0.1197,
      "step": 634
    },
    {
      "epoch": 0.25699397986543226,
      "grad_norm": 3.119826078414917,
      "learning_rate": 0.0002615135572642655,
      "loss": 0.0702,
      "step": 635
    },
    {
      "epoch": 0.25739869479435423,
      "grad_norm": 1.8801697492599487,
      "learning_rate": 0.00026145285309591256,
      "loss": 0.0306,
      "step": 636
    },
    {
      "epoch": 0.25780340972327614,
      "grad_norm": 1.570396900177002,
      "learning_rate": 0.0002613921489275597,
      "loss": 0.0581,
      "step": 637
    },
    {
      "epoch": 0.2582081246521981,
      "grad_norm": 1.554215669631958,
      "learning_rate": 0.0002613314447592068,
      "loss": 0.0355,
      "step": 638
    },
    {
      "epoch": 0.25861283958112,
      "grad_norm": 2.2372562885284424,
      "learning_rate": 0.0002612707405908539,
      "loss": 0.1194,
      "step": 639
    },
    {
      "epoch": 0.259017554510042,
      "grad_norm": 4.589914798736572,
      "learning_rate": 0.000261210036422501,
      "loss": 0.1091,
      "step": 640
    },
    {
      "epoch": 0.2594222694389639,
      "grad_norm": 4.021381855010986,
      "learning_rate": 0.0002611493322541481,
      "loss": 0.1387,
      "step": 641
    },
    {
      "epoch": 0.25982698436788587,
      "grad_norm": 2.1416351795196533,
      "learning_rate": 0.0002610886280857952,
      "loss": 0.1676,
      "step": 642
    },
    {
      "epoch": 0.2602316992968078,
      "grad_norm": 1.7725156545639038,
      "learning_rate": 0.00026102792391744233,
      "loss": 0.0822,
      "step": 643
    },
    {
      "epoch": 0.26063641422572975,
      "grad_norm": 2.5529985427856445,
      "learning_rate": 0.0002609672197490894,
      "loss": 0.0691,
      "step": 644
    },
    {
      "epoch": 0.2610411291546517,
      "grad_norm": 4.417157173156738,
      "learning_rate": 0.0002609065155807365,
      "loss": 0.1994,
      "step": 645
    },
    {
      "epoch": 0.26144584408357363,
      "grad_norm": 3.635301113128662,
      "learning_rate": 0.00026084581141238363,
      "loss": 0.0643,
      "step": 646
    },
    {
      "epoch": 0.2618505590124956,
      "grad_norm": 2.932326316833496,
      "learning_rate": 0.00026078510724403074,
      "loss": 0.0735,
      "step": 647
    },
    {
      "epoch": 0.2622552739414175,
      "grad_norm": 2.091024160385132,
      "learning_rate": 0.0002607244030756778,
      "loss": 0.0667,
      "step": 648
    },
    {
      "epoch": 0.2626599888703395,
      "grad_norm": 3.1428325176239014,
      "learning_rate": 0.0002606636989073249,
      "loss": 0.1226,
      "step": 649
    },
    {
      "epoch": 0.2630647037992614,
      "grad_norm": 0.8838170766830444,
      "learning_rate": 0.00026060299473897204,
      "loss": 0.0712,
      "step": 650
    },
    {
      "epoch": 0.26346941872818336,
      "grad_norm": 2.056532382965088,
      "learning_rate": 0.00026054229057061916,
      "loss": 0.1156,
      "step": 651
    },
    {
      "epoch": 0.26387413365710527,
      "grad_norm": 1.9185444116592407,
      "learning_rate": 0.0002604815864022662,
      "loss": 0.0575,
      "step": 652
    },
    {
      "epoch": 0.26427884858602724,
      "grad_norm": 1.0717769861221313,
      "learning_rate": 0.00026042088223391334,
      "loss": 0.034,
      "step": 653
    },
    {
      "epoch": 0.26468356351494915,
      "grad_norm": 1.4275987148284912,
      "learning_rate": 0.00026036017806556046,
      "loss": 0.064,
      "step": 654
    },
    {
      "epoch": 0.2650882784438711,
      "grad_norm": 2.4045610427856445,
      "learning_rate": 0.0002602994738972076,
      "loss": 0.0562,
      "step": 655
    },
    {
      "epoch": 0.26549299337279303,
      "grad_norm": 1.641444444656372,
      "learning_rate": 0.0002602387697288547,
      "loss": 0.0325,
      "step": 656
    },
    {
      "epoch": 0.265897708301715,
      "grad_norm": 4.478362083435059,
      "learning_rate": 0.0002601780655605018,
      "loss": 0.0966,
      "step": 657
    },
    {
      "epoch": 0.2663024232306369,
      "grad_norm": 1.3990603685379028,
      "learning_rate": 0.0002601173613921489,
      "loss": 0.0309,
      "step": 658
    },
    {
      "epoch": 0.2667071381595589,
      "grad_norm": 1.1092135906219482,
      "learning_rate": 0.000260056657223796,
      "loss": 0.0404,
      "step": 659
    },
    {
      "epoch": 0.2671118530884808,
      "grad_norm": 2.8962466716766357,
      "learning_rate": 0.0002599959530554431,
      "loss": 0.0525,
      "step": 660
    },
    {
      "epoch": 0.26751656801740276,
      "grad_norm": 1.9062154293060303,
      "learning_rate": 0.00025993524888709023,
      "loss": 0.0628,
      "step": 661
    },
    {
      "epoch": 0.26792128294632467,
      "grad_norm": 2.4244253635406494,
      "learning_rate": 0.00025987454471873735,
      "loss": 0.0627,
      "step": 662
    },
    {
      "epoch": 0.26832599787524664,
      "grad_norm": 7.148367404937744,
      "learning_rate": 0.0002598138405503844,
      "loss": 0.0935,
      "step": 663
    },
    {
      "epoch": 0.26873071280416855,
      "grad_norm": 4.200111389160156,
      "learning_rate": 0.00025975313638203153,
      "loss": 0.0859,
      "step": 664
    },
    {
      "epoch": 0.2691354277330905,
      "grad_norm": 2.034092426300049,
      "learning_rate": 0.00025969243221367865,
      "loss": 0.0666,
      "step": 665
    },
    {
      "epoch": 0.2695401426620124,
      "grad_norm": 1.7169662714004517,
      "learning_rate": 0.00025963172804532576,
      "loss": 0.1024,
      "step": 666
    },
    {
      "epoch": 0.2699448575909344,
      "grad_norm": 3.522824764251709,
      "learning_rate": 0.0002595710238769729,
      "loss": 0.1642,
      "step": 667
    },
    {
      "epoch": 0.2703495725198563,
      "grad_norm": 0.7055907249450684,
      "learning_rate": 0.00025951031970862,
      "loss": 0.0783,
      "step": 668
    },
    {
      "epoch": 0.2707542874487783,
      "grad_norm": 4.7757792472839355,
      "learning_rate": 0.00025944961554026706,
      "loss": 0.1437,
      "step": 669
    },
    {
      "epoch": 0.2711590023777002,
      "grad_norm": 2.544381618499756,
      "learning_rate": 0.0002593889113719142,
      "loss": 0.1083,
      "step": 670
    },
    {
      "epoch": 0.27156371730662215,
      "grad_norm": 2.36651611328125,
      "learning_rate": 0.0002593282072035613,
      "loss": 0.0869,
      "step": 671
    },
    {
      "epoch": 0.27196843223554407,
      "grad_norm": 1.5598145723342896,
      "learning_rate": 0.0002592675030352084,
      "loss": 0.0482,
      "step": 672
    },
    {
      "epoch": 0.27237314716446603,
      "grad_norm": 4.535067081451416,
      "learning_rate": 0.00025920679886685553,
      "loss": 0.139,
      "step": 673
    },
    {
      "epoch": 0.27277786209338795,
      "grad_norm": 5.594259262084961,
      "learning_rate": 0.0002591460946985026,
      "loss": 0.0674,
      "step": 674
    },
    {
      "epoch": 0.2731825770223099,
      "grad_norm": 3.544060230255127,
      "learning_rate": 0.0002590853905301497,
      "loss": 0.0819,
      "step": 675
    },
    {
      "epoch": 0.2735872919512318,
      "grad_norm": 4.120680332183838,
      "learning_rate": 0.00025902468636179683,
      "loss": 0.0972,
      "step": 676
    },
    {
      "epoch": 0.2739920068801538,
      "grad_norm": 4.008657932281494,
      "learning_rate": 0.00025896398219344395,
      "loss": 0.2321,
      "step": 677
    },
    {
      "epoch": 0.2743967218090757,
      "grad_norm": 3.357602596282959,
      "learning_rate": 0.00025890327802509107,
      "loss": 0.0787,
      "step": 678
    },
    {
      "epoch": 0.2748014367379977,
      "grad_norm": 2.4635517597198486,
      "learning_rate": 0.00025884257385673813,
      "loss": 0.082,
      "step": 679
    },
    {
      "epoch": 0.27520615166691964,
      "grad_norm": 1.8027247190475464,
      "learning_rate": 0.00025878186968838525,
      "loss": 0.1374,
      "step": 680
    },
    {
      "epoch": 0.27561086659584155,
      "grad_norm": 2.2578606605529785,
      "learning_rate": 0.00025872116552003236,
      "loss": 0.0679,
      "step": 681
    },
    {
      "epoch": 0.2760155815247635,
      "grad_norm": 1.147744059562683,
      "learning_rate": 0.0002586604613516795,
      "loss": 0.0337,
      "step": 682
    },
    {
      "epoch": 0.27642029645368543,
      "grad_norm": 1.939215898513794,
      "learning_rate": 0.0002585997571833266,
      "loss": 0.1156,
      "step": 683
    },
    {
      "epoch": 0.2768250113826074,
      "grad_norm": 2.785585641860962,
      "learning_rate": 0.00025853905301497366,
      "loss": 0.135,
      "step": 684
    },
    {
      "epoch": 0.2772297263115293,
      "grad_norm": 1.2609995603561401,
      "learning_rate": 0.0002584783488466208,
      "loss": 0.0425,
      "step": 685
    },
    {
      "epoch": 0.2776344412404513,
      "grad_norm": 1.1339956521987915,
      "learning_rate": 0.0002584176446782679,
      "loss": 0.0668,
      "step": 686
    },
    {
      "epoch": 0.2780391561693732,
      "grad_norm": 0.9422245621681213,
      "learning_rate": 0.000258356940509915,
      "loss": 0.0197,
      "step": 687
    },
    {
      "epoch": 0.27844387109829516,
      "grad_norm": 5.134159564971924,
      "learning_rate": 0.0002582962363415621,
      "loss": 0.1606,
      "step": 688
    },
    {
      "epoch": 0.27884858602721707,
      "grad_norm": 1.510309100151062,
      "learning_rate": 0.0002582355321732092,
      "loss": 0.0177,
      "step": 689
    },
    {
      "epoch": 0.27925330095613904,
      "grad_norm": 1.976007342338562,
      "learning_rate": 0.0002581748280048563,
      "loss": 0.0293,
      "step": 690
    },
    {
      "epoch": 0.27965801588506095,
      "grad_norm": 4.205446720123291,
      "learning_rate": 0.00025811412383650343,
      "loss": 0.1268,
      "step": 691
    },
    {
      "epoch": 0.2800627308139829,
      "grad_norm": 2.1000897884368896,
      "learning_rate": 0.0002580534196681505,
      "loss": 0.0371,
      "step": 692
    },
    {
      "epoch": 0.28046744574290483,
      "grad_norm": 4.356625556945801,
      "learning_rate": 0.0002579927154997976,
      "loss": 0.1049,
      "step": 693
    },
    {
      "epoch": 0.2808721606718268,
      "grad_norm": 2.723928928375244,
      "learning_rate": 0.00025793201133144473,
      "loss": 0.1123,
      "step": 694
    },
    {
      "epoch": 0.2812768756007487,
      "grad_norm": 0.21556764841079712,
      "learning_rate": 0.00025787130716309185,
      "loss": 0.0258,
      "step": 695
    },
    {
      "epoch": 0.2816815905296707,
      "grad_norm": 7.536472320556641,
      "learning_rate": 0.0002578106029947389,
      "loss": 0.1519,
      "step": 696
    },
    {
      "epoch": 0.2820863054585926,
      "grad_norm": 4.616150379180908,
      "learning_rate": 0.00025774989882638603,
      "loss": 0.0737,
      "step": 697
    },
    {
      "epoch": 0.28249102038751456,
      "grad_norm": 4.115657806396484,
      "learning_rate": 0.00025768919465803315,
      "loss": 0.0526,
      "step": 698
    },
    {
      "epoch": 0.28289573531643647,
      "grad_norm": 2.014742851257324,
      "learning_rate": 0.00025762849048968026,
      "loss": 0.0297,
      "step": 699
    },
    {
      "epoch": 0.28330045024535844,
      "grad_norm": 1.123844861984253,
      "learning_rate": 0.0002575677863213274,
      "loss": 0.0462,
      "step": 700
    },
    {
      "epoch": 0.28370516517428035,
      "grad_norm": 4.837038040161133,
      "learning_rate": 0.0002575070821529745,
      "loss": 0.1083,
      "step": 701
    },
    {
      "epoch": 0.2841098801032023,
      "grad_norm": 2.145879030227661,
      "learning_rate": 0.00025744637798462156,
      "loss": 0.1588,
      "step": 702
    },
    {
      "epoch": 0.28451459503212423,
      "grad_norm": 2.42649507522583,
      "learning_rate": 0.0002573856738162687,
      "loss": 0.0915,
      "step": 703
    },
    {
      "epoch": 0.2849193099610462,
      "grad_norm": 1.9030109643936157,
      "learning_rate": 0.0002573249696479158,
      "loss": 0.0431,
      "step": 704
    },
    {
      "epoch": 0.2853240248899681,
      "grad_norm": 1.0206345319747925,
      "learning_rate": 0.0002572642654795629,
      "loss": 0.0115,
      "step": 705
    },
    {
      "epoch": 0.2857287398188901,
      "grad_norm": 3.774456739425659,
      "learning_rate": 0.00025720356131121003,
      "loss": 0.0338,
      "step": 706
    },
    {
      "epoch": 0.286133454747812,
      "grad_norm": 5.519402980804443,
      "learning_rate": 0.0002571428571428571,
      "loss": 0.0667,
      "step": 707
    },
    {
      "epoch": 0.28653816967673396,
      "grad_norm": 2.8106400966644287,
      "learning_rate": 0.0002570821529745042,
      "loss": 0.0762,
      "step": 708
    },
    {
      "epoch": 0.28694288460565587,
      "grad_norm": 1.6530933380126953,
      "learning_rate": 0.00025702144880615133,
      "loss": 0.0879,
      "step": 709
    },
    {
      "epoch": 0.28734759953457784,
      "grad_norm": 2.5512845516204834,
      "learning_rate": 0.00025696074463779845,
      "loss": 0.0408,
      "step": 710
    },
    {
      "epoch": 0.28775231446349975,
      "grad_norm": 0.5601995587348938,
      "learning_rate": 0.00025690004046944557,
      "loss": 0.0327,
      "step": 711
    },
    {
      "epoch": 0.2881570293924217,
      "grad_norm": 0.9524599313735962,
      "learning_rate": 0.0002568393363010927,
      "loss": 0.0633,
      "step": 712
    },
    {
      "epoch": 0.2885617443213436,
      "grad_norm": 6.662018775939941,
      "learning_rate": 0.00025677863213273975,
      "loss": 0.235,
      "step": 713
    },
    {
      "epoch": 0.2889664592502656,
      "grad_norm": 1.8130452632904053,
      "learning_rate": 0.00025671792796438687,
      "loss": 0.0835,
      "step": 714
    },
    {
      "epoch": 0.28937117417918756,
      "grad_norm": 4.912254333496094,
      "learning_rate": 0.000256657223796034,
      "loss": 0.0942,
      "step": 715
    },
    {
      "epoch": 0.2897758891081095,
      "grad_norm": 2.55564022064209,
      "learning_rate": 0.0002565965196276811,
      "loss": 0.0855,
      "step": 716
    },
    {
      "epoch": 0.29018060403703144,
      "grad_norm": 2.7480335235595703,
      "learning_rate": 0.0002565358154593282,
      "loss": 0.0774,
      "step": 717
    },
    {
      "epoch": 0.29058531896595335,
      "grad_norm": 2.4539151191711426,
      "learning_rate": 0.0002564751112909753,
      "loss": 0.0578,
      "step": 718
    },
    {
      "epoch": 0.2909900338948753,
      "grad_norm": 4.430179595947266,
      "learning_rate": 0.0002564144071226224,
      "loss": 0.0628,
      "step": 719
    },
    {
      "epoch": 0.29139474882379723,
      "grad_norm": 5.548295021057129,
      "learning_rate": 0.0002563537029542695,
      "loss": 0.1372,
      "step": 720
    },
    {
      "epoch": 0.2917994637527192,
      "grad_norm": 0.9732357263565063,
      "learning_rate": 0.00025629299878591664,
      "loss": 0.0301,
      "step": 721
    },
    {
      "epoch": 0.2922041786816411,
      "grad_norm": 2.4223923683166504,
      "learning_rate": 0.00025623229461756375,
      "loss": 0.0626,
      "step": 722
    },
    {
      "epoch": 0.2926088936105631,
      "grad_norm": 2.996967315673828,
      "learning_rate": 0.0002561715904492108,
      "loss": 0.0407,
      "step": 723
    },
    {
      "epoch": 0.293013608539485,
      "grad_norm": 7.153995513916016,
      "learning_rate": 0.00025611088628085793,
      "loss": 0.2829,
      "step": 724
    },
    {
      "epoch": 0.29341832346840696,
      "grad_norm": 2.285310983657837,
      "learning_rate": 0.00025605018211250505,
      "loss": 0.1002,
      "step": 725
    },
    {
      "epoch": 0.2938230383973289,
      "grad_norm": 3.016895294189453,
      "learning_rate": 0.00025598947794415217,
      "loss": 0.0834,
      "step": 726
    },
    {
      "epoch": 0.29422775332625084,
      "grad_norm": 5.193478107452393,
      "learning_rate": 0.00025592877377579923,
      "loss": 0.0981,
      "step": 727
    },
    {
      "epoch": 0.29463246825517275,
      "grad_norm": 3.5436558723449707,
      "learning_rate": 0.00025586806960744635,
      "loss": 0.047,
      "step": 728
    },
    {
      "epoch": 0.2950371831840947,
      "grad_norm": 1.574978232383728,
      "learning_rate": 0.00025580736543909347,
      "loss": 0.0701,
      "step": 729
    },
    {
      "epoch": 0.29544189811301663,
      "grad_norm": 2.7017667293548584,
      "learning_rate": 0.0002557466612707406,
      "loss": 0.0195,
      "step": 730
    },
    {
      "epoch": 0.2958466130419386,
      "grad_norm": 1.2634663581848145,
      "learning_rate": 0.00025568595710238765,
      "loss": 0.0294,
      "step": 731
    },
    {
      "epoch": 0.2962513279708605,
      "grad_norm": 2.910609483718872,
      "learning_rate": 0.00025562525293403477,
      "loss": 0.0957,
      "step": 732
    },
    {
      "epoch": 0.2966560428997825,
      "grad_norm": 1.5539419651031494,
      "learning_rate": 0.0002555645487656819,
      "loss": 0.0371,
      "step": 733
    },
    {
      "epoch": 0.2970607578287044,
      "grad_norm": 2.5166683197021484,
      "learning_rate": 0.000255503844597329,
      "loss": 0.0803,
      "step": 734
    },
    {
      "epoch": 0.29746547275762636,
      "grad_norm": 3.6476423740386963,
      "learning_rate": 0.00025544314042897607,
      "loss": 0.0816,
      "step": 735
    },
    {
      "epoch": 0.29787018768654827,
      "grad_norm": 2.6656367778778076,
      "learning_rate": 0.0002553824362606232,
      "loss": 0.0984,
      "step": 736
    },
    {
      "epoch": 0.29827490261547024,
      "grad_norm": 4.256830215454102,
      "learning_rate": 0.0002553217320922703,
      "loss": 0.1414,
      "step": 737
    },
    {
      "epoch": 0.29867961754439215,
      "grad_norm": 1.9045343399047852,
      "learning_rate": 0.0002552610279239174,
      "loss": 0.0154,
      "step": 738
    },
    {
      "epoch": 0.2990843324733141,
      "grad_norm": 4.3754448890686035,
      "learning_rate": 0.00025520032375556454,
      "loss": 0.1657,
      "step": 739
    },
    {
      "epoch": 0.29948904740223603,
      "grad_norm": 2.797663688659668,
      "learning_rate": 0.0002551396195872116,
      "loss": 0.1489,
      "step": 740
    },
    {
      "epoch": 0.299893762331158,
      "grad_norm": 1.4765938520431519,
      "learning_rate": 0.0002550789154188587,
      "loss": 0.0654,
      "step": 741
    },
    {
      "epoch": 0.3002984772600799,
      "grad_norm": 0.9499439597129822,
      "learning_rate": 0.00025501821125050583,
      "loss": 0.0362,
      "step": 742
    },
    {
      "epoch": 0.3007031921890019,
      "grad_norm": 1.5627158880233765,
      "learning_rate": 0.00025495750708215295,
      "loss": 0.0436,
      "step": 743
    },
    {
      "epoch": 0.3011079071179238,
      "grad_norm": 2.28401517868042,
      "learning_rate": 0.00025489680291380007,
      "loss": 0.0634,
      "step": 744
    },
    {
      "epoch": 0.30151262204684576,
      "grad_norm": 0.6211035251617432,
      "learning_rate": 0.0002548360987454472,
      "loss": 0.0362,
      "step": 745
    },
    {
      "epoch": 0.30191733697576767,
      "grad_norm": 0.804502546787262,
      "learning_rate": 0.00025477539457709425,
      "loss": 0.0319,
      "step": 746
    },
    {
      "epoch": 0.30232205190468964,
      "grad_norm": 1.0152281522750854,
      "learning_rate": 0.00025471469040874137,
      "loss": 0.0168,
      "step": 747
    },
    {
      "epoch": 0.30272676683361155,
      "grad_norm": 3.118532419204712,
      "learning_rate": 0.0002546539862403885,
      "loss": 0.0875,
      "step": 748
    },
    {
      "epoch": 0.3031314817625335,
      "grad_norm": 3.8208250999450684,
      "learning_rate": 0.0002545932820720356,
      "loss": 0.1261,
      "step": 749
    },
    {
      "epoch": 0.30353619669145543,
      "grad_norm": 3.4006223678588867,
      "learning_rate": 0.0002545325779036827,
      "loss": 0.1384,
      "step": 750
    },
    {
      "epoch": 0.3039409116203774,
      "grad_norm": 2.6468441486358643,
      "learning_rate": 0.0002544718737353298,
      "loss": 0.0712,
      "step": 751
    },
    {
      "epoch": 0.30434562654929936,
      "grad_norm": 1.728218674659729,
      "learning_rate": 0.0002544111695669769,
      "loss": 0.048,
      "step": 752
    },
    {
      "epoch": 0.3047503414782213,
      "grad_norm": 2.4073591232299805,
      "learning_rate": 0.000254350465398624,
      "loss": 0.0846,
      "step": 753
    },
    {
      "epoch": 0.30515505640714324,
      "grad_norm": 0.8101916313171387,
      "learning_rate": 0.00025428976123027114,
      "loss": 0.0704,
      "step": 754
    },
    {
      "epoch": 0.30555977133606516,
      "grad_norm": 2.2253315448760986,
      "learning_rate": 0.00025422905706191825,
      "loss": 0.0368,
      "step": 755
    },
    {
      "epoch": 0.3059644862649871,
      "grad_norm": 1.3561404943466187,
      "learning_rate": 0.00025416835289356537,
      "loss": 0.0401,
      "step": 756
    },
    {
      "epoch": 0.30636920119390904,
      "grad_norm": 5.197948455810547,
      "learning_rate": 0.00025410764872521244,
      "loss": 0.0744,
      "step": 757
    },
    {
      "epoch": 0.306773916122831,
      "grad_norm": 0.5220168828964233,
      "learning_rate": 0.00025404694455685955,
      "loss": 0.0272,
      "step": 758
    },
    {
      "epoch": 0.3071786310517529,
      "grad_norm": 3.271611213684082,
      "learning_rate": 0.00025398624038850667,
      "loss": 0.0679,
      "step": 759
    },
    {
      "epoch": 0.3075833459806749,
      "grad_norm": 2.945481300354004,
      "learning_rate": 0.0002539255362201538,
      "loss": 0.055,
      "step": 760
    },
    {
      "epoch": 0.3079880609095968,
      "grad_norm": 1.2717112302780151,
      "learning_rate": 0.0002538648320518009,
      "loss": 0.0259,
      "step": 761
    },
    {
      "epoch": 0.30839277583851876,
      "grad_norm": 4.323166370391846,
      "learning_rate": 0.00025380412788344797,
      "loss": 0.0857,
      "step": 762
    },
    {
      "epoch": 0.3087974907674407,
      "grad_norm": 2.1103994846343994,
      "learning_rate": 0.0002537434237150951,
      "loss": 0.0618,
      "step": 763
    },
    {
      "epoch": 0.30920220569636264,
      "grad_norm": 1.0495275259017944,
      "learning_rate": 0.0002536827195467422,
      "loss": 0.0598,
      "step": 764
    },
    {
      "epoch": 0.30960692062528455,
      "grad_norm": 8.013548851013184,
      "learning_rate": 0.0002536220153783893,
      "loss": 0.0492,
      "step": 765
    },
    {
      "epoch": 0.3100116355542065,
      "grad_norm": 3.5251598358154297,
      "learning_rate": 0.0002535613112100364,
      "loss": 0.0601,
      "step": 766
    },
    {
      "epoch": 0.31041635048312843,
      "grad_norm": 4.089125156402588,
      "learning_rate": 0.0002535006070416835,
      "loss": 0.113,
      "step": 767
    },
    {
      "epoch": 0.3108210654120504,
      "grad_norm": 3.895482301712036,
      "learning_rate": 0.0002534399028733306,
      "loss": 0.0994,
      "step": 768
    },
    {
      "epoch": 0.3112257803409723,
      "grad_norm": 4.3544816970825195,
      "learning_rate": 0.00025337919870497774,
      "loss": 0.0902,
      "step": 769
    },
    {
      "epoch": 0.3116304952698943,
      "grad_norm": 3.1137337684631348,
      "learning_rate": 0.0002533184945366248,
      "loss": 0.0703,
      "step": 770
    },
    {
      "epoch": 0.3120352101988162,
      "grad_norm": 1.044204831123352,
      "learning_rate": 0.0002532577903682719,
      "loss": 0.0191,
      "step": 771
    },
    {
      "epoch": 0.31243992512773816,
      "grad_norm": 4.042741775512695,
      "learning_rate": 0.00025319708619991904,
      "loss": 0.1149,
      "step": 772
    },
    {
      "epoch": 0.3128446400566601,
      "grad_norm": 1.558321475982666,
      "learning_rate": 0.00025313638203156616,
      "loss": 0.0974,
      "step": 773
    },
    {
      "epoch": 0.31324935498558204,
      "grad_norm": 0.7040888071060181,
      "learning_rate": 0.0002530756778632132,
      "loss": 0.0298,
      "step": 774
    },
    {
      "epoch": 0.31365406991450395,
      "grad_norm": 5.492650985717773,
      "learning_rate": 0.00025301497369486034,
      "loss": 0.1938,
      "step": 775
    },
    {
      "epoch": 0.3140587848434259,
      "grad_norm": 1.8326704502105713,
      "learning_rate": 0.00025295426952650745,
      "loss": 0.1008,
      "step": 776
    },
    {
      "epoch": 0.31446349977234783,
      "grad_norm": 4.408595085144043,
      "learning_rate": 0.00025289356535815457,
      "loss": 0.0751,
      "step": 777
    },
    {
      "epoch": 0.3148682147012698,
      "grad_norm": 5.298696994781494,
      "learning_rate": 0.00025283286118980163,
      "loss": 0.0787,
      "step": 778
    },
    {
      "epoch": 0.3152729296301917,
      "grad_norm": 0.9216117858886719,
      "learning_rate": 0.00025277215702144875,
      "loss": 0.0379,
      "step": 779
    },
    {
      "epoch": 0.3156776445591137,
      "grad_norm": 3.5409016609191895,
      "learning_rate": 0.00025271145285309587,
      "loss": 0.056,
      "step": 780
    },
    {
      "epoch": 0.3160823594880356,
      "grad_norm": 0.5253769159317017,
      "learning_rate": 0.000252650748684743,
      "loss": 0.0292,
      "step": 781
    },
    {
      "epoch": 0.31648707441695756,
      "grad_norm": 1.1685692071914673,
      "learning_rate": 0.0002525900445163901,
      "loss": 0.0532,
      "step": 782
    },
    {
      "epoch": 0.31689178934587947,
      "grad_norm": 1.8187015056610107,
      "learning_rate": 0.0002525293403480372,
      "loss": 0.0371,
      "step": 783
    },
    {
      "epoch": 0.31729650427480144,
      "grad_norm": 2.290837526321411,
      "learning_rate": 0.0002524686361796843,
      "loss": 0.0718,
      "step": 784
    },
    {
      "epoch": 0.31770121920372335,
      "grad_norm": 5.911831855773926,
      "learning_rate": 0.0002524079320113314,
      "loss": 0.1581,
      "step": 785
    },
    {
      "epoch": 0.3181059341326453,
      "grad_norm": 4.16132926940918,
      "learning_rate": 0.0002523472278429785,
      "loss": 0.0817,
      "step": 786
    },
    {
      "epoch": 0.3185106490615673,
      "grad_norm": 2.170807361602783,
      "learning_rate": 0.00025228652367462564,
      "loss": 0.0859,
      "step": 787
    },
    {
      "epoch": 0.3189153639904892,
      "grad_norm": 1.9134783744812012,
      "learning_rate": 0.00025222581950627276,
      "loss": 0.05,
      "step": 788
    },
    {
      "epoch": 0.31932007891941117,
      "grad_norm": 2.135317325592041,
      "learning_rate": 0.0002521651153379198,
      "loss": 0.0365,
      "step": 789
    },
    {
      "epoch": 0.3197247938483331,
      "grad_norm": 0.6089878678321838,
      "learning_rate": 0.00025210441116956694,
      "loss": 0.0347,
      "step": 790
    },
    {
      "epoch": 0.32012950877725505,
      "grad_norm": 4.7214460372924805,
      "learning_rate": 0.00025204370700121406,
      "loss": 0.0461,
      "step": 791
    },
    {
      "epoch": 0.32053422370617696,
      "grad_norm": 3.3870701789855957,
      "learning_rate": 0.0002519830028328612,
      "loss": 0.0838,
      "step": 792
    },
    {
      "epoch": 0.3209389386350989,
      "grad_norm": 0.8573114275932312,
      "learning_rate": 0.0002519222986645083,
      "loss": 0.0208,
      "step": 793
    },
    {
      "epoch": 0.32134365356402084,
      "grad_norm": 2.101214647293091,
      "learning_rate": 0.0002518615944961554,
      "loss": 0.0598,
      "step": 794
    },
    {
      "epoch": 0.3217483684929428,
      "grad_norm": 1.293599009513855,
      "learning_rate": 0.00025180089032780247,
      "loss": 0.0434,
      "step": 795
    },
    {
      "epoch": 0.3221530834218647,
      "grad_norm": 1.5106661319732666,
      "learning_rate": 0.0002517401861594496,
      "loss": 0.0169,
      "step": 796
    },
    {
      "epoch": 0.3225577983507867,
      "grad_norm": 2.9357550144195557,
      "learning_rate": 0.0002516794819910967,
      "loss": 0.0518,
      "step": 797
    },
    {
      "epoch": 0.3229625132797086,
      "grad_norm": 7.424083232879639,
      "learning_rate": 0.0002516187778227438,
      "loss": 0.2508,
      "step": 798
    },
    {
      "epoch": 0.32336722820863056,
      "grad_norm": 0.7288024425506592,
      "learning_rate": 0.00025155807365439094,
      "loss": 0.0619,
      "step": 799
    },
    {
      "epoch": 0.3237719431375525,
      "grad_norm": 6.185051441192627,
      "learning_rate": 0.000251497369486038,
      "loss": 0.1726,
      "step": 800
    },
    {
      "epoch": 0.32417665806647444,
      "grad_norm": 4.057302951812744,
      "learning_rate": 0.0002514366653176851,
      "loss": 0.1277,
      "step": 801
    },
    {
      "epoch": 0.32458137299539636,
      "grad_norm": 3.6961185932159424,
      "learning_rate": 0.00025137596114933224,
      "loss": 0.0884,
      "step": 802
    },
    {
      "epoch": 0.3249860879243183,
      "grad_norm": 2.073087215423584,
      "learning_rate": 0.00025131525698097936,
      "loss": 0.0506,
      "step": 803
    },
    {
      "epoch": 0.32539080285324024,
      "grad_norm": 4.511754035949707,
      "learning_rate": 0.0002512545528126265,
      "loss": 0.0628,
      "step": 804
    },
    {
      "epoch": 0.3257955177821622,
      "grad_norm": 1.7549179792404175,
      "learning_rate": 0.00025119384864427354,
      "loss": 0.024,
      "step": 805
    },
    {
      "epoch": 0.3262002327110841,
      "grad_norm": 0.7757501602172852,
      "learning_rate": 0.00025113314447592066,
      "loss": 0.0218,
      "step": 806
    },
    {
      "epoch": 0.3266049476400061,
      "grad_norm": 5.354661464691162,
      "learning_rate": 0.0002510724403075678,
      "loss": 0.0807,
      "step": 807
    },
    {
      "epoch": 0.327009662568928,
      "grad_norm": 1.1616733074188232,
      "learning_rate": 0.0002510117361392149,
      "loss": 0.0241,
      "step": 808
    },
    {
      "epoch": 0.32741437749784996,
      "grad_norm": 0.6035143136978149,
      "learning_rate": 0.00025095103197086196,
      "loss": 0.0319,
      "step": 809
    },
    {
      "epoch": 0.3278190924267719,
      "grad_norm": 1.5072286128997803,
      "learning_rate": 0.0002508903278025091,
      "loss": 0.0555,
      "step": 810
    },
    {
      "epoch": 0.32822380735569384,
      "grad_norm": 4.699435234069824,
      "learning_rate": 0.0002508296236341562,
      "loss": 0.1118,
      "step": 811
    },
    {
      "epoch": 0.32862852228461575,
      "grad_norm": 2.390636444091797,
      "learning_rate": 0.0002507689194658033,
      "loss": 0.1037,
      "step": 812
    },
    {
      "epoch": 0.3290332372135377,
      "grad_norm": 3.758612632751465,
      "learning_rate": 0.00025070821529745037,
      "loss": 0.0902,
      "step": 813
    },
    {
      "epoch": 0.32943795214245963,
      "grad_norm": 6.471020698547363,
      "learning_rate": 0.0002506475111290975,
      "loss": 0.1349,
      "step": 814
    },
    {
      "epoch": 0.3298426670713816,
      "grad_norm": 1.4105876684188843,
      "learning_rate": 0.0002505868069607446,
      "loss": 0.0246,
      "step": 815
    },
    {
      "epoch": 0.3302473820003035,
      "grad_norm": 4.15931510925293,
      "learning_rate": 0.0002505261027923917,
      "loss": 0.0871,
      "step": 816
    },
    {
      "epoch": 0.3306520969292255,
      "grad_norm": 4.4515862464904785,
      "learning_rate": 0.00025046539862403884,
      "loss": 0.1108,
      "step": 817
    },
    {
      "epoch": 0.3310568118581474,
      "grad_norm": 3.933605432510376,
      "learning_rate": 0.0002504046944556859,
      "loss": 0.1059,
      "step": 818
    },
    {
      "epoch": 0.33146152678706936,
      "grad_norm": 2.2165677547454834,
      "learning_rate": 0.000250343990287333,
      "loss": 0.1303,
      "step": 819
    },
    {
      "epoch": 0.3318662417159913,
      "grad_norm": 5.900230407714844,
      "learning_rate": 0.00025028328611898014,
      "loss": 0.0858,
      "step": 820
    },
    {
      "epoch": 0.33227095664491324,
      "grad_norm": 3.358811140060425,
      "learning_rate": 0.00025022258195062726,
      "loss": 0.075,
      "step": 821
    },
    {
      "epoch": 0.33267567157383515,
      "grad_norm": 0.800666093826294,
      "learning_rate": 0.0002501618777822743,
      "loss": 0.0173,
      "step": 822
    },
    {
      "epoch": 0.3330803865027571,
      "grad_norm": 2.853527069091797,
      "learning_rate": 0.00025010117361392144,
      "loss": 0.0381,
      "step": 823
    },
    {
      "epoch": 0.3334851014316791,
      "grad_norm": 3.7441961765289307,
      "learning_rate": 0.00025004046944556856,
      "loss": 0.0946,
      "step": 824
    },
    {
      "epoch": 0.333889816360601,
      "grad_norm": 1.8425679206848145,
      "learning_rate": 0.0002499797652772157,
      "loss": 0.0562,
      "step": 825
    },
    {
      "epoch": 0.33429453128952297,
      "grad_norm": 1.1954946517944336,
      "learning_rate": 0.0002499190611088628,
      "loss": 0.0605,
      "step": 826
    },
    {
      "epoch": 0.3346992462184449,
      "grad_norm": 2.1368024349212646,
      "learning_rate": 0.0002498583569405099,
      "loss": 0.0766,
      "step": 827
    },
    {
      "epoch": 0.33510396114736685,
      "grad_norm": 2.0715150833129883,
      "learning_rate": 0.000249797652772157,
      "loss": 0.0257,
      "step": 828
    },
    {
      "epoch": 0.33550867607628876,
      "grad_norm": 3.4526665210723877,
      "learning_rate": 0.0002497369486038041,
      "loss": 0.1279,
      "step": 829
    },
    {
      "epoch": 0.3359133910052107,
      "grad_norm": 4.2038187980651855,
      "learning_rate": 0.0002496762444354512,
      "loss": 0.0824,
      "step": 830
    },
    {
      "epoch": 0.33631810593413264,
      "grad_norm": 0.8330318927764893,
      "learning_rate": 0.0002496155402670983,
      "loss": 0.0323,
      "step": 831
    },
    {
      "epoch": 0.3367228208630546,
      "grad_norm": 3.260485887527466,
      "learning_rate": 0.00024955483609874544,
      "loss": 0.1062,
      "step": 832
    },
    {
      "epoch": 0.3371275357919765,
      "grad_norm": 1.3032610416412354,
      "learning_rate": 0.0002494941319303925,
      "loss": 0.03,
      "step": 833
    },
    {
      "epoch": 0.3375322507208985,
      "grad_norm": 2.9363770484924316,
      "learning_rate": 0.0002494334277620396,
      "loss": 0.092,
      "step": 834
    },
    {
      "epoch": 0.3379369656498204,
      "grad_norm": 0.6168254613876343,
      "learning_rate": 0.00024937272359368674,
      "loss": 0.0576,
      "step": 835
    },
    {
      "epoch": 0.33834168057874237,
      "grad_norm": 6.3854660987854,
      "learning_rate": 0.00024931201942533386,
      "loss": 0.0718,
      "step": 836
    },
    {
      "epoch": 0.3387463955076643,
      "grad_norm": 2.645346164703369,
      "learning_rate": 0.000249251315256981,
      "loss": 0.0841,
      "step": 837
    },
    {
      "epoch": 0.33915111043658625,
      "grad_norm": 3.415520668029785,
      "learning_rate": 0.0002491906110886281,
      "loss": 0.0917,
      "step": 838
    },
    {
      "epoch": 0.33955582536550816,
      "grad_norm": 2.249101161956787,
      "learning_rate": 0.00024912990692027516,
      "loss": 0.0684,
      "step": 839
    },
    {
      "epoch": 0.3399605402944301,
      "grad_norm": 2.680298089981079,
      "learning_rate": 0.0002490692027519223,
      "loss": 0.1071,
      "step": 840
    },
    {
      "epoch": 0.34036525522335204,
      "grad_norm": 3.8148179054260254,
      "learning_rate": 0.0002490084985835694,
      "loss": 0.121,
      "step": 841
    },
    {
      "epoch": 0.340769970152274,
      "grad_norm": 2.2722558975219727,
      "learning_rate": 0.0002489477944152165,
      "loss": 0.0439,
      "step": 842
    },
    {
      "epoch": 0.3411746850811959,
      "grad_norm": 1.7649813890457153,
      "learning_rate": 0.00024888709024686363,
      "loss": 0.0382,
      "step": 843
    },
    {
      "epoch": 0.3415794000101179,
      "grad_norm": 4.619915962219238,
      "learning_rate": 0.0002488263860785107,
      "loss": 0.0869,
      "step": 844
    },
    {
      "epoch": 0.3419841149390398,
      "grad_norm": 2.6106374263763428,
      "learning_rate": 0.0002487656819101578,
      "loss": 0.0841,
      "step": 845
    },
    {
      "epoch": 0.34238882986796176,
      "grad_norm": 0.4474104046821594,
      "learning_rate": 0.00024870497774180493,
      "loss": 0.0374,
      "step": 846
    },
    {
      "epoch": 0.3427935447968837,
      "grad_norm": 4.999060153961182,
      "learning_rate": 0.00024864427357345205,
      "loss": 0.164,
      "step": 847
    },
    {
      "epoch": 0.34319825972580564,
      "grad_norm": 8.669425010681152,
      "learning_rate": 0.00024858356940509916,
      "loss": 0.0833,
      "step": 848
    },
    {
      "epoch": 0.34360297465472756,
      "grad_norm": 1.5282812118530273,
      "learning_rate": 0.0002485228652367462,
      "loss": 0.0652,
      "step": 849
    },
    {
      "epoch": 0.3440076895836495,
      "grad_norm": 0.38905206322669983,
      "learning_rate": 0.00024846216106839334,
      "loss": 0.0186,
      "step": 850
    },
    {
      "epoch": 0.34441240451257144,
      "grad_norm": 0.44081035256385803,
      "learning_rate": 0.00024840145690004046,
      "loss": 0.0313,
      "step": 851
    },
    {
      "epoch": 0.3448171194414934,
      "grad_norm": 2.210468292236328,
      "learning_rate": 0.0002483407527316876,
      "loss": 0.0427,
      "step": 852
    },
    {
      "epoch": 0.3452218343704153,
      "grad_norm": 3.6732349395751953,
      "learning_rate": 0.00024828004856333464,
      "loss": 0.0843,
      "step": 853
    },
    {
      "epoch": 0.3456265492993373,
      "grad_norm": 0.7008258700370789,
      "learning_rate": 0.00024821934439498176,
      "loss": 0.0663,
      "step": 854
    },
    {
      "epoch": 0.3460312642282592,
      "grad_norm": 1.144039273262024,
      "learning_rate": 0.0002481586402266289,
      "loss": 0.0348,
      "step": 855
    },
    {
      "epoch": 0.34643597915718116,
      "grad_norm": 0.7382415533065796,
      "learning_rate": 0.000248097936058276,
      "loss": 0.0145,
      "step": 856
    },
    {
      "epoch": 0.3468406940861031,
      "grad_norm": 3.8949694633483887,
      "learning_rate": 0.00024803723188992306,
      "loss": 0.112,
      "step": 857
    },
    {
      "epoch": 0.34724540901502504,
      "grad_norm": 4.248352527618408,
      "learning_rate": 0.0002479765277215702,
      "loss": 0.0827,
      "step": 858
    },
    {
      "epoch": 0.347650123943947,
      "grad_norm": 1.110355019569397,
      "learning_rate": 0.0002479158235532173,
      "loss": 0.0656,
      "step": 859
    },
    {
      "epoch": 0.3480548388728689,
      "grad_norm": 4.238198280334473,
      "learning_rate": 0.0002478551193848644,
      "loss": 0.1658,
      "step": 860
    },
    {
      "epoch": 0.3484595538017909,
      "grad_norm": 3.030139207839966,
      "learning_rate": 0.0002477944152165115,
      "loss": 0.052,
      "step": 861
    },
    {
      "epoch": 0.3488642687307128,
      "grad_norm": 8.424172401428223,
      "learning_rate": 0.0002477337110481586,
      "loss": 0.2223,
      "step": 862
    },
    {
      "epoch": 0.34926898365963477,
      "grad_norm": 3.2554264068603516,
      "learning_rate": 0.0002476730068798057,
      "loss": 0.0417,
      "step": 863
    },
    {
      "epoch": 0.3496736985885567,
      "grad_norm": 1.6811670064926147,
      "learning_rate": 0.00024761230271145283,
      "loss": 0.029,
      "step": 864
    },
    {
      "epoch": 0.35007841351747865,
      "grad_norm": 1.2840718030929565,
      "learning_rate": 0.00024755159854309995,
      "loss": 0.0602,
      "step": 865
    },
    {
      "epoch": 0.35048312844640056,
      "grad_norm": 4.219025611877441,
      "learning_rate": 0.000247490894374747,
      "loss": 0.1229,
      "step": 866
    },
    {
      "epoch": 0.35088784337532253,
      "grad_norm": 3.3551993370056152,
      "learning_rate": 0.0002474301902063941,
      "loss": 0.0999,
      "step": 867
    },
    {
      "epoch": 0.35129255830424444,
      "grad_norm": 2.490206241607666,
      "learning_rate": 0.00024736948603804124,
      "loss": 0.0843,
      "step": 868
    },
    {
      "epoch": 0.3516972732331664,
      "grad_norm": 2.706939935684204,
      "learning_rate": 0.00024730878186968836,
      "loss": 0.073,
      "step": 869
    },
    {
      "epoch": 0.3521019881620883,
      "grad_norm": 4.673961162567139,
      "learning_rate": 0.0002472480777013355,
      "loss": 0.0977,
      "step": 870
    },
    {
      "epoch": 0.3525067030910103,
      "grad_norm": 1.4208060503005981,
      "learning_rate": 0.0002471873735329826,
      "loss": 0.0414,
      "step": 871
    },
    {
      "epoch": 0.3529114180199322,
      "grad_norm": 5.09758996963501,
      "learning_rate": 0.00024712666936462966,
      "loss": 0.1074,
      "step": 872
    },
    {
      "epoch": 0.35331613294885417,
      "grad_norm": 2.699769973754883,
      "learning_rate": 0.0002470659651962768,
      "loss": 0.0975,
      "step": 873
    },
    {
      "epoch": 0.3537208478777761,
      "grad_norm": 3.1709742546081543,
      "learning_rate": 0.0002470052610279239,
      "loss": 0.0707,
      "step": 874
    },
    {
      "epoch": 0.35412556280669805,
      "grad_norm": 2.9814136028289795,
      "learning_rate": 0.000246944556859571,
      "loss": 0.0904,
      "step": 875
    },
    {
      "epoch": 0.35453027773561996,
      "grad_norm": 2.8795969486236572,
      "learning_rate": 0.00024688385269121813,
      "loss": 0.0903,
      "step": 876
    },
    {
      "epoch": 0.3549349926645419,
      "grad_norm": 3.566270112991333,
      "learning_rate": 0.0002468231485228652,
      "loss": 0.0919,
      "step": 877
    },
    {
      "epoch": 0.35533970759346384,
      "grad_norm": 1.1166813373565674,
      "learning_rate": 0.0002467624443545123,
      "loss": 0.0707,
      "step": 878
    },
    {
      "epoch": 0.3557444225223858,
      "grad_norm": 1.2445803880691528,
      "learning_rate": 0.00024670174018615943,
      "loss": 0.0161,
      "step": 879
    },
    {
      "epoch": 0.3561491374513077,
      "grad_norm": 1.0896270275115967,
      "learning_rate": 0.00024664103601780655,
      "loss": 0.049,
      "step": 880
    },
    {
      "epoch": 0.3565538523802297,
      "grad_norm": 1.0627312660217285,
      "learning_rate": 0.00024658033184945367,
      "loss": 0.0451,
      "step": 881
    },
    {
      "epoch": 0.3569585673091516,
      "grad_norm": 1.5717297792434692,
      "learning_rate": 0.0002465196276811008,
      "loss": 0.0544,
      "step": 882
    },
    {
      "epoch": 0.35736328223807357,
      "grad_norm": 4.77934455871582,
      "learning_rate": 0.00024645892351274785,
      "loss": 0.1363,
      "step": 883
    },
    {
      "epoch": 0.3577679971669955,
      "grad_norm": 0.5787398815155029,
      "learning_rate": 0.00024639821934439496,
      "loss": 0.0343,
      "step": 884
    },
    {
      "epoch": 0.35817271209591744,
      "grad_norm": 2.0546281337738037,
      "learning_rate": 0.0002463375151760421,
      "loss": 0.1076,
      "step": 885
    },
    {
      "epoch": 0.35857742702483936,
      "grad_norm": 1.3203954696655273,
      "learning_rate": 0.0002462768110076892,
      "loss": 0.0962,
      "step": 886
    },
    {
      "epoch": 0.3589821419537613,
      "grad_norm": 3.933891534805298,
      "learning_rate": 0.0002462161068393363,
      "loss": 0.0611,
      "step": 887
    },
    {
      "epoch": 0.35938685688268324,
      "grad_norm": 1.5891649723052979,
      "learning_rate": 0.0002461554026709834,
      "loss": 0.0165,
      "step": 888
    },
    {
      "epoch": 0.3597915718116052,
      "grad_norm": 0.6149147748947144,
      "learning_rate": 0.0002460946985026305,
      "loss": 0.0365,
      "step": 889
    },
    {
      "epoch": 0.3601962867405271,
      "grad_norm": 0.4068247079849243,
      "learning_rate": 0.0002460339943342776,
      "loss": 0.0202,
      "step": 890
    },
    {
      "epoch": 0.3606010016694491,
      "grad_norm": 3.4370675086975098,
      "learning_rate": 0.00024597329016592473,
      "loss": 0.1064,
      "step": 891
    },
    {
      "epoch": 0.361005716598371,
      "grad_norm": 1.1855545043945312,
      "learning_rate": 0.0002459125859975718,
      "loss": 0.0762,
      "step": 892
    },
    {
      "epoch": 0.36141043152729296,
      "grad_norm": 1.5342812538146973,
      "learning_rate": 0.0002458518818292189,
      "loss": 0.0843,
      "step": 893
    },
    {
      "epoch": 0.36181514645621493,
      "grad_norm": 1.7892110347747803,
      "learning_rate": 0.00024579117766086603,
      "loss": 0.0674,
      "step": 894
    },
    {
      "epoch": 0.36221986138513684,
      "grad_norm": 4.889941215515137,
      "learning_rate": 0.00024573047349251315,
      "loss": 0.1828,
      "step": 895
    },
    {
      "epoch": 0.3626245763140588,
      "grad_norm": 0.6574604511260986,
      "learning_rate": 0.0002456697693241602,
      "loss": 0.0182,
      "step": 896
    },
    {
      "epoch": 0.3630292912429807,
      "grad_norm": 1.0849379301071167,
      "learning_rate": 0.00024560906515580733,
      "loss": 0.0859,
      "step": 897
    },
    {
      "epoch": 0.3634340061719027,
      "grad_norm": 3.478064775466919,
      "learning_rate": 0.00024554836098745445,
      "loss": 0.1223,
      "step": 898
    },
    {
      "epoch": 0.3638387211008246,
      "grad_norm": 3.018484115600586,
      "learning_rate": 0.00024548765681910157,
      "loss": 0.1209,
      "step": 899
    },
    {
      "epoch": 0.36424343602974657,
      "grad_norm": 2.2979555130004883,
      "learning_rate": 0.00024542695265074863,
      "loss": 0.0568,
      "step": 900
    },
    {
      "epoch": 0.3646481509586685,
      "grad_norm": 1.5826281309127808,
      "learning_rate": 0.00024536624848239575,
      "loss": 0.0828,
      "step": 901
    },
    {
      "epoch": 0.36505286588759045,
      "grad_norm": 1.2777010202407837,
      "learning_rate": 0.00024530554431404286,
      "loss": 0.0412,
      "step": 902
    },
    {
      "epoch": 0.36545758081651236,
      "grad_norm": 1.4949240684509277,
      "learning_rate": 0.00024524484014569,
      "loss": 0.0613,
      "step": 903
    },
    {
      "epoch": 0.36586229574543433,
      "grad_norm": 1.1096590757369995,
      "learning_rate": 0.0002451841359773371,
      "loss": 0.0515,
      "step": 904
    },
    {
      "epoch": 0.36626701067435624,
      "grad_norm": 6.330252170562744,
      "learning_rate": 0.00024512343180898416,
      "loss": 0.0822,
      "step": 905
    },
    {
      "epoch": 0.3666717256032782,
      "grad_norm": 1.8533117771148682,
      "learning_rate": 0.0002450627276406313,
      "loss": 0.0445,
      "step": 906
    },
    {
      "epoch": 0.3670764405322001,
      "grad_norm": 1.5038915872573853,
      "learning_rate": 0.0002450020234722784,
      "loss": 0.0918,
      "step": 907
    },
    {
      "epoch": 0.3674811554611221,
      "grad_norm": 2.4305970668792725,
      "learning_rate": 0.0002449413193039255,
      "loss": 0.0975,
      "step": 908
    },
    {
      "epoch": 0.367885870390044,
      "grad_norm": 2.913900852203369,
      "learning_rate": 0.00024488061513557263,
      "loss": 0.08,
      "step": 909
    },
    {
      "epoch": 0.36829058531896597,
      "grad_norm": 1.3152751922607422,
      "learning_rate": 0.0002448199109672197,
      "loss": 0.0442,
      "step": 910
    },
    {
      "epoch": 0.3686953002478879,
      "grad_norm": 1.2131402492523193,
      "learning_rate": 0.0002447592067988668,
      "loss": 0.0499,
      "step": 911
    },
    {
      "epoch": 0.36910001517680985,
      "grad_norm": 7.605318069458008,
      "learning_rate": 0.00024469850263051393,
      "loss": 0.0979,
      "step": 912
    },
    {
      "epoch": 0.36950473010573176,
      "grad_norm": 2.4758100509643555,
      "learning_rate": 0.00024463779846216105,
      "loss": 0.0489,
      "step": 913
    },
    {
      "epoch": 0.3699094450346537,
      "grad_norm": 2.2592225074768066,
      "learning_rate": 0.00024457709429380817,
      "loss": 0.0536,
      "step": 914
    },
    {
      "epoch": 0.37031415996357564,
      "grad_norm": 1.5654698610305786,
      "learning_rate": 0.0002445163901254553,
      "loss": 0.0252,
      "step": 915
    },
    {
      "epoch": 0.3707188748924976,
      "grad_norm": 4.530949115753174,
      "learning_rate": 0.00024445568595710235,
      "loss": 0.1145,
      "step": 916
    },
    {
      "epoch": 0.3711235898214195,
      "grad_norm": 4.606192588806152,
      "learning_rate": 0.00024439498178874947,
      "loss": 0.2173,
      "step": 917
    },
    {
      "epoch": 0.3715283047503415,
      "grad_norm": 5.842713356018066,
      "learning_rate": 0.0002443342776203966,
      "loss": 0.0987,
      "step": 918
    },
    {
      "epoch": 0.3719330196792634,
      "grad_norm": 2.3517255783081055,
      "learning_rate": 0.0002442735734520437,
      "loss": 0.0394,
      "step": 919
    },
    {
      "epoch": 0.37233773460818537,
      "grad_norm": 1.1050533056259155,
      "learning_rate": 0.0002442128692836908,
      "loss": 0.0428,
      "step": 920
    },
    {
      "epoch": 0.3727424495371073,
      "grad_norm": 3.472595453262329,
      "learning_rate": 0.0002441521651153379,
      "loss": 0.1019,
      "step": 921
    },
    {
      "epoch": 0.37314716446602925,
      "grad_norm": 1.9931069612503052,
      "learning_rate": 0.000244091460946985,
      "loss": 0.0398,
      "step": 922
    },
    {
      "epoch": 0.37355187939495116,
      "grad_norm": 1.5863209962844849,
      "learning_rate": 0.00024403075677863212,
      "loss": 0.0868,
      "step": 923
    },
    {
      "epoch": 0.3739565943238731,
      "grad_norm": 1.4944846630096436,
      "learning_rate": 0.0002439700526102792,
      "loss": 0.0591,
      "step": 924
    },
    {
      "epoch": 0.37436130925279504,
      "grad_norm": 4.276716709136963,
      "learning_rate": 0.00024390934844192633,
      "loss": 0.0864,
      "step": 925
    },
    {
      "epoch": 0.374766024181717,
      "grad_norm": 1.6828709840774536,
      "learning_rate": 0.00024384864427357344,
      "loss": 0.0456,
      "step": 926
    },
    {
      "epoch": 0.3751707391106389,
      "grad_norm": 1.8488284349441528,
      "learning_rate": 0.00024378794010522053,
      "loss": 0.0435,
      "step": 927
    },
    {
      "epoch": 0.3755754540395609,
      "grad_norm": 2.477120876312256,
      "learning_rate": 0.00024372723593686762,
      "loss": 0.0768,
      "step": 928
    },
    {
      "epoch": 0.3759801689684828,
      "grad_norm": 3.1120622158050537,
      "learning_rate": 0.00024366653176851474,
      "loss": 0.1373,
      "step": 929
    },
    {
      "epoch": 0.37638488389740477,
      "grad_norm": 0.7389892935752869,
      "learning_rate": 0.00024360582760016186,
      "loss": 0.0697,
      "step": 930
    },
    {
      "epoch": 0.37678959882632673,
      "grad_norm": 0.8252789974212646,
      "learning_rate": 0.00024354512343180898,
      "loss": 0.0394,
      "step": 931
    },
    {
      "epoch": 0.37719431375524864,
      "grad_norm": 1.9653921127319336,
      "learning_rate": 0.00024348441926345607,
      "loss": 0.0845,
      "step": 932
    },
    {
      "epoch": 0.3775990286841706,
      "grad_norm": 3.588275671005249,
      "learning_rate": 0.00024342371509510316,
      "loss": 0.0942,
      "step": 933
    },
    {
      "epoch": 0.3780037436130925,
      "grad_norm": 4.969222068786621,
      "learning_rate": 0.00024336301092675028,
      "loss": 0.0905,
      "step": 934
    },
    {
      "epoch": 0.3784084585420145,
      "grad_norm": 0.8429448008537292,
      "learning_rate": 0.0002433023067583974,
      "loss": 0.049,
      "step": 935
    },
    {
      "epoch": 0.3788131734709364,
      "grad_norm": 2.087303638458252,
      "learning_rate": 0.0002432416025900445,
      "loss": 0.0762,
      "step": 936
    },
    {
      "epoch": 0.37921788839985837,
      "grad_norm": 0.4658062160015106,
      "learning_rate": 0.00024318089842169163,
      "loss": 0.0351,
      "step": 937
    },
    {
      "epoch": 0.3796226033287803,
      "grad_norm": 5.528832912445068,
      "learning_rate": 0.0002431201942533387,
      "loss": 0.1321,
      "step": 938
    },
    {
      "epoch": 0.38002731825770225,
      "grad_norm": 2.341977119445801,
      "learning_rate": 0.0002430594900849858,
      "loss": 0.0733,
      "step": 939
    },
    {
      "epoch": 0.38043203318662416,
      "grad_norm": 2.4631917476654053,
      "learning_rate": 0.00024299878591663293,
      "loss": 0.0645,
      "step": 940
    },
    {
      "epoch": 0.38083674811554613,
      "grad_norm": 3.3775570392608643,
      "learning_rate": 0.00024293808174828004,
      "loss": 0.152,
      "step": 941
    },
    {
      "epoch": 0.38124146304446804,
      "grad_norm": 5.475233554840088,
      "learning_rate": 0.00024287737757992714,
      "loss": 0.1205,
      "step": 942
    },
    {
      "epoch": 0.38164617797339,
      "grad_norm": 5.5129075050354,
      "learning_rate": 0.00024281667341157423,
      "loss": 0.1295,
      "step": 943
    },
    {
      "epoch": 0.3820508929023119,
      "grad_norm": 0.9711857438087463,
      "learning_rate": 0.00024275596924322134,
      "loss": 0.0075,
      "step": 944
    },
    {
      "epoch": 0.3824556078312339,
      "grad_norm": 3.4874114990234375,
      "learning_rate": 0.00024269526507486846,
      "loss": 0.0335,
      "step": 945
    },
    {
      "epoch": 0.3828603227601558,
      "grad_norm": 4.03566312789917,
      "learning_rate": 0.00024263456090651555,
      "loss": 0.1194,
      "step": 946
    },
    {
      "epoch": 0.38326503768907777,
      "grad_norm": 1.35918390750885,
      "learning_rate": 0.00024257385673816267,
      "loss": 0.0787,
      "step": 947
    },
    {
      "epoch": 0.3836697526179997,
      "grad_norm": 3.874033212661743,
      "learning_rate": 0.0002425131525698098,
      "loss": 0.1735,
      "step": 948
    },
    {
      "epoch": 0.38407446754692165,
      "grad_norm": 2.5289313793182373,
      "learning_rate": 0.00024245244840145688,
      "loss": 0.1317,
      "step": 949
    },
    {
      "epoch": 0.38447918247584356,
      "grad_norm": 3.762587070465088,
      "learning_rate": 0.00024239174423310397,
      "loss": 0.0728,
      "step": 950
    },
    {
      "epoch": 0.38488389740476553,
      "grad_norm": 0.26889461278915405,
      "learning_rate": 0.00024233104006475109,
      "loss": 0.0174,
      "step": 951
    },
    {
      "epoch": 0.38528861233368744,
      "grad_norm": 1.4292341470718384,
      "learning_rate": 0.0002422703358963982,
      "loss": 0.0418,
      "step": 952
    },
    {
      "epoch": 0.3856933272626094,
      "grad_norm": 0.5531431436538696,
      "learning_rate": 0.00024220963172804532,
      "loss": 0.0657,
      "step": 953
    },
    {
      "epoch": 0.3860980421915313,
      "grad_norm": 1.2161619663238525,
      "learning_rate": 0.00024214892755969238,
      "loss": 0.0446,
      "step": 954
    },
    {
      "epoch": 0.3865027571204533,
      "grad_norm": 4.282018661499023,
      "learning_rate": 0.0002420882233913395,
      "loss": 0.0909,
      "step": 955
    },
    {
      "epoch": 0.3869074720493752,
      "grad_norm": 1.384809136390686,
      "learning_rate": 0.00024202751922298662,
      "loss": 0.097,
      "step": 956
    },
    {
      "epoch": 0.38731218697829717,
      "grad_norm": 1.997260332107544,
      "learning_rate": 0.00024196681505463374,
      "loss": 0.063,
      "step": 957
    },
    {
      "epoch": 0.3877169019072191,
      "grad_norm": 1.588596224784851,
      "learning_rate": 0.00024190611088628085,
      "loss": 0.0924,
      "step": 958
    },
    {
      "epoch": 0.38812161683614105,
      "grad_norm": 0.6704742312431335,
      "learning_rate": 0.00024184540671792797,
      "loss": 0.0235,
      "step": 959
    },
    {
      "epoch": 0.38852633176506296,
      "grad_norm": 4.038373947143555,
      "learning_rate": 0.00024178470254957504,
      "loss": 0.0734,
      "step": 960
    },
    {
      "epoch": 0.3889310466939849,
      "grad_norm": 2.7004969120025635,
      "learning_rate": 0.00024172399838122215,
      "loss": 0.0782,
      "step": 961
    },
    {
      "epoch": 0.38933576162290684,
      "grad_norm": 1.2184549570083618,
      "learning_rate": 0.00024166329421286927,
      "loss": 0.0465,
      "step": 962
    },
    {
      "epoch": 0.3897404765518288,
      "grad_norm": 2.4165401458740234,
      "learning_rate": 0.0002416025900445164,
      "loss": 0.065,
      "step": 963
    },
    {
      "epoch": 0.3901451914807507,
      "grad_norm": 0.5994613170623779,
      "learning_rate": 0.00024154188587616348,
      "loss": 0.0284,
      "step": 964
    },
    {
      "epoch": 0.3905499064096727,
      "grad_norm": 2.2310595512390137,
      "learning_rate": 0.00024148118170781057,
      "loss": 0.0492,
      "step": 965
    },
    {
      "epoch": 0.39095462133859465,
      "grad_norm": 2.9054620265960693,
      "learning_rate": 0.0002414204775394577,
      "loss": 0.0951,
      "step": 966
    },
    {
      "epoch": 0.39135933626751657,
      "grad_norm": 3.8415703773498535,
      "learning_rate": 0.0002413597733711048,
      "loss": 0.1001,
      "step": 967
    },
    {
      "epoch": 0.39176405119643853,
      "grad_norm": 4.478612899780273,
      "learning_rate": 0.0002412990692027519,
      "loss": 0.087,
      "step": 968
    },
    {
      "epoch": 0.39216876612536045,
      "grad_norm": 1.9472408294677734,
      "learning_rate": 0.000241238365034399,
      "loss": 0.0128,
      "step": 969
    },
    {
      "epoch": 0.3925734810542824,
      "grad_norm": 1.5184080600738525,
      "learning_rate": 0.00024117766086604613,
      "loss": 0.0458,
      "step": 970
    },
    {
      "epoch": 0.3929781959832043,
      "grad_norm": 1.214929461479187,
      "learning_rate": 0.00024111695669769322,
      "loss": 0.0516,
      "step": 971
    },
    {
      "epoch": 0.3933829109121263,
      "grad_norm": 2.1330783367156982,
      "learning_rate": 0.0002410562525293403,
      "loss": 0.0566,
      "step": 972
    },
    {
      "epoch": 0.3937876258410482,
      "grad_norm": 2.7895519733428955,
      "learning_rate": 0.00024099554836098743,
      "loss": 0.0878,
      "step": 973
    },
    {
      "epoch": 0.3941923407699702,
      "grad_norm": 2.2250638008117676,
      "learning_rate": 0.00024093484419263455,
      "loss": 0.0358,
      "step": 974
    },
    {
      "epoch": 0.3945970556988921,
      "grad_norm": 0.9852149486541748,
      "learning_rate": 0.00024087414002428166,
      "loss": 0.0183,
      "step": 975
    },
    {
      "epoch": 0.39500177062781405,
      "grad_norm": 2.2999801635742188,
      "learning_rate": 0.00024081343585592873,
      "loss": 0.0738,
      "step": 976
    },
    {
      "epoch": 0.39540648555673596,
      "grad_norm": 1.560041069984436,
      "learning_rate": 0.00024075273168757585,
      "loss": 0.0606,
      "step": 977
    },
    {
      "epoch": 0.39581120048565793,
      "grad_norm": 1.844535231590271,
      "learning_rate": 0.00024069202751922296,
      "loss": 0.0245,
      "step": 978
    },
    {
      "epoch": 0.39621591541457984,
      "grad_norm": 0.19722917675971985,
      "learning_rate": 0.00024063132335087008,
      "loss": 0.008,
      "step": 979
    },
    {
      "epoch": 0.3966206303435018,
      "grad_norm": 2.2061870098114014,
      "learning_rate": 0.0002405706191825172,
      "loss": 0.0342,
      "step": 980
    },
    {
      "epoch": 0.3970253452724237,
      "grad_norm": 4.368879795074463,
      "learning_rate": 0.0002405099150141643,
      "loss": 0.0665,
      "step": 981
    },
    {
      "epoch": 0.3974300602013457,
      "grad_norm": 1.1590206623077393,
      "learning_rate": 0.00024044921084581138,
      "loss": 0.0563,
      "step": 982
    },
    {
      "epoch": 0.3978347751302676,
      "grad_norm": 2.318772315979004,
      "learning_rate": 0.0002403885066774585,
      "loss": 0.0368,
      "step": 983
    },
    {
      "epoch": 0.39823949005918957,
      "grad_norm": 4.726341247558594,
      "learning_rate": 0.00024032780250910561,
      "loss": 0.0921,
      "step": 984
    },
    {
      "epoch": 0.3986442049881115,
      "grad_norm": 0.6779621839523315,
      "learning_rate": 0.0002402670983407527,
      "loss": 0.0458,
      "step": 985
    },
    {
      "epoch": 0.39904891991703345,
      "grad_norm": 2.4925003051757812,
      "learning_rate": 0.00024020639417239982,
      "loss": 0.1245,
      "step": 986
    },
    {
      "epoch": 0.39945363484595536,
      "grad_norm": 2.4813382625579834,
      "learning_rate": 0.0002401456900040469,
      "loss": 0.0716,
      "step": 987
    },
    {
      "epoch": 0.39985834977487733,
      "grad_norm": 3.6719584465026855,
      "learning_rate": 0.00024008498583569403,
      "loss": 0.0959,
      "step": 988
    },
    {
      "epoch": 0.40026306470379924,
      "grad_norm": 2.1738674640655518,
      "learning_rate": 0.00024002428166734112,
      "loss": 0.0326,
      "step": 989
    },
    {
      "epoch": 0.4006677796327212,
      "grad_norm": 1.28714919090271,
      "learning_rate": 0.00023996357749898824,
      "loss": 0.0543,
      "step": 990
    },
    {
      "epoch": 0.4010724945616431,
      "grad_norm": 4.246404647827148,
      "learning_rate": 0.00023990287333063536,
      "loss": 0.0664,
      "step": 991
    },
    {
      "epoch": 0.4014772094905651,
      "grad_norm": 5.123708248138428,
      "learning_rate": 0.00023984216916228247,
      "loss": 0.1239,
      "step": 992
    },
    {
      "epoch": 0.401881924419487,
      "grad_norm": 3.135321617126465,
      "learning_rate": 0.00023978146499392954,
      "loss": 0.0874,
      "step": 993
    },
    {
      "epoch": 0.40228663934840897,
      "grad_norm": 2.1856327056884766,
      "learning_rate": 0.00023972076082557665,
      "loss": 0.0482,
      "step": 994
    },
    {
      "epoch": 0.4026913542773309,
      "grad_norm": 2.0339150428771973,
      "learning_rate": 0.00023966005665722377,
      "loss": 0.0522,
      "step": 995
    },
    {
      "epoch": 0.40309606920625285,
      "grad_norm": 2.2532544136047363,
      "learning_rate": 0.0002395993524888709,
      "loss": 0.0898,
      "step": 996
    },
    {
      "epoch": 0.40350078413517476,
      "grad_norm": 0.6683946847915649,
      "learning_rate": 0.000239538648320518,
      "loss": 0.0151,
      "step": 997
    },
    {
      "epoch": 0.40390549906409673,
      "grad_norm": 4.654819965362549,
      "learning_rate": 0.00023947794415216507,
      "loss": 0.1161,
      "step": 998
    },
    {
      "epoch": 0.40431021399301864,
      "grad_norm": 4.935729026794434,
      "learning_rate": 0.0002394172399838122,
      "loss": 0.0583,
      "step": 999
    },
    {
      "epoch": 0.4047149289219406,
      "grad_norm": 3.13324236869812,
      "learning_rate": 0.0002393565358154593,
      "loss": 0.1198,
      "step": 1000
    },
    {
      "epoch": 0.4051196438508626,
      "grad_norm": 3.6456692218780518,
      "learning_rate": 0.00023929583164710642,
      "loss": 0.07,
      "step": 1001
    },
    {
      "epoch": 0.4055243587797845,
      "grad_norm": 1.334214448928833,
      "learning_rate": 0.00023923512747875354,
      "loss": 0.05,
      "step": 1002
    },
    {
      "epoch": 0.40592907370870646,
      "grad_norm": 2.568730592727661,
      "learning_rate": 0.00023917442331040063,
      "loss": 0.0821,
      "step": 1003
    },
    {
      "epoch": 0.40633378863762837,
      "grad_norm": 2.781501054763794,
      "learning_rate": 0.00023911371914204772,
      "loss": 0.0385,
      "step": 1004
    },
    {
      "epoch": 0.40673850356655034,
      "grad_norm": 2.0780797004699707,
      "learning_rate": 0.00023905301497369484,
      "loss": 0.0374,
      "step": 1005
    },
    {
      "epoch": 0.40714321849547225,
      "grad_norm": 2.2803242206573486,
      "learning_rate": 0.00023899231080534196,
      "loss": 0.0162,
      "step": 1006
    },
    {
      "epoch": 0.4075479334243942,
      "grad_norm": 2.454805374145508,
      "learning_rate": 0.00023893160663698905,
      "loss": 0.0385,
      "step": 1007
    },
    {
      "epoch": 0.4079526483533161,
      "grad_norm": 5.14401388168335,
      "learning_rate": 0.00023887090246863617,
      "loss": 0.0666,
      "step": 1008
    },
    {
      "epoch": 0.4083573632822381,
      "grad_norm": 5.177791118621826,
      "learning_rate": 0.00023881019830028326,
      "loss": 0.0587,
      "step": 1009
    },
    {
      "epoch": 0.40876207821116,
      "grad_norm": 1.8399354219436646,
      "learning_rate": 0.00023874949413193037,
      "loss": 0.1027,
      "step": 1010
    },
    {
      "epoch": 0.409166793140082,
      "grad_norm": 2.5383760929107666,
      "learning_rate": 0.00023868878996357746,
      "loss": 0.0747,
      "step": 1011
    },
    {
      "epoch": 0.4095715080690039,
      "grad_norm": 0.6489133238792419,
      "learning_rate": 0.00023862808579522458,
      "loss": 0.0118,
      "step": 1012
    },
    {
      "epoch": 0.40997622299792585,
      "grad_norm": 0.6905893683433533,
      "learning_rate": 0.0002385673816268717,
      "loss": 0.0261,
      "step": 1013
    },
    {
      "epoch": 0.41038093792684777,
      "grad_norm": 8.4207124710083,
      "learning_rate": 0.00023850667745851882,
      "loss": 0.0971,
      "step": 1014
    },
    {
      "epoch": 0.41078565285576973,
      "grad_norm": 4.1666579246521,
      "learning_rate": 0.00023844597329016588,
      "loss": 0.1525,
      "step": 1015
    },
    {
      "epoch": 0.41119036778469165,
      "grad_norm": 1.9622441530227661,
      "learning_rate": 0.000238385269121813,
      "loss": 0.0907,
      "step": 1016
    },
    {
      "epoch": 0.4115950827136136,
      "grad_norm": 3.3772075176239014,
      "learning_rate": 0.00023832456495346012,
      "loss": 0.1078,
      "step": 1017
    },
    {
      "epoch": 0.4119997976425355,
      "grad_norm": 3.4861748218536377,
      "learning_rate": 0.00023826386078510723,
      "loss": 0.0623,
      "step": 1018
    },
    {
      "epoch": 0.4124045125714575,
      "grad_norm": 3.5013561248779297,
      "learning_rate": 0.00023820315661675435,
      "loss": 0.0504,
      "step": 1019
    },
    {
      "epoch": 0.4128092275003794,
      "grad_norm": 3.995182514190674,
      "learning_rate": 0.00023814245244840141,
      "loss": 0.1156,
      "step": 1020
    },
    {
      "epoch": 0.4132139424293014,
      "grad_norm": 0.8019339442253113,
      "learning_rate": 0.00023808174828004853,
      "loss": 0.0834,
      "step": 1021
    },
    {
      "epoch": 0.4136186573582233,
      "grad_norm": 3.294832229614258,
      "learning_rate": 0.00023802104411169565,
      "loss": 0.1034,
      "step": 1022
    },
    {
      "epoch": 0.41402337228714525,
      "grad_norm": 2.942317008972168,
      "learning_rate": 0.00023796033994334277,
      "loss": 0.0467,
      "step": 1023
    },
    {
      "epoch": 0.41442808721606716,
      "grad_norm": 1.2305500507354736,
      "learning_rate": 0.00023789963577498989,
      "loss": 0.0558,
      "step": 1024
    },
    {
      "epoch": 0.41483280214498913,
      "grad_norm": 0.7854287028312683,
      "learning_rate": 0.00023783893160663698,
      "loss": 0.0216,
      "step": 1025
    },
    {
      "epoch": 0.41523751707391104,
      "grad_norm": 2.949735403060913,
      "learning_rate": 0.00023777822743828407,
      "loss": 0.0538,
      "step": 1026
    },
    {
      "epoch": 0.415642232002833,
      "grad_norm": 1.9768462181091309,
      "learning_rate": 0.00023771752326993118,
      "loss": 0.0428,
      "step": 1027
    },
    {
      "epoch": 0.4160469469317549,
      "grad_norm": 1.230392575263977,
      "learning_rate": 0.0002376568191015783,
      "loss": 0.0477,
      "step": 1028
    },
    {
      "epoch": 0.4164516618606769,
      "grad_norm": 2.9634909629821777,
      "learning_rate": 0.0002375961149332254,
      "loss": 0.0873,
      "step": 1029
    },
    {
      "epoch": 0.4168563767895988,
      "grad_norm": 0.5686731338500977,
      "learning_rate": 0.0002375354107648725,
      "loss": 0.053,
      "step": 1030
    },
    {
      "epoch": 0.41726109171852077,
      "grad_norm": 2.404881000518799,
      "learning_rate": 0.0002374747065965196,
      "loss": 0.083,
      "step": 1031
    },
    {
      "epoch": 0.4176658066474427,
      "grad_norm": 0.8590404391288757,
      "learning_rate": 0.00023741400242816672,
      "loss": 0.0294,
      "step": 1032
    },
    {
      "epoch": 0.41807052157636465,
      "grad_norm": 3.2862539291381836,
      "learning_rate": 0.0002373532982598138,
      "loss": 0.0465,
      "step": 1033
    },
    {
      "epoch": 0.41847523650528656,
      "grad_norm": 3.7982470989227295,
      "learning_rate": 0.00023729259409146093,
      "loss": 0.0769,
      "step": 1034
    },
    {
      "epoch": 0.41887995143420853,
      "grad_norm": 4.717284679412842,
      "learning_rate": 0.00023723188992310804,
      "loss": 0.1142,
      "step": 1035
    },
    {
      "epoch": 0.41928466636313044,
      "grad_norm": 1.3379836082458496,
      "learning_rate": 0.00023717118575475516,
      "loss": 0.0362,
      "step": 1036
    },
    {
      "epoch": 0.4196893812920524,
      "grad_norm": 4.660694122314453,
      "learning_rate": 0.00023711048158640222,
      "loss": 0.1708,
      "step": 1037
    },
    {
      "epoch": 0.4200940962209744,
      "grad_norm": 1.7786778211593628,
      "learning_rate": 0.00023704977741804934,
      "loss": 0.0321,
      "step": 1038
    },
    {
      "epoch": 0.4204988111498963,
      "grad_norm": 0.4674426019191742,
      "learning_rate": 0.00023698907324969646,
      "loss": 0.0098,
      "step": 1039
    },
    {
      "epoch": 0.42090352607881826,
      "grad_norm": 3.798171281814575,
      "learning_rate": 0.00023692836908134358,
      "loss": 0.0461,
      "step": 1040
    },
    {
      "epoch": 0.42130824100774017,
      "grad_norm": 4.785565376281738,
      "learning_rate": 0.0002368676649129907,
      "loss": 0.1512,
      "step": 1041
    },
    {
      "epoch": 0.42171295593666214,
      "grad_norm": 1.846943974494934,
      "learning_rate": 0.00023680696074463776,
      "loss": 0.1165,
      "step": 1042
    },
    {
      "epoch": 0.42211767086558405,
      "grad_norm": 0.47116655111312866,
      "learning_rate": 0.00023674625657628488,
      "loss": 0.0166,
      "step": 1043
    },
    {
      "epoch": 0.422522385794506,
      "grad_norm": 1.2821426391601562,
      "learning_rate": 0.000236685552407932,
      "loss": 0.0732,
      "step": 1044
    },
    {
      "epoch": 0.42292710072342793,
      "grad_norm": 0.45601364970207214,
      "learning_rate": 0.0002366248482395791,
      "loss": 0.0117,
      "step": 1045
    },
    {
      "epoch": 0.4233318156523499,
      "grad_norm": 3.3287110328674316,
      "learning_rate": 0.0002365641440712262,
      "loss": 0.082,
      "step": 1046
    },
    {
      "epoch": 0.4237365305812718,
      "grad_norm": 1.780035138130188,
      "learning_rate": 0.00023650343990287332,
      "loss": 0.0264,
      "step": 1047
    },
    {
      "epoch": 0.4241412455101938,
      "grad_norm": 4.717859268188477,
      "learning_rate": 0.0002364427357345204,
      "loss": 0.118,
      "step": 1048
    },
    {
      "epoch": 0.4245459604391157,
      "grad_norm": 3.638721227645874,
      "learning_rate": 0.00023638203156616753,
      "loss": 0.1161,
      "step": 1049
    },
    {
      "epoch": 0.42495067536803766,
      "grad_norm": 3.1376171112060547,
      "learning_rate": 0.00023632132739781462,
      "loss": 0.0974,
      "step": 1050
    },
    {
      "epoch": 0.42535539029695957,
      "grad_norm": 2.7846248149871826,
      "learning_rate": 0.00023626062322946174,
      "loss": 0.0311,
      "step": 1051
    },
    {
      "epoch": 0.42576010522588154,
      "grad_norm": 1.9203933477401733,
      "learning_rate": 0.00023619991906110885,
      "loss": 0.0507,
      "step": 1052
    },
    {
      "epoch": 0.42616482015480345,
      "grad_norm": 1.7713038921356201,
      "learning_rate": 0.00023613921489275594,
      "loss": 0.0248,
      "step": 1053
    },
    {
      "epoch": 0.4265695350837254,
      "grad_norm": 2.210858106613159,
      "learning_rate": 0.00023607851072440303,
      "loss": 0.0586,
      "step": 1054
    },
    {
      "epoch": 0.4269742500126473,
      "grad_norm": 1.7678499221801758,
      "learning_rate": 0.00023601780655605015,
      "loss": 0.0684,
      "step": 1055
    },
    {
      "epoch": 0.4273789649415693,
      "grad_norm": 1.8656306266784668,
      "learning_rate": 0.00023595710238769727,
      "loss": 0.0869,
      "step": 1056
    },
    {
      "epoch": 0.4277836798704912,
      "grad_norm": 3.3731813430786133,
      "learning_rate": 0.0002358963982193444,
      "loss": 0.1077,
      "step": 1057
    },
    {
      "epoch": 0.4281883947994132,
      "grad_norm": 2.694059133529663,
      "learning_rate": 0.0002358356940509915,
      "loss": 0.032,
      "step": 1058
    },
    {
      "epoch": 0.4285931097283351,
      "grad_norm": 0.5243057608604431,
      "learning_rate": 0.00023577498988263857,
      "loss": 0.0414,
      "step": 1059
    },
    {
      "epoch": 0.42899782465725705,
      "grad_norm": 2.22044038772583,
      "learning_rate": 0.00023571428571428569,
      "loss": 0.042,
      "step": 1060
    },
    {
      "epoch": 0.42940253958617897,
      "grad_norm": 0.9584161639213562,
      "learning_rate": 0.0002356535815459328,
      "loss": 0.0529,
      "step": 1061
    },
    {
      "epoch": 0.42980725451510093,
      "grad_norm": 1.0000560283660889,
      "learning_rate": 0.00023559287737757992,
      "loss": 0.0758,
      "step": 1062
    },
    {
      "epoch": 0.43021196944402285,
      "grad_norm": 3.788637638092041,
      "learning_rate": 0.00023553217320922704,
      "loss": 0.0868,
      "step": 1063
    },
    {
      "epoch": 0.4306166843729448,
      "grad_norm": 2.9344825744628906,
      "learning_rate": 0.0002354714690408741,
      "loss": 0.0403,
      "step": 1064
    },
    {
      "epoch": 0.4310213993018667,
      "grad_norm": 1.8205486536026,
      "learning_rate": 0.00023541076487252122,
      "loss": 0.0443,
      "step": 1065
    },
    {
      "epoch": 0.4314261142307887,
      "grad_norm": 2.262218952178955,
      "learning_rate": 0.00023535006070416834,
      "loss": 0.0804,
      "step": 1066
    },
    {
      "epoch": 0.4318308291597106,
      "grad_norm": 0.33702996373176575,
      "learning_rate": 0.00023528935653581546,
      "loss": 0.0095,
      "step": 1067
    },
    {
      "epoch": 0.4322355440886326,
      "grad_norm": 4.288097381591797,
      "learning_rate": 0.00023522865236746255,
      "loss": 0.1298,
      "step": 1068
    },
    {
      "epoch": 0.4326402590175545,
      "grad_norm": 4.9828009605407715,
      "learning_rate": 0.00023516794819910966,
      "loss": 0.1583,
      "step": 1069
    },
    {
      "epoch": 0.43304497394647645,
      "grad_norm": 2.4090704917907715,
      "learning_rate": 0.00023510724403075675,
      "loss": 0.0879,
      "step": 1070
    },
    {
      "epoch": 0.43344968887539836,
      "grad_norm": 2.05975079536438,
      "learning_rate": 0.00023504653986240387,
      "loss": 0.0365,
      "step": 1071
    },
    {
      "epoch": 0.43385440380432033,
      "grad_norm": 0.8169935345649719,
      "learning_rate": 0.00023498583569405096,
      "loss": 0.0537,
      "step": 1072
    },
    {
      "epoch": 0.4342591187332423,
      "grad_norm": 1.0334705114364624,
      "learning_rate": 0.00023492513152569808,
      "loss": 0.0366,
      "step": 1073
    },
    {
      "epoch": 0.4346638336621642,
      "grad_norm": 1.3205864429473877,
      "learning_rate": 0.0002348644273573452,
      "loss": 0.0299,
      "step": 1074
    },
    {
      "epoch": 0.4350685485910862,
      "grad_norm": 2.239816665649414,
      "learning_rate": 0.0002348037231889923,
      "loss": 0.0282,
      "step": 1075
    },
    {
      "epoch": 0.4354732635200081,
      "grad_norm": 2.0976781845092773,
      "learning_rate": 0.00023474301902063938,
      "loss": 0.0479,
      "step": 1076
    },
    {
      "epoch": 0.43587797844893006,
      "grad_norm": 1.2257256507873535,
      "learning_rate": 0.0002346823148522865,
      "loss": 0.0885,
      "step": 1077
    },
    {
      "epoch": 0.43628269337785197,
      "grad_norm": 0.9512069821357727,
      "learning_rate": 0.0002346216106839336,
      "loss": 0.0654,
      "step": 1078
    },
    {
      "epoch": 0.43668740830677394,
      "grad_norm": 2.315716505050659,
      "learning_rate": 0.00023456090651558073,
      "loss": 0.0477,
      "step": 1079
    },
    {
      "epoch": 0.43709212323569585,
      "grad_norm": 1.2900294065475464,
      "learning_rate": 0.00023450020234722785,
      "loss": 0.0383,
      "step": 1080
    },
    {
      "epoch": 0.4374968381646178,
      "grad_norm": 0.554391086101532,
      "learning_rate": 0.0002344394981788749,
      "loss": 0.0283,
      "step": 1081
    },
    {
      "epoch": 0.43790155309353973,
      "grad_norm": 1.9523264169692993,
      "learning_rate": 0.00023437879401052203,
      "loss": 0.0446,
      "step": 1082
    },
    {
      "epoch": 0.4383062680224617,
      "grad_norm": 3.6462652683258057,
      "learning_rate": 0.00023431808984216915,
      "loss": 0.0712,
      "step": 1083
    },
    {
      "epoch": 0.4387109829513836,
      "grad_norm": 0.4185962378978729,
      "learning_rate": 0.00023425738567381626,
      "loss": 0.0204,
      "step": 1084
    },
    {
      "epoch": 0.4391156978803056,
      "grad_norm": 2.846853256225586,
      "learning_rate": 0.00023419668150546336,
      "loss": 0.037,
      "step": 1085
    },
    {
      "epoch": 0.4395204128092275,
      "grad_norm": 3.1607086658477783,
      "learning_rate": 0.00023413597733711045,
      "loss": 0.0388,
      "step": 1086
    },
    {
      "epoch": 0.43992512773814946,
      "grad_norm": 2.727428913116455,
      "learning_rate": 0.00023407527316875756,
      "loss": 0.0797,
      "step": 1087
    },
    {
      "epoch": 0.44032984266707137,
      "grad_norm": 2.197791814804077,
      "learning_rate": 0.00023401456900040468,
      "loss": 0.1199,
      "step": 1088
    },
    {
      "epoch": 0.44073455759599334,
      "grad_norm": 2.3125603199005127,
      "learning_rate": 0.00023395386483205177,
      "loss": 0.0473,
      "step": 1089
    },
    {
      "epoch": 0.44113927252491525,
      "grad_norm": 3.9410717487335205,
      "learning_rate": 0.0002338931606636989,
      "loss": 0.0844,
      "step": 1090
    },
    {
      "epoch": 0.4415439874538372,
      "grad_norm": 4.014216423034668,
      "learning_rate": 0.00023383245649534598,
      "loss": 0.057,
      "step": 1091
    },
    {
      "epoch": 0.44194870238275913,
      "grad_norm": 0.4775863587856293,
      "learning_rate": 0.0002337717523269931,
      "loss": 0.0048,
      "step": 1092
    },
    {
      "epoch": 0.4423534173116811,
      "grad_norm": 0.4194650948047638,
      "learning_rate": 0.00023371104815864021,
      "loss": 0.0122,
      "step": 1093
    },
    {
      "epoch": 0.442758132240603,
      "grad_norm": 0.6701355576515198,
      "learning_rate": 0.0002336503439902873,
      "loss": 0.0411,
      "step": 1094
    },
    {
      "epoch": 0.443162847169525,
      "grad_norm": 1.1733243465423584,
      "learning_rate": 0.00023358963982193442,
      "loss": 0.0169,
      "step": 1095
    },
    {
      "epoch": 0.4435675620984469,
      "grad_norm": 0.3568701446056366,
      "learning_rate": 0.00023352893565358154,
      "loss": 0.0258,
      "step": 1096
    },
    {
      "epoch": 0.44397227702736886,
      "grad_norm": 4.02470064163208,
      "learning_rate": 0.00023346823148522863,
      "loss": 0.062,
      "step": 1097
    },
    {
      "epoch": 0.44437699195629077,
      "grad_norm": 0.8006343841552734,
      "learning_rate": 0.00023340752731687572,
      "loss": 0.0574,
      "step": 1098
    },
    {
      "epoch": 0.44478170688521274,
      "grad_norm": 1.5015782117843628,
      "learning_rate": 0.00023334682314852284,
      "loss": 0.0563,
      "step": 1099
    },
    {
      "epoch": 0.44518642181413465,
      "grad_norm": 2.0437870025634766,
      "learning_rate": 0.00023328611898016996,
      "loss": 0.0353,
      "step": 1100
    },
    {
      "epoch": 0.4455911367430566,
      "grad_norm": 1.588769555091858,
      "learning_rate": 0.00023322541481181707,
      "loss": 0.0687,
      "step": 1101
    },
    {
      "epoch": 0.4459958516719785,
      "grad_norm": 1.771862268447876,
      "learning_rate": 0.00023316471064346414,
      "loss": 0.0562,
      "step": 1102
    },
    {
      "epoch": 0.4464005666009005,
      "grad_norm": 1.62269926071167,
      "learning_rate": 0.00023310400647511126,
      "loss": 0.0208,
      "step": 1103
    },
    {
      "epoch": 0.4468052815298224,
      "grad_norm": 1.2440574169158936,
      "learning_rate": 0.00023304330230675837,
      "loss": 0.1145,
      "step": 1104
    },
    {
      "epoch": 0.4472099964587444,
      "grad_norm": 0.37191277742385864,
      "learning_rate": 0.0002329825981384055,
      "loss": 0.0289,
      "step": 1105
    },
    {
      "epoch": 0.4476147113876663,
      "grad_norm": 5.4362311363220215,
      "learning_rate": 0.0002329218939700526,
      "loss": 0.0911,
      "step": 1106
    },
    {
      "epoch": 0.44801942631658825,
      "grad_norm": 4.597850322723389,
      "learning_rate": 0.0002328611898016997,
      "loss": 0.1187,
      "step": 1107
    },
    {
      "epoch": 0.44842414124551017,
      "grad_norm": 1.7995123863220215,
      "learning_rate": 0.0002328004856333468,
      "loss": 0.0814,
      "step": 1108
    },
    {
      "epoch": 0.44882885617443213,
      "grad_norm": 0.9766404628753662,
      "learning_rate": 0.0002327397814649939,
      "loss": 0.0127,
      "step": 1109
    },
    {
      "epoch": 0.4492335711033541,
      "grad_norm": 5.124098777770996,
      "learning_rate": 0.00023267907729664102,
      "loss": 0.1082,
      "step": 1110
    },
    {
      "epoch": 0.449638286032276,
      "grad_norm": 0.5403643846511841,
      "learning_rate": 0.00023261837312828812,
      "loss": 0.0178,
      "step": 1111
    },
    {
      "epoch": 0.450043000961198,
      "grad_norm": 3.360283374786377,
      "learning_rate": 0.00023255766895993523,
      "loss": 0.0568,
      "step": 1112
    },
    {
      "epoch": 0.4504477158901199,
      "grad_norm": 2.343146800994873,
      "learning_rate": 0.00023249696479158232,
      "loss": 0.0629,
      "step": 1113
    },
    {
      "epoch": 0.45085243081904186,
      "grad_norm": 1.9560160636901855,
      "learning_rate": 0.00023243626062322944,
      "loss": 0.0647,
      "step": 1114
    },
    {
      "epoch": 0.4512571457479638,
      "grad_norm": 3.243638515472412,
      "learning_rate": 0.00023237555645487653,
      "loss": 0.132,
      "step": 1115
    },
    {
      "epoch": 0.45166186067688574,
      "grad_norm": 0.43347617983818054,
      "learning_rate": 0.00023231485228652365,
      "loss": 0.0263,
      "step": 1116
    },
    {
      "epoch": 0.45206657560580765,
      "grad_norm": 2.5036802291870117,
      "learning_rate": 0.00023225414811817077,
      "loss": 0.0799,
      "step": 1117
    },
    {
      "epoch": 0.4524712905347296,
      "grad_norm": 0.5603606104850769,
      "learning_rate": 0.00023219344394981788,
      "loss": 0.0179,
      "step": 1118
    },
    {
      "epoch": 0.45287600546365153,
      "grad_norm": 0.7128017544746399,
      "learning_rate": 0.00023213273978146495,
      "loss": 0.033,
      "step": 1119
    },
    {
      "epoch": 0.4532807203925735,
      "grad_norm": 0.6767462491989136,
      "learning_rate": 0.00023207203561311207,
      "loss": 0.0205,
      "step": 1120
    },
    {
      "epoch": 0.4536854353214954,
      "grad_norm": 1.8806129693984985,
      "learning_rate": 0.00023201133144475918,
      "loss": 0.0426,
      "step": 1121
    },
    {
      "epoch": 0.4540901502504174,
      "grad_norm": 4.95392370223999,
      "learning_rate": 0.0002319506272764063,
      "loss": 0.148,
      "step": 1122
    },
    {
      "epoch": 0.4544948651793393,
      "grad_norm": 2.2022736072540283,
      "learning_rate": 0.00023188992310805342,
      "loss": 0.035,
      "step": 1123
    },
    {
      "epoch": 0.45489958010826126,
      "grad_norm": 0.8177714347839355,
      "learning_rate": 0.00023182921893970048,
      "loss": 0.0342,
      "step": 1124
    },
    {
      "epoch": 0.45530429503718317,
      "grad_norm": 5.011894226074219,
      "learning_rate": 0.0002317685147713476,
      "loss": 0.1138,
      "step": 1125
    },
    {
      "epoch": 0.45570900996610514,
      "grad_norm": 1.437825083732605,
      "learning_rate": 0.00023170781060299472,
      "loss": 0.0493,
      "step": 1126
    },
    {
      "epoch": 0.45611372489502705,
      "grad_norm": 2.4518511295318604,
      "learning_rate": 0.00023164710643464183,
      "loss": 0.0431,
      "step": 1127
    },
    {
      "epoch": 0.456518439823949,
      "grad_norm": 1.3694790601730347,
      "learning_rate": 0.00023158640226628895,
      "loss": 0.0316,
      "step": 1128
    },
    {
      "epoch": 0.45692315475287093,
      "grad_norm": 0.9385908842086792,
      "learning_rate": 0.00023152569809793604,
      "loss": 0.0694,
      "step": 1129
    },
    {
      "epoch": 0.4573278696817929,
      "grad_norm": 1.244499683380127,
      "learning_rate": 0.00023146499392958313,
      "loss": 0.0442,
      "step": 1130
    },
    {
      "epoch": 0.4577325846107148,
      "grad_norm": 2.312593936920166,
      "learning_rate": 0.00023140428976123025,
      "loss": 0.0635,
      "step": 1131
    },
    {
      "epoch": 0.4581372995396368,
      "grad_norm": 3.1040329933166504,
      "learning_rate": 0.00023134358559287737,
      "loss": 0.0853,
      "step": 1132
    },
    {
      "epoch": 0.4585420144685587,
      "grad_norm": 0.7427851557731628,
      "learning_rate": 0.00023128288142452446,
      "loss": 0.0283,
      "step": 1133
    },
    {
      "epoch": 0.45894672939748066,
      "grad_norm": 1.0183520317077637,
      "learning_rate": 0.00023122217725617158,
      "loss": 0.0206,
      "step": 1134
    },
    {
      "epoch": 0.45935144432640257,
      "grad_norm": 3.748988151550293,
      "learning_rate": 0.00023116147308781867,
      "loss": 0.0737,
      "step": 1135
    },
    {
      "epoch": 0.45975615925532454,
      "grad_norm": 1.7411025762557983,
      "learning_rate": 0.00023110076891946578,
      "loss": 0.1326,
      "step": 1136
    },
    {
      "epoch": 0.46016087418424645,
      "grad_norm": 2.2164790630340576,
      "learning_rate": 0.00023104006475111288,
      "loss": 0.1139,
      "step": 1137
    },
    {
      "epoch": 0.4605655891131684,
      "grad_norm": 2.737461805343628,
      "learning_rate": 0.00023097936058276,
      "loss": 0.0546,
      "step": 1138
    },
    {
      "epoch": 0.46097030404209033,
      "grad_norm": 1.8950461149215698,
      "learning_rate": 0.0002309186564144071,
      "loss": 0.0506,
      "step": 1139
    },
    {
      "epoch": 0.4613750189710123,
      "grad_norm": 0.17021916806697845,
      "learning_rate": 0.00023085795224605423,
      "loss": 0.0129,
      "step": 1140
    },
    {
      "epoch": 0.4617797338999342,
      "grad_norm": 1.6203750371932983,
      "learning_rate": 0.0002307972480777013,
      "loss": 0.0788,
      "step": 1141
    },
    {
      "epoch": 0.4621844488288562,
      "grad_norm": 3.608870267868042,
      "learning_rate": 0.0002307365439093484,
      "loss": 0.1181,
      "step": 1142
    },
    {
      "epoch": 0.4625891637577781,
      "grad_norm": 0.6528216600418091,
      "learning_rate": 0.00023067583974099553,
      "loss": 0.0211,
      "step": 1143
    },
    {
      "epoch": 0.46299387868670006,
      "grad_norm": 3.551862955093384,
      "learning_rate": 0.00023061513557264264,
      "loss": 0.0612,
      "step": 1144
    },
    {
      "epoch": 0.463398593615622,
      "grad_norm": 0.4060385525226593,
      "learning_rate": 0.00023055443140428976,
      "loss": 0.0263,
      "step": 1145
    },
    {
      "epoch": 0.46380330854454394,
      "grad_norm": 3.9579172134399414,
      "learning_rate": 0.00023049372723593683,
      "loss": 0.0655,
      "step": 1146
    },
    {
      "epoch": 0.4642080234734659,
      "grad_norm": 4.964641094207764,
      "learning_rate": 0.00023043302306758394,
      "loss": 0.127,
      "step": 1147
    },
    {
      "epoch": 0.4646127384023878,
      "grad_norm": 2.36132550239563,
      "learning_rate": 0.00023037231889923106,
      "loss": 0.0573,
      "step": 1148
    },
    {
      "epoch": 0.4650174533313098,
      "grad_norm": 0.5831285119056702,
      "learning_rate": 0.00023031161473087818,
      "loss": 0.0375,
      "step": 1149
    },
    {
      "epoch": 0.4654221682602317,
      "grad_norm": 0.26207754015922546,
      "learning_rate": 0.00023025091056252527,
      "loss": 0.0214,
      "step": 1150
    },
    {
      "epoch": 0.46582688318915366,
      "grad_norm": 1.0469002723693848,
      "learning_rate": 0.00023019020639417239,
      "loss": 0.0308,
      "step": 1151
    },
    {
      "epoch": 0.4662315981180756,
      "grad_norm": 1.1088972091674805,
      "learning_rate": 0.00023012950222581948,
      "loss": 0.0087,
      "step": 1152
    },
    {
      "epoch": 0.46663631304699754,
      "grad_norm": 2.2956833839416504,
      "learning_rate": 0.0002300687980574666,
      "loss": 0.0413,
      "step": 1153
    },
    {
      "epoch": 0.46704102797591945,
      "grad_norm": 1.4015320539474487,
      "learning_rate": 0.00023000809388911368,
      "loss": 0.0514,
      "step": 1154
    },
    {
      "epoch": 0.4674457429048414,
      "grad_norm": 2.55195951461792,
      "learning_rate": 0.0002299473897207608,
      "loss": 0.2009,
      "step": 1155
    },
    {
      "epoch": 0.46785045783376333,
      "grad_norm": 2.413362741470337,
      "learning_rate": 0.00022988668555240792,
      "loss": 0.0492,
      "step": 1156
    },
    {
      "epoch": 0.4682551727626853,
      "grad_norm": 1.6028162240982056,
      "learning_rate": 0.000229825981384055,
      "loss": 0.0153,
      "step": 1157
    },
    {
      "epoch": 0.4686598876916072,
      "grad_norm": 2.162950038909912,
      "learning_rate": 0.00022976527721570213,
      "loss": 0.0331,
      "step": 1158
    },
    {
      "epoch": 0.4690646026205292,
      "grad_norm": 1.7795571088790894,
      "learning_rate": 0.00022970457304734922,
      "loss": 0.0747,
      "step": 1159
    },
    {
      "epoch": 0.4694693175494511,
      "grad_norm": 2.5510189533233643,
      "learning_rate": 0.00022964386887899634,
      "loss": 0.0751,
      "step": 1160
    },
    {
      "epoch": 0.46987403247837306,
      "grad_norm": 1.4997308254241943,
      "learning_rate": 0.00022958316471064345,
      "loss": 0.0414,
      "step": 1161
    },
    {
      "epoch": 0.470278747407295,
      "grad_norm": 3.2620224952697754,
      "learning_rate": 0.00022952246054229057,
      "loss": 0.0525,
      "step": 1162
    },
    {
      "epoch": 0.47068346233621694,
      "grad_norm": 1.6054562330245972,
      "learning_rate": 0.00022946175637393763,
      "loss": 0.0425,
      "step": 1163
    },
    {
      "epoch": 0.47108817726513885,
      "grad_norm": 0.7463842630386353,
      "learning_rate": 0.00022940105220558475,
      "loss": 0.0129,
      "step": 1164
    },
    {
      "epoch": 0.4714928921940608,
      "grad_norm": 2.622443914413452,
      "learning_rate": 0.00022934034803723187,
      "loss": 0.0514,
      "step": 1165
    },
    {
      "epoch": 0.47189760712298273,
      "grad_norm": 6.068304538726807,
      "learning_rate": 0.000229279643868879,
      "loss": 0.1188,
      "step": 1166
    },
    {
      "epoch": 0.4723023220519047,
      "grad_norm": 3.0839498043060303,
      "learning_rate": 0.0002292189397005261,
      "loss": 0.1211,
      "step": 1167
    },
    {
      "epoch": 0.4727070369808266,
      "grad_norm": 1.499857783317566,
      "learning_rate": 0.00022915823553217317,
      "loss": 0.0632,
      "step": 1168
    },
    {
      "epoch": 0.4731117519097486,
      "grad_norm": 1.7259752750396729,
      "learning_rate": 0.0002290975313638203,
      "loss": 0.1148,
      "step": 1169
    },
    {
      "epoch": 0.4735164668386705,
      "grad_norm": 1.3730136156082153,
      "learning_rate": 0.0002290368271954674,
      "loss": 0.0449,
      "step": 1170
    },
    {
      "epoch": 0.47392118176759246,
      "grad_norm": 1.2506574392318726,
      "learning_rate": 0.00022897612302711452,
      "loss": 0.0406,
      "step": 1171
    },
    {
      "epoch": 0.47432589669651437,
      "grad_norm": 1.9523319005966187,
      "learning_rate": 0.0002289154188587616,
      "loss": 0.0566,
      "step": 1172
    },
    {
      "epoch": 0.47473061162543634,
      "grad_norm": 3.1376547813415527,
      "learning_rate": 0.00022885471469040873,
      "loss": 0.0344,
      "step": 1173
    },
    {
      "epoch": 0.47513532655435825,
      "grad_norm": 1.1486972570419312,
      "learning_rate": 0.00022879401052205582,
      "loss": 0.0258,
      "step": 1174
    },
    {
      "epoch": 0.4755400414832802,
      "grad_norm": 0.5382946729660034,
      "learning_rate": 0.00022873330635370294,
      "loss": 0.0463,
      "step": 1175
    },
    {
      "epoch": 0.47594475641220213,
      "grad_norm": 4.298567771911621,
      "learning_rate": 0.00022867260218535003,
      "loss": 0.1088,
      "step": 1176
    },
    {
      "epoch": 0.4763494713411241,
      "grad_norm": 2.13274884223938,
      "learning_rate": 0.00022861189801699715,
      "loss": 0.0669,
      "step": 1177
    },
    {
      "epoch": 0.476754186270046,
      "grad_norm": 2.5478999614715576,
      "learning_rate": 0.00022855119384864426,
      "loss": 0.0834,
      "step": 1178
    },
    {
      "epoch": 0.477158901198968,
      "grad_norm": 2.204749345779419,
      "learning_rate": 0.00022849048968029135,
      "loss": 0.0599,
      "step": 1179
    },
    {
      "epoch": 0.47756361612788994,
      "grad_norm": 2.189638137817383,
      "learning_rate": 0.00022842978551193844,
      "loss": 0.0461,
      "step": 1180
    },
    {
      "epoch": 0.47796833105681186,
      "grad_norm": 5.003203868865967,
      "learning_rate": 0.00022836908134358556,
      "loss": 0.1646,
      "step": 1181
    },
    {
      "epoch": 0.4783730459857338,
      "grad_norm": 2.1948513984680176,
      "learning_rate": 0.00022830837717523268,
      "loss": 0.0526,
      "step": 1182
    },
    {
      "epoch": 0.47877776091465574,
      "grad_norm": 0.604621946811676,
      "learning_rate": 0.0002282476730068798,
      "loss": 0.0232,
      "step": 1183
    },
    {
      "epoch": 0.4791824758435777,
      "grad_norm": 0.45030680298805237,
      "learning_rate": 0.00022818696883852692,
      "loss": 0.0153,
      "step": 1184
    },
    {
      "epoch": 0.4795871907724996,
      "grad_norm": 1.7429063320159912,
      "learning_rate": 0.00022812626467017398,
      "loss": 0.0647,
      "step": 1185
    },
    {
      "epoch": 0.4799919057014216,
      "grad_norm": 0.34943297505378723,
      "learning_rate": 0.0002280655605018211,
      "loss": 0.0183,
      "step": 1186
    },
    {
      "epoch": 0.4803966206303435,
      "grad_norm": 0.32777631282806396,
      "learning_rate": 0.00022800485633346821,
      "loss": 0.0054,
      "step": 1187
    },
    {
      "epoch": 0.48080133555926546,
      "grad_norm": 3.6037471294403076,
      "learning_rate": 0.00022794415216511533,
      "loss": 0.06,
      "step": 1188
    },
    {
      "epoch": 0.4812060504881874,
      "grad_norm": 0.5448558926582336,
      "learning_rate": 0.00022788344799676245,
      "loss": 0.0187,
      "step": 1189
    },
    {
      "epoch": 0.48161076541710934,
      "grad_norm": 2.3346426486968994,
      "learning_rate": 0.0002278227438284095,
      "loss": 0.1428,
      "step": 1190
    },
    {
      "epoch": 0.48201548034603126,
      "grad_norm": 2.219336986541748,
      "learning_rate": 0.00022776203966005663,
      "loss": 0.1132,
      "step": 1191
    },
    {
      "epoch": 0.4824201952749532,
      "grad_norm": 3.040512800216675,
      "learning_rate": 0.00022770133549170375,
      "loss": 0.0882,
      "step": 1192
    },
    {
      "epoch": 0.48282491020387514,
      "grad_norm": 0.8141483664512634,
      "learning_rate": 0.00022764063132335087,
      "loss": 0.0422,
      "step": 1193
    },
    {
      "epoch": 0.4832296251327971,
      "grad_norm": 1.7412939071655273,
      "learning_rate": 0.00022757992715499796,
      "loss": 0.0204,
      "step": 1194
    },
    {
      "epoch": 0.483634340061719,
      "grad_norm": 2.742347240447998,
      "learning_rate": 0.00022751922298664507,
      "loss": 0.0408,
      "step": 1195
    },
    {
      "epoch": 0.484039054990641,
      "grad_norm": 1.4792532920837402,
      "learning_rate": 0.00022745851881829216,
      "loss": 0.0992,
      "step": 1196
    },
    {
      "epoch": 0.4844437699195629,
      "grad_norm": 1.1145482063293457,
      "learning_rate": 0.00022739781464993928,
      "loss": 0.0326,
      "step": 1197
    },
    {
      "epoch": 0.48484848484848486,
      "grad_norm": 0.7686268091201782,
      "learning_rate": 0.00022733711048158637,
      "loss": 0.0187,
      "step": 1198
    },
    {
      "epoch": 0.4852531997774068,
      "grad_norm": 0.347028523683548,
      "learning_rate": 0.0002272764063132335,
      "loss": 0.0261,
      "step": 1199
    },
    {
      "epoch": 0.48565791470632874,
      "grad_norm": 0.44553452730178833,
      "learning_rate": 0.0002272157021448806,
      "loss": 0.0206,
      "step": 1200
    },
    {
      "epoch": 0.48606262963525065,
      "grad_norm": 0.8570733070373535,
      "learning_rate": 0.0002271549979765277,
      "loss": 0.046,
      "step": 1201
    },
    {
      "epoch": 0.4864673445641726,
      "grad_norm": 0.46652987599372864,
      "learning_rate": 0.0002270942938081748,
      "loss": 0.0184,
      "step": 1202
    },
    {
      "epoch": 0.48687205949309453,
      "grad_norm": 2.1485202312469482,
      "learning_rate": 0.0002270335896398219,
      "loss": 0.0808,
      "step": 1203
    },
    {
      "epoch": 0.4872767744220165,
      "grad_norm": 3.8890392780303955,
      "learning_rate": 0.00022697288547146902,
      "loss": 0.1424,
      "step": 1204
    },
    {
      "epoch": 0.4876814893509384,
      "grad_norm": 1.066042184829712,
      "learning_rate": 0.00022691218130311614,
      "loss": 0.0439,
      "step": 1205
    },
    {
      "epoch": 0.4880862042798604,
      "grad_norm": 1.4578437805175781,
      "learning_rate": 0.00022685147713476326,
      "loss": 0.0956,
      "step": 1206
    },
    {
      "epoch": 0.4884909192087823,
      "grad_norm": 0.6418514847755432,
      "learning_rate": 0.00022679077296641032,
      "loss": 0.032,
      "step": 1207
    },
    {
      "epoch": 0.48889563413770426,
      "grad_norm": 3.870816946029663,
      "learning_rate": 0.00022673006879805744,
      "loss": 0.1712,
      "step": 1208
    },
    {
      "epoch": 0.4893003490666262,
      "grad_norm": 2.504094362258911,
      "learning_rate": 0.00022666936462970456,
      "loss": 0.0812,
      "step": 1209
    },
    {
      "epoch": 0.48970506399554814,
      "grad_norm": 1.4154095649719238,
      "learning_rate": 0.00022660866046135168,
      "loss": 0.0753,
      "step": 1210
    },
    {
      "epoch": 0.49010977892447005,
      "grad_norm": 0.6516372561454773,
      "learning_rate": 0.00022654795629299877,
      "loss": 0.031,
      "step": 1211
    },
    {
      "epoch": 0.490514493853392,
      "grad_norm": 1.6409977674484253,
      "learning_rate": 0.00022648725212464586,
      "loss": 0.0658,
      "step": 1212
    },
    {
      "epoch": 0.49091920878231393,
      "grad_norm": 0.9144663214683533,
      "learning_rate": 0.00022642654795629297,
      "loss": 0.0456,
      "step": 1213
    },
    {
      "epoch": 0.4913239237112359,
      "grad_norm": 0.7616639137268066,
      "learning_rate": 0.0002263658437879401,
      "loss": 0.0373,
      "step": 1214
    },
    {
      "epoch": 0.4917286386401578,
      "grad_norm": 2.0930166244506836,
      "learning_rate": 0.00022630513961958718,
      "loss": 0.0491,
      "step": 1215
    },
    {
      "epoch": 0.4921333535690798,
      "grad_norm": 0.6250747442245483,
      "learning_rate": 0.0002262444354512343,
      "loss": 0.071,
      "step": 1216
    },
    {
      "epoch": 0.49253806849800175,
      "grad_norm": 2.836562156677246,
      "learning_rate": 0.00022618373128288142,
      "loss": 0.1432,
      "step": 1217
    },
    {
      "epoch": 0.49294278342692366,
      "grad_norm": 1.409144639968872,
      "learning_rate": 0.0002261230271145285,
      "loss": 0.1008,
      "step": 1218
    },
    {
      "epoch": 0.4933474983558456,
      "grad_norm": 1.9107720851898193,
      "learning_rate": 0.0002260623229461756,
      "loss": 0.0573,
      "step": 1219
    },
    {
      "epoch": 0.49375221328476754,
      "grad_norm": 1.3003900051116943,
      "learning_rate": 0.00022600161877782272,
      "loss": 0.0158,
      "step": 1220
    },
    {
      "epoch": 0.4941569282136895,
      "grad_norm": 0.3064455986022949,
      "learning_rate": 0.00022594091460946983,
      "loss": 0.0213,
      "step": 1221
    },
    {
      "epoch": 0.4945616431426114,
      "grad_norm": 0.6459449529647827,
      "learning_rate": 0.00022588021044111695,
      "loss": 0.0505,
      "step": 1222
    },
    {
      "epoch": 0.4949663580715334,
      "grad_norm": 0.9864842891693115,
      "learning_rate": 0.00022581950627276404,
      "loss": 0.0329,
      "step": 1223
    },
    {
      "epoch": 0.4953710730004553,
      "grad_norm": 3.314080238342285,
      "learning_rate": 0.00022575880210441113,
      "loss": 0.1113,
      "step": 1224
    },
    {
      "epoch": 0.49577578792937727,
      "grad_norm": 1.5853458642959595,
      "learning_rate": 0.00022569809793605825,
      "loss": 0.0357,
      "step": 1225
    },
    {
      "epoch": 0.4961805028582992,
      "grad_norm": 0.6822401881217957,
      "learning_rate": 0.00022563739376770537,
      "loss": 0.0504,
      "step": 1226
    },
    {
      "epoch": 0.49658521778722114,
      "grad_norm": 0.7629080414772034,
      "learning_rate": 0.00022557668959935248,
      "loss": 0.0244,
      "step": 1227
    },
    {
      "epoch": 0.49698993271614306,
      "grad_norm": 2.1292924880981445,
      "learning_rate": 0.0002255159854309996,
      "loss": 0.0587,
      "step": 1228
    },
    {
      "epoch": 0.497394647645065,
      "grad_norm": 0.29388779401779175,
      "learning_rate": 0.00022545528126264667,
      "loss": 0.0178,
      "step": 1229
    },
    {
      "epoch": 0.49779936257398694,
      "grad_norm": 0.9593489766120911,
      "learning_rate": 0.00022539457709429378,
      "loss": 0.0658,
      "step": 1230
    },
    {
      "epoch": 0.4982040775029089,
      "grad_norm": 0.26849332451820374,
      "learning_rate": 0.0002253338729259409,
      "loss": 0.0081,
      "step": 1231
    },
    {
      "epoch": 0.4986087924318308,
      "grad_norm": 1.6257959604263306,
      "learning_rate": 0.00022527316875758802,
      "loss": 0.1054,
      "step": 1232
    },
    {
      "epoch": 0.4990135073607528,
      "grad_norm": 3.8070855140686035,
      "learning_rate": 0.0002252124645892351,
      "loss": 0.0564,
      "step": 1233
    },
    {
      "epoch": 0.4994182222896747,
      "grad_norm": 5.773539066314697,
      "learning_rate": 0.0002251517604208822,
      "loss": 0.101,
      "step": 1234
    },
    {
      "epoch": 0.49982293721859666,
      "grad_norm": 2.609480857849121,
      "learning_rate": 0.00022509105625252932,
      "loss": 0.053,
      "step": 1235
    },
    {
      "epoch": 0.5002276521475186,
      "grad_norm": 1.685221552848816,
      "learning_rate": 0.00022503035208417644,
      "loss": 0.0716,
      "step": 1236
    },
    {
      "epoch": 0.5006323670764405,
      "grad_norm": 4.137184143066406,
      "learning_rate": 0.00022496964791582353,
      "loss": 0.0772,
      "step": 1237
    },
    {
      "epoch": 0.5010370820053625,
      "grad_norm": 0.6917895078659058,
      "learning_rate": 0.00022490894374747064,
      "loss": 0.0381,
      "step": 1238
    },
    {
      "epoch": 0.5014417969342844,
      "grad_norm": 5.334466934204102,
      "learning_rate": 0.00022484823957911776,
      "loss": 0.0934,
      "step": 1239
    },
    {
      "epoch": 0.5018465118632064,
      "grad_norm": 2.428896903991699,
      "learning_rate": 0.00022478753541076485,
      "loss": 0.0484,
      "step": 1240
    },
    {
      "epoch": 0.5022512267921283,
      "grad_norm": 0.9318876266479492,
      "learning_rate": 0.00022472683124241194,
      "loss": 0.0209,
      "step": 1241
    },
    {
      "epoch": 0.5026559417210502,
      "grad_norm": 0.5802958607673645,
      "learning_rate": 0.00022466612707405906,
      "loss": 0.016,
      "step": 1242
    },
    {
      "epoch": 0.5030606566499721,
      "grad_norm": 1.198583722114563,
      "learning_rate": 0.00022460542290570618,
      "loss": 0.0394,
      "step": 1243
    },
    {
      "epoch": 0.5034653715788941,
      "grad_norm": 0.7412123084068298,
      "learning_rate": 0.0002245447187373533,
      "loss": 0.0145,
      "step": 1244
    },
    {
      "epoch": 0.5038700865078161,
      "grad_norm": 2.1019937992095947,
      "learning_rate": 0.00022448401456900036,
      "loss": 0.0502,
      "step": 1245
    },
    {
      "epoch": 0.504274801436738,
      "grad_norm": 1.4908297061920166,
      "learning_rate": 0.00022442331040064748,
      "loss": 0.046,
      "step": 1246
    },
    {
      "epoch": 0.5046795163656599,
      "grad_norm": 7.57504940032959,
      "learning_rate": 0.0002243626062322946,
      "loss": 0.1151,
      "step": 1247
    },
    {
      "epoch": 0.5050842312945819,
      "grad_norm": 3.734349012374878,
      "learning_rate": 0.0002243019020639417,
      "loss": 0.04,
      "step": 1248
    },
    {
      "epoch": 0.5054889462235038,
      "grad_norm": 1.5196236371994019,
      "learning_rate": 0.00022424119789558883,
      "loss": 0.0997,
      "step": 1249
    },
    {
      "epoch": 0.5058936611524257,
      "grad_norm": 4.6154584884643555,
      "learning_rate": 0.00022418049372723592,
      "loss": 0.1071,
      "step": 1250
    },
    {
      "epoch": 0.5062983760813476,
      "grad_norm": 1.6635020971298218,
      "learning_rate": 0.000224119789558883,
      "loss": 0.0386,
      "step": 1251
    },
    {
      "epoch": 0.5067030910102697,
      "grad_norm": 2.6906180381774902,
      "learning_rate": 0.00022405908539053013,
      "loss": 0.0311,
      "step": 1252
    },
    {
      "epoch": 0.5071078059391916,
      "grad_norm": 1.5921406745910645,
      "learning_rate": 0.00022399838122217724,
      "loss": 0.0471,
      "step": 1253
    },
    {
      "epoch": 0.5075125208681135,
      "grad_norm": 1.7584229707717896,
      "learning_rate": 0.00022393767705382436,
      "loss": 0.0481,
      "step": 1254
    },
    {
      "epoch": 0.5079172357970355,
      "grad_norm": 0.4736618101596832,
      "learning_rate": 0.00022387697288547145,
      "loss": 0.0216,
      "step": 1255
    },
    {
      "epoch": 0.5083219507259574,
      "grad_norm": 2.346573829650879,
      "learning_rate": 0.00022381626871711854,
      "loss": 0.0493,
      "step": 1256
    },
    {
      "epoch": 0.5087266656548793,
      "grad_norm": 1.2243785858154297,
      "learning_rate": 0.00022375556454876566,
      "loss": 0.0312,
      "step": 1257
    },
    {
      "epoch": 0.5091313805838013,
      "grad_norm": 1.1948316097259521,
      "learning_rate": 0.00022369486038041278,
      "loss": 0.0378,
      "step": 1258
    },
    {
      "epoch": 0.5095360955127233,
      "grad_norm": 0.3210607171058655,
      "learning_rate": 0.00022363415621205987,
      "loss": 0.016,
      "step": 1259
    },
    {
      "epoch": 0.5099408104416452,
      "grad_norm": 4.676641941070557,
      "learning_rate": 0.000223573452043707,
      "loss": 0.0734,
      "step": 1260
    },
    {
      "epoch": 0.5103455253705671,
      "grad_norm": 2.9708597660064697,
      "learning_rate": 0.0002235127478753541,
      "loss": 0.083,
      "step": 1261
    },
    {
      "epoch": 0.510750240299489,
      "grad_norm": 0.9394223093986511,
      "learning_rate": 0.0002234520437070012,
      "loss": 0.0184,
      "step": 1262
    },
    {
      "epoch": 0.511154955228411,
      "grad_norm": 1.5263861417770386,
      "learning_rate": 0.00022339133953864829,
      "loss": 0.0307,
      "step": 1263
    },
    {
      "epoch": 0.511559670157333,
      "grad_norm": 2.330514669418335,
      "learning_rate": 0.0002233306353702954,
      "loss": 0.0434,
      "step": 1264
    },
    {
      "epoch": 0.5119643850862549,
      "grad_norm": 2.984499216079712,
      "learning_rate": 0.00022326993120194252,
      "loss": 0.0898,
      "step": 1265
    },
    {
      "epoch": 0.5123691000151768,
      "grad_norm": 1.3990230560302734,
      "learning_rate": 0.00022320922703358964,
      "loss": 0.0608,
      "step": 1266
    },
    {
      "epoch": 0.5127738149440988,
      "grad_norm": 3.9323596954345703,
      "learning_rate": 0.0002231485228652367,
      "loss": 0.0647,
      "step": 1267
    },
    {
      "epoch": 0.5131785298730207,
      "grad_norm": 3.0301718711853027,
      "learning_rate": 0.00022308781869688382,
      "loss": 0.0396,
      "step": 1268
    },
    {
      "epoch": 0.5135832448019426,
      "grad_norm": 2.6134140491485596,
      "learning_rate": 0.00022302711452853094,
      "loss": 0.0496,
      "step": 1269
    },
    {
      "epoch": 0.5139879597308645,
      "grad_norm": 1.9879875183105469,
      "learning_rate": 0.00022296641036017805,
      "loss": 0.0999,
      "step": 1270
    },
    {
      "epoch": 0.5143926746597866,
      "grad_norm": 1.4783344268798828,
      "learning_rate": 0.00022290570619182517,
      "loss": 0.0468,
      "step": 1271
    },
    {
      "epoch": 0.5147973895887085,
      "grad_norm": 0.8845518827438354,
      "learning_rate": 0.00022284500202347226,
      "loss": 0.0094,
      "step": 1272
    },
    {
      "epoch": 0.5152021045176304,
      "grad_norm": 1.6487609148025513,
      "learning_rate": 0.00022278429785511935,
      "loss": 0.0826,
      "step": 1273
    },
    {
      "epoch": 0.5156068194465523,
      "grad_norm": 0.5910059809684753,
      "learning_rate": 0.00022272359368676647,
      "loss": 0.0396,
      "step": 1274
    },
    {
      "epoch": 0.5160115343754743,
      "grad_norm": 0.48637598752975464,
      "learning_rate": 0.0002226628895184136,
      "loss": 0.0201,
      "step": 1275
    },
    {
      "epoch": 0.5164162493043962,
      "grad_norm": 1.0676556825637817,
      "learning_rate": 0.00022260218535006068,
      "loss": 0.0695,
      "step": 1276
    },
    {
      "epoch": 0.5168209642333181,
      "grad_norm": 2.8039169311523438,
      "learning_rate": 0.0002225414811817078,
      "loss": 0.1152,
      "step": 1277
    },
    {
      "epoch": 0.51722567916224,
      "grad_norm": 0.46557438373565674,
      "learning_rate": 0.0002224807770133549,
      "loss": 0.0201,
      "step": 1278
    },
    {
      "epoch": 0.5176303940911621,
      "grad_norm": 0.7456721663475037,
      "learning_rate": 0.000222420072845002,
      "loss": 0.0214,
      "step": 1279
    },
    {
      "epoch": 0.518035109020084,
      "grad_norm": 1.8904114961624146,
      "learning_rate": 0.0002223593686766491,
      "loss": 0.0676,
      "step": 1280
    },
    {
      "epoch": 0.5184398239490059,
      "grad_norm": 2.964827060699463,
      "learning_rate": 0.0002222986645082962,
      "loss": 0.0834,
      "step": 1281
    },
    {
      "epoch": 0.5188445388779278,
      "grad_norm": 1.225103497505188,
      "learning_rate": 0.00022223796033994333,
      "loss": 0.0816,
      "step": 1282
    },
    {
      "epoch": 0.5192492538068498,
      "grad_norm": 3.0500128269195557,
      "learning_rate": 0.00022217725617159045,
      "loss": 0.0487,
      "step": 1283
    },
    {
      "epoch": 0.5196539687357717,
      "grad_norm": 0.900558590888977,
      "learning_rate": 0.0002221165520032375,
      "loss": 0.0221,
      "step": 1284
    },
    {
      "epoch": 0.5200586836646937,
      "grad_norm": 1.309268593788147,
      "learning_rate": 0.00022205584783488463,
      "loss": 0.0235,
      "step": 1285
    },
    {
      "epoch": 0.5204633985936156,
      "grad_norm": 2.393723726272583,
      "learning_rate": 0.00022199514366653175,
      "loss": 0.0923,
      "step": 1286
    },
    {
      "epoch": 0.5208681135225376,
      "grad_norm": 0.7877005934715271,
      "learning_rate": 0.00022193443949817886,
      "loss": 0.0452,
      "step": 1287
    },
    {
      "epoch": 0.5212728284514595,
      "grad_norm": 0.6722469925880432,
      "learning_rate": 0.00022187373532982598,
      "loss": 0.0195,
      "step": 1288
    },
    {
      "epoch": 0.5216775433803814,
      "grad_norm": 0.40578779578208923,
      "learning_rate": 0.00022181303116147305,
      "loss": 0.0218,
      "step": 1289
    },
    {
      "epoch": 0.5220822583093034,
      "grad_norm": 3.8640964031219482,
      "learning_rate": 0.00022175232699312016,
      "loss": 0.052,
      "step": 1290
    },
    {
      "epoch": 0.5224869732382253,
      "grad_norm": 0.7539133429527283,
      "learning_rate": 0.00022169162282476728,
      "loss": 0.028,
      "step": 1291
    },
    {
      "epoch": 0.5228916881671473,
      "grad_norm": 6.164989471435547,
      "learning_rate": 0.0002216309186564144,
      "loss": 0.0944,
      "step": 1292
    },
    {
      "epoch": 0.5232964030960692,
      "grad_norm": 6.566061019897461,
      "learning_rate": 0.00022157021448806152,
      "loss": 0.0703,
      "step": 1293
    },
    {
      "epoch": 0.5237011180249912,
      "grad_norm": 0.8518933057785034,
      "learning_rate": 0.0002215095103197086,
      "loss": 0.0212,
      "step": 1294
    },
    {
      "epoch": 0.5241058329539131,
      "grad_norm": 1.645415186882019,
      "learning_rate": 0.0002214488061513557,
      "loss": 0.0429,
      "step": 1295
    },
    {
      "epoch": 0.524510547882835,
      "grad_norm": 1.9983139038085938,
      "learning_rate": 0.00022138810198300281,
      "loss": 0.0473,
      "step": 1296
    },
    {
      "epoch": 0.5249152628117569,
      "grad_norm": 2.5601558685302734,
      "learning_rate": 0.00022132739781464993,
      "loss": 0.1045,
      "step": 1297
    },
    {
      "epoch": 0.525319977740679,
      "grad_norm": 2.6811277866363525,
      "learning_rate": 0.00022126669364629702,
      "loss": 0.0418,
      "step": 1298
    },
    {
      "epoch": 0.5257246926696009,
      "grad_norm": 6.833272457122803,
      "learning_rate": 0.00022120598947794414,
      "loss": 0.2796,
      "step": 1299
    },
    {
      "epoch": 0.5261294075985228,
      "grad_norm": 0.5998154878616333,
      "learning_rate": 0.00022114528530959123,
      "loss": 0.0257,
      "step": 1300
    },
    {
      "epoch": 0.5265341225274447,
      "grad_norm": 2.814631223678589,
      "learning_rate": 0.00022108458114123835,
      "loss": 0.1654,
      "step": 1301
    },
    {
      "epoch": 0.5269388374563667,
      "grad_norm": 0.49198827147483826,
      "learning_rate": 0.00022102387697288544,
      "loss": 0.0258,
      "step": 1302
    },
    {
      "epoch": 0.5273435523852886,
      "grad_norm": 0.2675751745700836,
      "learning_rate": 0.00022096317280453256,
      "loss": 0.0091,
      "step": 1303
    },
    {
      "epoch": 0.5277482673142105,
      "grad_norm": 0.20992103219032288,
      "learning_rate": 0.00022090246863617967,
      "loss": 0.0092,
      "step": 1304
    },
    {
      "epoch": 0.5281529822431325,
      "grad_norm": 2.8077032566070557,
      "learning_rate": 0.0002208417644678268,
      "loss": 0.0761,
      "step": 1305
    },
    {
      "epoch": 0.5285576971720545,
      "grad_norm": 0.4164433777332306,
      "learning_rate": 0.00022078106029947386,
      "loss": 0.0073,
      "step": 1306
    },
    {
      "epoch": 0.5289624121009764,
      "grad_norm": 2.0566587448120117,
      "learning_rate": 0.00022072035613112097,
      "loss": 0.0747,
      "step": 1307
    },
    {
      "epoch": 0.5293671270298983,
      "grad_norm": 1.4858635663986206,
      "learning_rate": 0.0002206596519627681,
      "loss": 0.0591,
      "step": 1308
    },
    {
      "epoch": 0.5297718419588202,
      "grad_norm": 1.1602423191070557,
      "learning_rate": 0.0002205989477944152,
      "loss": 0.0257,
      "step": 1309
    },
    {
      "epoch": 0.5301765568877422,
      "grad_norm": 1.522391438484192,
      "learning_rate": 0.00022053824362606233,
      "loss": 0.0598,
      "step": 1310
    },
    {
      "epoch": 0.5305812718166641,
      "grad_norm": 2.041339874267578,
      "learning_rate": 0.0002204775394577094,
      "loss": 0.056,
      "step": 1311
    },
    {
      "epoch": 0.5309859867455861,
      "grad_norm": 2.6692183017730713,
      "learning_rate": 0.0002204168352893565,
      "loss": 0.036,
      "step": 1312
    },
    {
      "epoch": 0.531390701674508,
      "grad_norm": 3.060802936553955,
      "learning_rate": 0.00022035613112100362,
      "loss": 0.0256,
      "step": 1313
    },
    {
      "epoch": 0.53179541660343,
      "grad_norm": 0.45866963267326355,
      "learning_rate": 0.00022029542695265074,
      "loss": 0.0272,
      "step": 1314
    },
    {
      "epoch": 0.5322001315323519,
      "grad_norm": 0.9328057169914246,
      "learning_rate": 0.00022023472278429783,
      "loss": 0.0296,
      "step": 1315
    },
    {
      "epoch": 0.5326048464612738,
      "grad_norm": 1.935218095779419,
      "learning_rate": 0.00022017401861594495,
      "loss": 0.0518,
      "step": 1316
    },
    {
      "epoch": 0.5330095613901957,
      "grad_norm": 1.559291124343872,
      "learning_rate": 0.00022011331444759204,
      "loss": 0.0419,
      "step": 1317
    },
    {
      "epoch": 0.5334142763191178,
      "grad_norm": 1.730683445930481,
      "learning_rate": 0.00022005261027923916,
      "loss": 0.0376,
      "step": 1318
    },
    {
      "epoch": 0.5338189912480397,
      "grad_norm": 2.036949634552002,
      "learning_rate": 0.00021999190611088628,
      "loss": 0.1164,
      "step": 1319
    },
    {
      "epoch": 0.5342237061769616,
      "grad_norm": 3.027951717376709,
      "learning_rate": 0.00021993120194253337,
      "loss": 0.1242,
      "step": 1320
    },
    {
      "epoch": 0.5346284211058835,
      "grad_norm": 0.44690507650375366,
      "learning_rate": 0.00021987049777418048,
      "loss": 0.0221,
      "step": 1321
    },
    {
      "epoch": 0.5350331360348055,
      "grad_norm": 2.0644588470458984,
      "learning_rate": 0.00021980979360582757,
      "loss": 0.0363,
      "step": 1322
    },
    {
      "epoch": 0.5354378509637274,
      "grad_norm": 2.429682493209839,
      "learning_rate": 0.0002197490894374747,
      "loss": 0.0397,
      "step": 1323
    },
    {
      "epoch": 0.5358425658926493,
      "grad_norm": 1.9912633895874023,
      "learning_rate": 0.00021968838526912178,
      "loss": 0.0548,
      "step": 1324
    },
    {
      "epoch": 0.5362472808215714,
      "grad_norm": 0.6611655950546265,
      "learning_rate": 0.0002196276811007689,
      "loss": 0.0465,
      "step": 1325
    },
    {
      "epoch": 0.5366519957504933,
      "grad_norm": 2.6018002033233643,
      "learning_rate": 0.00021956697693241602,
      "loss": 0.0728,
      "step": 1326
    },
    {
      "epoch": 0.5370567106794152,
      "grad_norm": 1.7229421138763428,
      "learning_rate": 0.00021950627276406314,
      "loss": 0.0348,
      "step": 1327
    },
    {
      "epoch": 0.5374614256083371,
      "grad_norm": 1.2401820421218872,
      "learning_rate": 0.0002194455685957102,
      "loss": 0.0666,
      "step": 1328
    },
    {
      "epoch": 0.5378661405372591,
      "grad_norm": 5.315018177032471,
      "learning_rate": 0.00021938486442735732,
      "loss": 0.0989,
      "step": 1329
    },
    {
      "epoch": 0.538270855466181,
      "grad_norm": 0.7108698487281799,
      "learning_rate": 0.00021932416025900443,
      "loss": 0.0089,
      "step": 1330
    },
    {
      "epoch": 0.5386755703951029,
      "grad_norm": 0.3734856843948364,
      "learning_rate": 0.00021926345609065155,
      "loss": 0.0221,
      "step": 1331
    },
    {
      "epoch": 0.5390802853240249,
      "grad_norm": 1.990067958831787,
      "learning_rate": 0.00021920275192229867,
      "loss": 0.042,
      "step": 1332
    },
    {
      "epoch": 0.5394850002529469,
      "grad_norm": 1.0561976432800293,
      "learning_rate": 0.00021914204775394573,
      "loss": 0.0792,
      "step": 1333
    },
    {
      "epoch": 0.5398897151818688,
      "grad_norm": 1.549487590789795,
      "learning_rate": 0.00021908134358559285,
      "loss": 0.008,
      "step": 1334
    },
    {
      "epoch": 0.5402944301107907,
      "grad_norm": 0.9509010910987854,
      "learning_rate": 0.00021902063941723997,
      "loss": 0.0168,
      "step": 1335
    },
    {
      "epoch": 0.5406991450397126,
      "grad_norm": 2.5607552528381348,
      "learning_rate": 0.00021895993524888709,
      "loss": 0.0833,
      "step": 1336
    },
    {
      "epoch": 0.5411038599686346,
      "grad_norm": 3.6643385887145996,
      "learning_rate": 0.00021889923108053418,
      "loss": 0.027,
      "step": 1337
    },
    {
      "epoch": 0.5415085748975565,
      "grad_norm": 3.6928181648254395,
      "learning_rate": 0.0002188385269121813,
      "loss": 0.116,
      "step": 1338
    },
    {
      "epoch": 0.5419132898264785,
      "grad_norm": 1.7098037004470825,
      "learning_rate": 0.00021877782274382838,
      "loss": 0.0703,
      "step": 1339
    },
    {
      "epoch": 0.5423180047554004,
      "grad_norm": 1.1993095874786377,
      "learning_rate": 0.0002187171185754755,
      "loss": 0.0694,
      "step": 1340
    },
    {
      "epoch": 0.5427227196843224,
      "grad_norm": 2.8212578296661377,
      "learning_rate": 0.0002186564144071226,
      "loss": 0.045,
      "step": 1341
    },
    {
      "epoch": 0.5431274346132443,
      "grad_norm": 0.44916626811027527,
      "learning_rate": 0.0002185957102387697,
      "loss": 0.057,
      "step": 1342
    },
    {
      "epoch": 0.5435321495421662,
      "grad_norm": 1.279313325881958,
      "learning_rate": 0.00021853500607041683,
      "loss": 0.0267,
      "step": 1343
    },
    {
      "epoch": 0.5439368644710881,
      "grad_norm": 3.3911831378936768,
      "learning_rate": 0.00021847430190206392,
      "loss": 0.048,
      "step": 1344
    },
    {
      "epoch": 0.5443415794000102,
      "grad_norm": 4.89183235168457,
      "learning_rate": 0.000218413597733711,
      "loss": 0.1194,
      "step": 1345
    },
    {
      "epoch": 0.5447462943289321,
      "grad_norm": 0.4474908411502838,
      "learning_rate": 0.00021835289356535813,
      "loss": 0.0334,
      "step": 1346
    },
    {
      "epoch": 0.545151009257854,
      "grad_norm": 2.892573595046997,
      "learning_rate": 0.00021829218939700524,
      "loss": 0.0734,
      "step": 1347
    },
    {
      "epoch": 0.5455557241867759,
      "grad_norm": 0.4778691232204437,
      "learning_rate": 0.00021823148522865236,
      "loss": 0.0214,
      "step": 1348
    },
    {
      "epoch": 0.5459604391156979,
      "grad_norm": 1.0997931957244873,
      "learning_rate": 0.00021817078106029948,
      "loss": 0.0313,
      "step": 1349
    },
    {
      "epoch": 0.5463651540446198,
      "grad_norm": 1.9087074995040894,
      "learning_rate": 0.00021811007689194654,
      "loss": 0.0154,
      "step": 1350
    },
    {
      "epoch": 0.5467698689735417,
      "grad_norm": 0.781522810459137,
      "learning_rate": 0.00021804937272359366,
      "loss": 0.0108,
      "step": 1351
    },
    {
      "epoch": 0.5471745839024637,
      "grad_norm": 2.6034116744995117,
      "learning_rate": 0.00021798866855524078,
      "loss": 0.0936,
      "step": 1352
    },
    {
      "epoch": 0.5475792988313857,
      "grad_norm": 2.4079198837280273,
      "learning_rate": 0.0002179279643868879,
      "loss": 0.0989,
      "step": 1353
    },
    {
      "epoch": 0.5479840137603076,
      "grad_norm": 1.6810328960418701,
      "learning_rate": 0.000217867260218535,
      "loss": 0.0707,
      "step": 1354
    },
    {
      "epoch": 0.5483887286892295,
      "grad_norm": 1.3854042291641235,
      "learning_rate": 0.00021780655605018208,
      "loss": 0.0225,
      "step": 1355
    },
    {
      "epoch": 0.5487934436181514,
      "grad_norm": 1.7897993326187134,
      "learning_rate": 0.0002177458518818292,
      "loss": 0.0775,
      "step": 1356
    },
    {
      "epoch": 0.5491981585470734,
      "grad_norm": 1.3420664072036743,
      "learning_rate": 0.0002176851477134763,
      "loss": 0.0222,
      "step": 1357
    },
    {
      "epoch": 0.5496028734759953,
      "grad_norm": 0.7721344232559204,
      "learning_rate": 0.00021762444354512343,
      "loss": 0.0207,
      "step": 1358
    },
    {
      "epoch": 0.5500075884049173,
      "grad_norm": 0.42645707726478577,
      "learning_rate": 0.00021756373937677052,
      "loss": 0.0114,
      "step": 1359
    },
    {
      "epoch": 0.5504123033338393,
      "grad_norm": 2.6302850246429443,
      "learning_rate": 0.00021750303520841764,
      "loss": 0.1175,
      "step": 1360
    },
    {
      "epoch": 0.5508170182627612,
      "grad_norm": 2.8327012062072754,
      "learning_rate": 0.00021744233104006473,
      "loss": 0.0548,
      "step": 1361
    },
    {
      "epoch": 0.5512217331916831,
      "grad_norm": 1.6026406288146973,
      "learning_rate": 0.00021738162687171185,
      "loss": 0.0247,
      "step": 1362
    },
    {
      "epoch": 0.551626448120605,
      "grad_norm": 1.9968703985214233,
      "learning_rate": 0.00021732092270335894,
      "loss": 0.0269,
      "step": 1363
    },
    {
      "epoch": 0.552031163049527,
      "grad_norm": 1.0415195226669312,
      "learning_rate": 0.00021726021853500605,
      "loss": 0.0663,
      "step": 1364
    },
    {
      "epoch": 0.552435877978449,
      "grad_norm": 0.4465568959712982,
      "learning_rate": 0.00021719951436665317,
      "loss": 0.0103,
      "step": 1365
    },
    {
      "epoch": 0.5528405929073709,
      "grad_norm": 3.8604085445404053,
      "learning_rate": 0.00021713881019830026,
      "loss": 0.0615,
      "step": 1366
    },
    {
      "epoch": 0.5532453078362928,
      "grad_norm": 1.1819484233856201,
      "learning_rate": 0.00021707810602994735,
      "loss": 0.0349,
      "step": 1367
    },
    {
      "epoch": 0.5536500227652148,
      "grad_norm": 0.8618736267089844,
      "learning_rate": 0.00021701740186159447,
      "loss": 0.0481,
      "step": 1368
    },
    {
      "epoch": 0.5540547376941367,
      "grad_norm": 1.0894819498062134,
      "learning_rate": 0.0002169566976932416,
      "loss": 0.0351,
      "step": 1369
    },
    {
      "epoch": 0.5544594526230586,
      "grad_norm": 4.128244876861572,
      "learning_rate": 0.0002168959935248887,
      "loss": 0.0533,
      "step": 1370
    },
    {
      "epoch": 0.5548641675519805,
      "grad_norm": 2.0420703887939453,
      "learning_rate": 0.00021683528935653582,
      "loss": 0.0383,
      "step": 1371
    },
    {
      "epoch": 0.5552688824809026,
      "grad_norm": 2.139113426208496,
      "learning_rate": 0.00021677458518818289,
      "loss": 0.0709,
      "step": 1372
    },
    {
      "epoch": 0.5556735974098245,
      "grad_norm": 3.3454666137695312,
      "learning_rate": 0.00021671388101983,
      "loss": 0.0526,
      "step": 1373
    },
    {
      "epoch": 0.5560783123387464,
      "grad_norm": 1.7111358642578125,
      "learning_rate": 0.00021665317685147712,
      "loss": 0.0522,
      "step": 1374
    },
    {
      "epoch": 0.5564830272676683,
      "grad_norm": 4.125695705413818,
      "learning_rate": 0.00021659247268312424,
      "loss": 0.0514,
      "step": 1375
    },
    {
      "epoch": 0.5568877421965903,
      "grad_norm": 2.388007402420044,
      "learning_rate": 0.00021653176851477133,
      "loss": 0.0575,
      "step": 1376
    },
    {
      "epoch": 0.5572924571255122,
      "grad_norm": 1.0833216905593872,
      "learning_rate": 0.00021647106434641842,
      "loss": 0.0365,
      "step": 1377
    },
    {
      "epoch": 0.5576971720544341,
      "grad_norm": 1.2478526830673218,
      "learning_rate": 0.00021641036017806554,
      "loss": 0.0556,
      "step": 1378
    },
    {
      "epoch": 0.558101886983356,
      "grad_norm": 2.27677321434021,
      "learning_rate": 0.00021634965600971266,
      "loss": 0.0516,
      "step": 1379
    },
    {
      "epoch": 0.5585066019122781,
      "grad_norm": 2.5907890796661377,
      "learning_rate": 0.00021628895184135975,
      "loss": 0.0593,
      "step": 1380
    },
    {
      "epoch": 0.5589113168412,
      "grad_norm": 0.28880199790000916,
      "learning_rate": 0.00021622824767300686,
      "loss": 0.014,
      "step": 1381
    },
    {
      "epoch": 0.5593160317701219,
      "grad_norm": 0.5561617016792297,
      "learning_rate": 0.00021616754350465398,
      "loss": 0.0264,
      "step": 1382
    },
    {
      "epoch": 0.5597207466990438,
      "grad_norm": 2.817544937133789,
      "learning_rate": 0.00021610683933630107,
      "loss": 0.0681,
      "step": 1383
    },
    {
      "epoch": 0.5601254616279658,
      "grad_norm": 4.139232635498047,
      "learning_rate": 0.0002160461351679482,
      "loss": 0.083,
      "step": 1384
    },
    {
      "epoch": 0.5605301765568877,
      "grad_norm": 0.38088274002075195,
      "learning_rate": 0.00021598543099959528,
      "loss": 0.0193,
      "step": 1385
    },
    {
      "epoch": 0.5609348914858097,
      "grad_norm": 3.177917718887329,
      "learning_rate": 0.0002159247268312424,
      "loss": 0.0464,
      "step": 1386
    },
    {
      "epoch": 0.5613396064147316,
      "grad_norm": 2.3033335208892822,
      "learning_rate": 0.00021586402266288951,
      "loss": 0.0714,
      "step": 1387
    },
    {
      "epoch": 0.5617443213436536,
      "grad_norm": 2.3785789012908936,
      "learning_rate": 0.0002158033184945366,
      "loss": 0.1023,
      "step": 1388
    },
    {
      "epoch": 0.5621490362725755,
      "grad_norm": 0.8404483199119568,
      "learning_rate": 0.0002157426143261837,
      "loss": 0.0245,
      "step": 1389
    },
    {
      "epoch": 0.5625537512014974,
      "grad_norm": 0.8903877139091492,
      "learning_rate": 0.0002156819101578308,
      "loss": 0.0232,
      "step": 1390
    },
    {
      "epoch": 0.5629584661304193,
      "grad_norm": 3.885406970977783,
      "learning_rate": 0.00021562120598947793,
      "loss": 0.0838,
      "step": 1391
    },
    {
      "epoch": 0.5633631810593414,
      "grad_norm": 1.473663091659546,
      "learning_rate": 0.00021556050182112505,
      "loss": 0.0674,
      "step": 1392
    },
    {
      "epoch": 0.5637678959882633,
      "grad_norm": 2.574843645095825,
      "learning_rate": 0.0002154997976527721,
      "loss": 0.1134,
      "step": 1393
    },
    {
      "epoch": 0.5641726109171852,
      "grad_norm": 2.2758407592773438,
      "learning_rate": 0.00021543909348441923,
      "loss": 0.1097,
      "step": 1394
    },
    {
      "epoch": 0.5645773258461072,
      "grad_norm": 0.8136201500892639,
      "learning_rate": 0.00021537838931606635,
      "loss": 0.0392,
      "step": 1395
    },
    {
      "epoch": 0.5649820407750291,
      "grad_norm": 2.758969783782959,
      "learning_rate": 0.00021531768514771346,
      "loss": 0.0891,
      "step": 1396
    },
    {
      "epoch": 0.565386755703951,
      "grad_norm": 0.6362152099609375,
      "learning_rate": 0.00021525698097936058,
      "loss": 0.0169,
      "step": 1397
    },
    {
      "epoch": 0.5657914706328729,
      "grad_norm": 3.5578625202178955,
      "learning_rate": 0.00021519627681100767,
      "loss": 0.0721,
      "step": 1398
    },
    {
      "epoch": 0.566196185561795,
      "grad_norm": 3.2959275245666504,
      "learning_rate": 0.00021513557264265476,
      "loss": 0.0465,
      "step": 1399
    },
    {
      "epoch": 0.5666009004907169,
      "grad_norm": 1.8979530334472656,
      "learning_rate": 0.00021507486847430188,
      "loss": 0.0931,
      "step": 1400
    },
    {
      "epoch": 0.5670056154196388,
      "grad_norm": 4.349502086639404,
      "learning_rate": 0.000215014164305949,
      "loss": 0.0255,
      "step": 1401
    },
    {
      "epoch": 0.5674103303485607,
      "grad_norm": 2.1974661350250244,
      "learning_rate": 0.0002149534601375961,
      "loss": 0.0582,
      "step": 1402
    },
    {
      "epoch": 0.5678150452774827,
      "grad_norm": 1.071811318397522,
      "learning_rate": 0.0002148927559692432,
      "loss": 0.0109,
      "step": 1403
    },
    {
      "epoch": 0.5682197602064046,
      "grad_norm": 0.9651944637298584,
      "learning_rate": 0.0002148320518008903,
      "loss": 0.0279,
      "step": 1404
    },
    {
      "epoch": 0.5686244751353265,
      "grad_norm": 1.7994575500488281,
      "learning_rate": 0.00021477134763253742,
      "loss": 0.0723,
      "step": 1405
    },
    {
      "epoch": 0.5690291900642485,
      "grad_norm": 1.8913007974624634,
      "learning_rate": 0.0002147106434641845,
      "loss": 0.0277,
      "step": 1406
    },
    {
      "epoch": 0.5694339049931705,
      "grad_norm": 3.6626875400543213,
      "learning_rate": 0.00021464993929583162,
      "loss": 0.0982,
      "step": 1407
    },
    {
      "epoch": 0.5698386199220924,
      "grad_norm": 0.5979498028755188,
      "learning_rate": 0.00021458923512747874,
      "loss": 0.0136,
      "step": 1408
    },
    {
      "epoch": 0.5702433348510143,
      "grad_norm": 1.431747317314148,
      "learning_rate": 0.00021452853095912586,
      "loss": 0.0292,
      "step": 1409
    },
    {
      "epoch": 0.5706480497799362,
      "grad_norm": 1.5129247903823853,
      "learning_rate": 0.00021446782679077292,
      "loss": 0.052,
      "step": 1410
    },
    {
      "epoch": 0.5710527647088582,
      "grad_norm": 0.0784635916352272,
      "learning_rate": 0.00021440712262242004,
      "loss": 0.0018,
      "step": 1411
    },
    {
      "epoch": 0.5714574796377802,
      "grad_norm": 6.759774208068848,
      "learning_rate": 0.00021434641845406716,
      "loss": 0.1404,
      "step": 1412
    },
    {
      "epoch": 0.5718621945667021,
      "grad_norm": 1.9804381132125854,
      "learning_rate": 0.00021428571428571427,
      "loss": 0.0189,
      "step": 1413
    },
    {
      "epoch": 0.572266909495624,
      "grad_norm": 1.4851856231689453,
      "learning_rate": 0.0002142250101173614,
      "loss": 0.0761,
      "step": 1414
    },
    {
      "epoch": 0.572671624424546,
      "grad_norm": 1.4964290857315063,
      "learning_rate": 0.00021416430594900846,
      "loss": 0.0484,
      "step": 1415
    },
    {
      "epoch": 0.5730763393534679,
      "grad_norm": 2.1743996143341064,
      "learning_rate": 0.00021410360178065557,
      "loss": 0.063,
      "step": 1416
    },
    {
      "epoch": 0.5734810542823898,
      "grad_norm": 5.2507734298706055,
      "learning_rate": 0.0002140428976123027,
      "loss": 0.0315,
      "step": 1417
    },
    {
      "epoch": 0.5738857692113117,
      "grad_norm": 1.8959916830062866,
      "learning_rate": 0.0002139821934439498,
      "loss": 0.0551,
      "step": 1418
    },
    {
      "epoch": 0.5742904841402338,
      "grad_norm": 2.389193534851074,
      "learning_rate": 0.00021392148927559693,
      "loss": 0.0456,
      "step": 1419
    },
    {
      "epoch": 0.5746951990691557,
      "grad_norm": 2.0087029933929443,
      "learning_rate": 0.00021386078510724402,
      "loss": 0.0178,
      "step": 1420
    },
    {
      "epoch": 0.5750999139980776,
      "grad_norm": 1.2702523469924927,
      "learning_rate": 0.0002138000809388911,
      "loss": 0.0248,
      "step": 1421
    },
    {
      "epoch": 0.5755046289269995,
      "grad_norm": 4.681711673736572,
      "learning_rate": 0.00021373937677053822,
      "loss": 0.0819,
      "step": 1422
    },
    {
      "epoch": 0.5759093438559215,
      "grad_norm": 1.0840479135513306,
      "learning_rate": 0.00021367867260218534,
      "loss": 0.0302,
      "step": 1423
    },
    {
      "epoch": 0.5763140587848434,
      "grad_norm": 0.8576205372810364,
      "learning_rate": 0.00021361796843383243,
      "loss": 0.0259,
      "step": 1424
    },
    {
      "epoch": 0.5767187737137653,
      "grad_norm": 2.2902090549468994,
      "learning_rate": 0.00021355726426547955,
      "loss": 0.0824,
      "step": 1425
    },
    {
      "epoch": 0.5771234886426873,
      "grad_norm": 3.07403826713562,
      "learning_rate": 0.00021349656009712664,
      "loss": 0.0367,
      "step": 1426
    },
    {
      "epoch": 0.5775282035716093,
      "grad_norm": 1.028857946395874,
      "learning_rate": 0.00021343585592877376,
      "loss": 0.0203,
      "step": 1427
    },
    {
      "epoch": 0.5779329185005312,
      "grad_norm": 3.7826366424560547,
      "learning_rate": 0.00021337515176042085,
      "loss": 0.04,
      "step": 1428
    },
    {
      "epoch": 0.5783376334294531,
      "grad_norm": 0.8323581218719482,
      "learning_rate": 0.00021331444759206797,
      "loss": 0.0244,
      "step": 1429
    },
    {
      "epoch": 0.5787423483583751,
      "grad_norm": 4.851275444030762,
      "learning_rate": 0.00021325374342371508,
      "loss": 0.0456,
      "step": 1430
    },
    {
      "epoch": 0.579147063287297,
      "grad_norm": 1.1861262321472168,
      "learning_rate": 0.0002131930392553622,
      "loss": 0.0307,
      "step": 1431
    },
    {
      "epoch": 0.579551778216219,
      "grad_norm": 6.757365703582764,
      "learning_rate": 0.00021313233508700927,
      "loss": 0.0312,
      "step": 1432
    },
    {
      "epoch": 0.5799564931451409,
      "grad_norm": 4.634129047393799,
      "learning_rate": 0.00021307163091865638,
      "loss": 0.0298,
      "step": 1433
    },
    {
      "epoch": 0.5803612080740629,
      "grad_norm": 0.07608584314584732,
      "learning_rate": 0.0002130109267503035,
      "loss": 0.0023,
      "step": 1434
    },
    {
      "epoch": 0.5807659230029848,
      "grad_norm": 3.9715120792388916,
      "learning_rate": 0.00021295022258195062,
      "loss": 0.0768,
      "step": 1435
    },
    {
      "epoch": 0.5811706379319067,
      "grad_norm": 1.2087315320968628,
      "learning_rate": 0.00021288951841359774,
      "loss": 0.0694,
      "step": 1436
    },
    {
      "epoch": 0.5815753528608286,
      "grad_norm": 2.6308724880218506,
      "learning_rate": 0.0002128288142452448,
      "loss": 0.0651,
      "step": 1437
    },
    {
      "epoch": 0.5819800677897506,
      "grad_norm": 0.45512139797210693,
      "learning_rate": 0.00021276811007689192,
      "loss": 0.0169,
      "step": 1438
    },
    {
      "epoch": 0.5823847827186726,
      "grad_norm": 0.7173782587051392,
      "learning_rate": 0.00021270740590853903,
      "loss": 0.0342,
      "step": 1439
    },
    {
      "epoch": 0.5827894976475945,
      "grad_norm": 1.219461441040039,
      "learning_rate": 0.00021264670174018615,
      "loss": 0.0097,
      "step": 1440
    },
    {
      "epoch": 0.5831942125765164,
      "grad_norm": 0.5755008459091187,
      "learning_rate": 0.00021258599757183324,
      "loss": 0.028,
      "step": 1441
    },
    {
      "epoch": 0.5835989275054384,
      "grad_norm": 0.7591521739959717,
      "learning_rate": 0.00021252529340348036,
      "loss": 0.031,
      "step": 1442
    },
    {
      "epoch": 0.5840036424343603,
      "grad_norm": 1.1431807279586792,
      "learning_rate": 0.00021246458923512745,
      "loss": 0.0538,
      "step": 1443
    },
    {
      "epoch": 0.5844083573632822,
      "grad_norm": 0.8024325370788574,
      "learning_rate": 0.00021240388506677457,
      "loss": 0.035,
      "step": 1444
    },
    {
      "epoch": 0.5848130722922041,
      "grad_norm": 0.9910147786140442,
      "learning_rate": 0.00021234318089842166,
      "loss": 0.0819,
      "step": 1445
    },
    {
      "epoch": 0.5852177872211262,
      "grad_norm": 1.5825101137161255,
      "learning_rate": 0.00021228247673006878,
      "loss": 0.0323,
      "step": 1446
    },
    {
      "epoch": 0.5856225021500481,
      "grad_norm": 0.9964414834976196,
      "learning_rate": 0.0002122217725617159,
      "loss": 0.0276,
      "step": 1447
    },
    {
      "epoch": 0.58602721707897,
      "grad_norm": 3.4478886127471924,
      "learning_rate": 0.00021216106839336298,
      "loss": 0.1668,
      "step": 1448
    },
    {
      "epoch": 0.5864319320078919,
      "grad_norm": 0.6209176182746887,
      "learning_rate": 0.00021210036422501008,
      "loss": 0.0162,
      "step": 1449
    },
    {
      "epoch": 0.5868366469368139,
      "grad_norm": 0.853117823600769,
      "learning_rate": 0.0002120396600566572,
      "loss": 0.0296,
      "step": 1450
    },
    {
      "epoch": 0.2750059227671168,
      "grad_norm": 8.127561569213867,
      "learning_rate": 0.0002119789558883043,
      "loss": 1.7342,
      "step": 1451
    },
    {
      "epoch": 0.2751954513148543,
      "grad_norm": 5.762104511260986,
      "learning_rate": 0.0002587549744172825,
      "loss": 0.8895,
      "step": 1452
    },
    {
      "epoch": 0.2753849798625918,
      "grad_norm": 6.335174083709717,
      "learning_rate": 0.00025872654917566797,
      "loss": 1.2567,
      "step": 1453
    },
    {
      "epoch": 0.2755745084103293,
      "grad_norm": 3.165539026260376,
      "learning_rate": 0.0002586981239340534,
      "loss": 0.498,
      "step": 1454
    },
    {
      "epoch": 0.2757640369580668,
      "grad_norm": 3.2324166297912598,
      "learning_rate": 0.00025866969869243887,
      "loss": 0.7524,
      "step": 1455
    },
    {
      "epoch": 0.2759535655058043,
      "grad_norm": 2.3337457180023193,
      "learning_rate": 0.0002586412734508243,
      "loss": 0.4963,
      "step": 1456
    },
    {
      "epoch": 0.2761430940535418,
      "grad_norm": 2.509737968444824,
      "learning_rate": 0.00025861284820920976,
      "loss": 0.4951,
      "step": 1457
    },
    {
      "epoch": 0.27633262260127933,
      "grad_norm": 2.92039155960083,
      "learning_rate": 0.0002585844229675952,
      "loss": 0.2314,
      "step": 1458
    },
    {
      "epoch": 0.27652215114901685,
      "grad_norm": 3.8123462200164795,
      "learning_rate": 0.00025855599772598066,
      "loss": 0.3835,
      "step": 1459
    },
    {
      "epoch": 0.2767116796967543,
      "grad_norm": 1.9940756559371948,
      "learning_rate": 0.0002585275724843661,
      "loss": 0.2683,
      "step": 1460
    },
    {
      "epoch": 0.2769012082444918,
      "grad_norm": 1.5855708122253418,
      "learning_rate": 0.00025849914724275155,
      "loss": 0.2492,
      "step": 1461
    },
    {
      "epoch": 0.27709073679222934,
      "grad_norm": 4.2590532302856445,
      "learning_rate": 0.000258470722001137,
      "loss": 0.3094,
      "step": 1462
    },
    {
      "epoch": 0.27728026533996686,
      "grad_norm": 1.3550159931182861,
      "learning_rate": 0.00025844229675952245,
      "loss": 0.2122,
      "step": 1463
    },
    {
      "epoch": 0.2774697938877043,
      "grad_norm": 3.975019693374634,
      "learning_rate": 0.0002584138715179079,
      "loss": 0.1418,
      "step": 1464
    },
    {
      "epoch": 0.27765932243544184,
      "grad_norm": 2.6393516063690186,
      "learning_rate": 0.0002583854462762933,
      "loss": 0.3003,
      "step": 1465
    },
    {
      "epoch": 0.27784885098317935,
      "grad_norm": 1.5036885738372803,
      "learning_rate": 0.0002583570210346788,
      "loss": 0.2916,
      "step": 1466
    },
    {
      "epoch": 0.27803837953091687,
      "grad_norm": 3.868302345275879,
      "learning_rate": 0.0002583285957930642,
      "loss": 0.3998,
      "step": 1467
    },
    {
      "epoch": 0.27822790807865433,
      "grad_norm": 0.828289806842804,
      "learning_rate": 0.0002583001705514497,
      "loss": 0.1222,
      "step": 1468
    },
    {
      "epoch": 0.27841743662639185,
      "grad_norm": 1.8248372077941895,
      "learning_rate": 0.0002582717453098351,
      "loss": 0.1891,
      "step": 1469
    },
    {
      "epoch": 0.27860696517412936,
      "grad_norm": 3.35007381439209,
      "learning_rate": 0.0002582433200682206,
      "loss": 0.3735,
      "step": 1470
    },
    {
      "epoch": 0.2787964937218669,
      "grad_norm": 1.2657275199890137,
      "learning_rate": 0.000258214894826606,
      "loss": 0.1203,
      "step": 1471
    },
    {
      "epoch": 0.27898602226960434,
      "grad_norm": 5.1276936531066895,
      "learning_rate": 0.0002581864695849914,
      "loss": 0.1697,
      "step": 1472
    },
    {
      "epoch": 0.27917555081734186,
      "grad_norm": 1.1384589672088623,
      "learning_rate": 0.0002581580443433769,
      "loss": 0.1447,
      "step": 1473
    },
    {
      "epoch": 0.27936507936507937,
      "grad_norm": 2.5893969535827637,
      "learning_rate": 0.0002581296191017623,
      "loss": 0.4498,
      "step": 1474
    },
    {
      "epoch": 0.2795546079128169,
      "grad_norm": 1.1333706378936768,
      "learning_rate": 0.0002581011938601478,
      "loss": 0.2006,
      "step": 1475
    },
    {
      "epoch": 0.27974413646055435,
      "grad_norm": 1.8606764078140259,
      "learning_rate": 0.0002580727686185332,
      "loss": 0.2492,
      "step": 1476
    },
    {
      "epoch": 0.27993366500829187,
      "grad_norm": 2.313523054122925,
      "learning_rate": 0.0002580443433769187,
      "loss": 0.1872,
      "step": 1477
    },
    {
      "epoch": 0.2801231935560294,
      "grad_norm": 1.4095994234085083,
      "learning_rate": 0.0002580159181353041,
      "loss": 0.194,
      "step": 1478
    },
    {
      "epoch": 0.2803127221037669,
      "grad_norm": 1.3689919710159302,
      "learning_rate": 0.00025798749289368956,
      "loss": 0.1309,
      "step": 1479
    },
    {
      "epoch": 0.28050225065150436,
      "grad_norm": 1.5872477293014526,
      "learning_rate": 0.000257959067652075,
      "loss": 0.2492,
      "step": 1480
    },
    {
      "epoch": 0.2806917791992419,
      "grad_norm": 1.2904844284057617,
      "learning_rate": 0.00025793064241046046,
      "loss": 0.1188,
      "step": 1481
    },
    {
      "epoch": 0.2808813077469794,
      "grad_norm": 4.355881214141846,
      "learning_rate": 0.0002579022171688459,
      "loss": 0.3534,
      "step": 1482
    },
    {
      "epoch": 0.2810708362947169,
      "grad_norm": 1.3883638381958008,
      "learning_rate": 0.00025787379192723135,
      "loss": 0.2248,
      "step": 1483
    },
    {
      "epoch": 0.28126036484245437,
      "grad_norm": 1.2939093112945557,
      "learning_rate": 0.0002578453666856168,
      "loss": 0.1836,
      "step": 1484
    },
    {
      "epoch": 0.2814498933901919,
      "grad_norm": 4.927705764770508,
      "learning_rate": 0.00025781694144400225,
      "loss": 0.4215,
      "step": 1485
    },
    {
      "epoch": 0.2816394219379294,
      "grad_norm": 1.2898504734039307,
      "learning_rate": 0.0002577885162023877,
      "loss": 0.1623,
      "step": 1486
    },
    {
      "epoch": 0.2818289504856669,
      "grad_norm": 2.6187031269073486,
      "learning_rate": 0.00025776009096077315,
      "loss": 0.1784,
      "step": 1487
    },
    {
      "epoch": 0.2820184790334044,
      "grad_norm": 2.234072208404541,
      "learning_rate": 0.0002577316657191586,
      "loss": 0.3467,
      "step": 1488
    },
    {
      "epoch": 0.2822080075811419,
      "grad_norm": 2.164005756378174,
      "learning_rate": 0.00025770324047754404,
      "loss": 0.1831,
      "step": 1489
    },
    {
      "epoch": 0.2823975361288794,
      "grad_norm": 3.088031530380249,
      "learning_rate": 0.0002576748152359295,
      "loss": 0.1845,
      "step": 1490
    },
    {
      "epoch": 0.28258706467661693,
      "grad_norm": 3.4716103076934814,
      "learning_rate": 0.00025764638999431494,
      "loss": 0.1929,
      "step": 1491
    },
    {
      "epoch": 0.28277659322435444,
      "grad_norm": 2.76918625831604,
      "learning_rate": 0.0002576179647527004,
      "loss": 0.18,
      "step": 1492
    },
    {
      "epoch": 0.2829661217720919,
      "grad_norm": 1.7360742092132568,
      "learning_rate": 0.00025758953951108583,
      "loss": 0.2291,
      "step": 1493
    },
    {
      "epoch": 0.2831556503198294,
      "grad_norm": 1.631914496421814,
      "learning_rate": 0.0002575611142694713,
      "loss": 0.2465,
      "step": 1494
    },
    {
      "epoch": 0.28334517886756694,
      "grad_norm": 1.3504695892333984,
      "learning_rate": 0.00025753268902785673,
      "loss": 0.1174,
      "step": 1495
    },
    {
      "epoch": 0.28353470741530445,
      "grad_norm": 3.904390573501587,
      "learning_rate": 0.0002575042637862422,
      "loss": 0.1825,
      "step": 1496
    },
    {
      "epoch": 0.2837242359630419,
      "grad_norm": 2.627885341644287,
      "learning_rate": 0.0002574758385446276,
      "loss": 0.1862,
      "step": 1497
    },
    {
      "epoch": 0.28391376451077943,
      "grad_norm": 3.239832878112793,
      "learning_rate": 0.0002574474133030131,
      "loss": 0.2606,
      "step": 1498
    },
    {
      "epoch": 0.28410329305851695,
      "grad_norm": 0.9222208857536316,
      "learning_rate": 0.00025741898806139847,
      "loss": 0.0662,
      "step": 1499
    },
    {
      "epoch": 0.28429282160625446,
      "grad_norm": 1.7968775033950806,
      "learning_rate": 0.00025739056281978397,
      "loss": 0.2373,
      "step": 1500
    },
    {
      "epoch": 0.48028157747380207,
      "grad_norm": 11.966684341430664,
      "learning_rate": 0.00025736213757816936,
      "loss": 2.7299,
      "step": 1501
    },
    {
      "epoch": 0.4806015518758499,
      "grad_norm": 8.877211570739746,
      "learning_rate": 0.0001559500959692898,
      "loss": 2.2669,
      "step": 1502
    },
    {
      "epoch": 0.48092152627789775,
      "grad_norm": 10.749866485595703,
      "learning_rate": 0.00015585412667946255,
      "loss": 2.2383,
      "step": 1503
    },
    {
      "epoch": 0.4812415006799456,
      "grad_norm": 12.44593620300293,
      "learning_rate": 0.0001557581573896353,
      "loss": 2.2509,
      "step": 1504
    },
    {
      "epoch": 0.48156147508199343,
      "grad_norm": 8.11463451385498,
      "learning_rate": 0.00015566218809980804,
      "loss": 1.3435,
      "step": 1505
    },
    {
      "epoch": 0.48188144948404127,
      "grad_norm": 6.1967878341674805,
      "learning_rate": 0.0001555662188099808,
      "loss": 1.0023,
      "step": 1506
    },
    {
      "epoch": 0.4822014238860891,
      "grad_norm": 4.232693672180176,
      "learning_rate": 0.00015547024952015354,
      "loss": 0.5721,
      "step": 1507
    },
    {
      "epoch": 0.48252139828813695,
      "grad_norm": 3.1314682960510254,
      "learning_rate": 0.0001553742802303263,
      "loss": 0.4622,
      "step": 1508
    },
    {
      "epoch": 0.4828413726901848,
      "grad_norm": 12.646327018737793,
      "learning_rate": 0.00015527831094049903,
      "loss": 1.0679,
      "step": 1509
    },
    {
      "epoch": 0.48316134709223263,
      "grad_norm": 2.276909112930298,
      "learning_rate": 0.00015518234165067178,
      "loss": 0.4031,
      "step": 1510
    },
    {
      "epoch": 0.48348132149428047,
      "grad_norm": 7.945936679840088,
      "learning_rate": 0.00015508637236084452,
      "loss": 0.4122,
      "step": 1511
    },
    {
      "epoch": 0.4838012958963283,
      "grad_norm": 2.5043022632598877,
      "learning_rate": 0.00015499040307101727,
      "loss": 0.1197,
      "step": 1512
    },
    {
      "epoch": 0.48412127029837615,
      "grad_norm": 0.5694628357887268,
      "learning_rate": 0.00015489443378119002,
      "loss": 0.0273,
      "step": 1513
    },
    {
      "epoch": 0.484441244700424,
      "grad_norm": 5.242062568664551,
      "learning_rate": 0.00015479846449136276,
      "loss": 0.2583,
      "step": 1514
    },
    {
      "epoch": 0.48476121910247183,
      "grad_norm": 8.111255645751953,
      "learning_rate": 0.00015470249520153548,
      "loss": 0.3895,
      "step": 1515
    },
    {
      "epoch": 0.4850811935045196,
      "grad_norm": 4.77578067779541,
      "learning_rate": 0.00015460652591170823,
      "loss": 0.2513,
      "step": 1516
    },
    {
      "epoch": 0.48540116790656745,
      "grad_norm": 8.985650062561035,
      "learning_rate": 0.00015451055662188098,
      "loss": 0.2681,
      "step": 1517
    },
    {
      "epoch": 0.4857211423086153,
      "grad_norm": 4.201861381530762,
      "learning_rate": 0.00015441458733205372,
      "loss": 0.1199,
      "step": 1518
    },
    {
      "epoch": 0.48604111671066313,
      "grad_norm": 3.4989707469940186,
      "learning_rate": 0.00015431861804222647,
      "loss": 0.278,
      "step": 1519
    },
    {
      "epoch": 0.486361091112711,
      "grad_norm": 6.92397928237915,
      "learning_rate": 0.00015422264875239922,
      "loss": 0.253,
      "step": 1520
    },
    {
      "epoch": 0.4866810655147588,
      "grad_norm": 4.800976753234863,
      "learning_rate": 0.000154126679462572,
      "loss": 0.3016,
      "step": 1521
    },
    {
      "epoch": 0.48700103991680666,
      "grad_norm": 0.015458508394658566,
      "learning_rate": 0.0001540307101727447,
      "loss": 0.0009,
      "step": 1522
    },
    {
      "epoch": 0.4873210143188545,
      "grad_norm": 3.1010100841522217,
      "learning_rate": 0.00015393474088291746,
      "loss": 0.2361,
      "step": 1523
    },
    {
      "epoch": 0.48764098872090234,
      "grad_norm": 8.729921340942383,
      "learning_rate": 0.0001538387715930902,
      "loss": 0.265,
      "step": 1524
    },
    {
      "epoch": 0.4879609631229502,
      "grad_norm": 4.465453624725342,
      "learning_rate": 0.00015374280230326295,
      "loss": 0.2444,
      "step": 1525
    },
    {
      "epoch": 0.488280937524998,
      "grad_norm": 0.4970971941947937,
      "learning_rate": 0.0001536468330134357,
      "loss": 0.0074,
      "step": 1526
    },
    {
      "epoch": 0.48860091192704586,
      "grad_norm": 0.44902727007865906,
      "learning_rate": 0.00015355086372360844,
      "loss": 0.0101,
      "step": 1527
    },
    {
      "epoch": 0.4889208863290937,
      "grad_norm": 0.06991666555404663,
      "learning_rate": 0.00015345489443378116,
      "loss": 0.0007,
      "step": 1528
    },
    {
      "epoch": 0.48924086073114154,
      "grad_norm": 0.01922784559428692,
      "learning_rate": 0.0001533589251439539,
      "loss": 0.0005,
      "step": 1529
    },
    {
      "epoch": 0.4895608351331893,
      "grad_norm": 5.301447868347168,
      "learning_rate": 0.00015326295585412665,
      "loss": 0.064,
      "step": 1530
    },
    {
      "epoch": 0.48988080953523716,
      "grad_norm": 0.010096804238855839,
      "learning_rate": 0.0001531669865642994,
      "loss": 0.0005,
      "step": 1531
    },
    {
      "epoch": 0.490200783937285,
      "grad_norm": 3.5526623725891113,
      "learning_rate": 0.00015307101727447215,
      "loss": 0.1554,
      "step": 1532
    },
    {
      "epoch": 0.49052075833933284,
      "grad_norm": 5.285055637359619,
      "learning_rate": 0.00015297504798464492,
      "loss": 0.0345,
      "step": 1533
    },
    {
      "epoch": 0.4908407327413807,
      "grad_norm": 1.4369633197784424,
      "learning_rate": 0.00015287907869481767,
      "loss": 0.0984,
      "step": 1534
    },
    {
      "epoch": 0.4911607071434285,
      "grad_norm": 5.821697235107422,
      "learning_rate": 0.0001527831094049904,
      "loss": 0.2323,
      "step": 1535
    },
    {
      "epoch": 0.49148068154547636,
      "grad_norm": 0.0045950268395245075,
      "learning_rate": 0.00015268714011516313,
      "loss": 0.0003,
      "step": 1536
    },
    {
      "epoch": 0.4918006559475242,
      "grad_norm": 0.004527261946350336,
      "learning_rate": 0.00015259117082533588,
      "loss": 0.0002,
      "step": 1537
    },
    {
      "epoch": 0.49212063034957204,
      "grad_norm": 1.675019383430481,
      "learning_rate": 0.00015249520153550863,
      "loss": 0.2138,
      "step": 1538
    },
    {
      "epoch": 0.4924406047516199,
      "grad_norm": 1.3717948198318481,
      "learning_rate": 0.00015239923224568137,
      "loss": 0.1943,
      "step": 1539
    },
    {
      "epoch": 0.4927605791536677,
      "grad_norm": 0.22804303467273712,
      "learning_rate": 0.00015230326295585412,
      "loss": 0.0012,
      "step": 1540
    },
    {
      "epoch": 0.49308055355571556,
      "grad_norm": 0.016852209344506264,
      "learning_rate": 0.00015220729366602684,
      "loss": 0.0007,
      "step": 1541
    },
    {
      "epoch": 0.4934005279577634,
      "grad_norm": 0.018932661041617393,
      "learning_rate": 0.0001521113243761996,
      "loss": 0.0007,
      "step": 1542
    },
    {
      "epoch": 0.49372050235981124,
      "grad_norm": 0.029305854812264442,
      "learning_rate": 0.00015201535508637233,
      "loss": 0.0012,
      "step": 1543
    },
    {
      "epoch": 0.494040476761859,
      "grad_norm": 0.004180664662271738,
      "learning_rate": 0.00015191938579654508,
      "loss": 0.0002,
      "step": 1544
    },
    {
      "epoch": 0.49436045116390687,
      "grad_norm": 0.04187583923339844,
      "learning_rate": 0.00015182341650671783,
      "loss": 0.0003,
      "step": 1545
    },
    {
      "epoch": 0.4946804255659547,
      "grad_norm": 0.014612878672778606,
      "learning_rate": 0.0001517274472168906,
      "loss": 0.0003,
      "step": 1546
    },
    {
      "epoch": 0.49500039996800255,
      "grad_norm": 0.0060427309945225716,
      "learning_rate": 0.00015163147792706335,
      "loss": 0.0002,
      "step": 1547
    },
    {
      "epoch": 0.4953203743700504,
      "grad_norm": 0.9062721729278564,
      "learning_rate": 0.00015153550863723607,
      "loss": 0.1383,
      "step": 1548
    },
    {
      "epoch": 0.4956403487720982,
      "grad_norm": 1.723035216331482,
      "learning_rate": 0.00015143953934740881,
      "loss": 0.4087,
      "step": 1549
    },
    {
      "epoch": 0.49596032317414607,
      "grad_norm": 1.4654022455215454,
      "learning_rate": 0.00015134357005758156,
      "loss": 0.1693,
      "step": 1550
    },
    {
      "epoch": 0.4962802975761939,
      "grad_norm": 0.8045018911361694,
      "learning_rate": 0.0001512476007677543,
      "loss": 0.039,
      "step": 1551
    },
    {
      "epoch": 0.49660027197824175,
      "grad_norm": 0.013219058513641357,
      "learning_rate": 0.00015115163147792705,
      "loss": 0.0004,
      "step": 1552
    },
    {
      "epoch": 0.4969202463802896,
      "grad_norm": 0.007997206412255764,
      "learning_rate": 0.0001510556621880998,
      "loss": 0.0004,
      "step": 1553
    },
    {
      "epoch": 0.4972402207823374,
      "grad_norm": 0.008447091095149517,
      "learning_rate": 0.00015095969289827252,
      "loss": 0.0004,
      "step": 1554
    },
    {
      "epoch": 0.49756019518438527,
      "grad_norm": 0.87677401304245,
      "learning_rate": 0.00015086372360844527,
      "loss": 0.0042,
      "step": 1555
    },
    {
      "epoch": 0.4978801695864331,
      "grad_norm": 0.020209474489092827,
      "learning_rate": 0.00015076775431861801,
      "loss": 0.0006,
      "step": 1556
    },
    {
      "epoch": 0.49820014398848095,
      "grad_norm": 0.6315971612930298,
      "learning_rate": 0.00015067178502879076,
      "loss": 0.0343,
      "step": 1557
    },
    {
      "epoch": 0.49852011839052873,
      "grad_norm": 0.013876579701900482,
      "learning_rate": 0.00015057581573896353,
      "loss": 0.0006,
      "step": 1558
    },
    {
      "epoch": 0.4988400927925766,
      "grad_norm": 1.3861942291259766,
      "learning_rate": 0.00015047984644913628,
      "loss": 0.0171,
      "step": 1559
    },
    {
      "epoch": 0.4991600671946244,
      "grad_norm": 0.1501612514257431,
      "learning_rate": 0.00015038387715930903,
      "loss": 0.0054,
      "step": 1560
    },
    {
      "epoch": 0.49948004159667225,
      "grad_norm": 1.3536503314971924,
      "learning_rate": 0.00015028790786948177,
      "loss": 0.1123,
      "step": 1561
    },
    {
      "epoch": 0.4998000159987201,
      "grad_norm": 0.03732600063085556,
      "learning_rate": 0.0001501919385796545,
      "loss": 0.0013,
      "step": 1562
    },
    {
      "epoch": 0.5001199904007679,
      "grad_norm": 0.010288628749549389,
      "learning_rate": 0.00015009596928982724,
      "loss": 0.0004,
      "step": 1563
    },
    {
      "epoch": 0.5004399648028158,
      "grad_norm": 1.2370924949645996,
      "learning_rate": 0.00015,
      "loss": 0.1818,
      "step": 1564
    },
    {
      "epoch": 0.5007599392048636,
      "grad_norm": 2.173680305480957,
      "learning_rate": 0.00014990403071017273,
      "loss": 0.099,
      "step": 1565
    },
    {
      "epoch": 0.5010799136069114,
      "grad_norm": 2.943103790283203,
      "learning_rate": 0.00014980806142034548,
      "loss": 0.3259,
      "step": 1566
    },
    {
      "epoch": 0.5013998880089593,
      "grad_norm": 1.240263819694519,
      "learning_rate": 0.0001497120921305182,
      "loss": 0.0269,
      "step": 1567
    },
    {
      "epoch": 0.5017198624110071,
      "grad_norm": 1.3762791156768799,
      "learning_rate": 0.00014961612284069097,
      "loss": 0.0357,
      "step": 1568
    },
    {
      "epoch": 0.502039836813055,
      "grad_norm": 0.010594380088150501,
      "learning_rate": 0.00014952015355086372,
      "loss": 0.0003,
      "step": 1569
    },
    {
      "epoch": 0.5023598112151028,
      "grad_norm": 2.8513948917388916,
      "learning_rate": 0.00014942418426103647,
      "loss": 0.0464,
      "step": 1570
    },
    {
      "epoch": 0.5026797856171507,
      "grad_norm": 0.0174835454672575,
      "learning_rate": 0.00014932821497120919,
      "loss": 0.0005,
      "step": 1571
    },
    {
      "epoch": 0.5029997600191984,
      "grad_norm": 0.01165272481739521,
      "learning_rate": 0.00014923224568138193,
      "loss": 0.0004,
      "step": 1572
    },
    {
      "epoch": 0.5033197344212463,
      "grad_norm": 0.013747582212090492,
      "learning_rate": 0.00014913627639155468,
      "loss": 0.0005,
      "step": 1573
    },
    {
      "epoch": 0.5036397088232941,
      "grad_norm": 0.007766442373394966,
      "learning_rate": 0.00014904030710172745,
      "loss": 0.0003,
      "step": 1574
    },
    {
      "epoch": 0.503959683225342,
      "grad_norm": 9.653186798095703,
      "learning_rate": 0.00014894433781190017,
      "loss": 0.1629,
      "step": 1575
    },
    {
      "epoch": 0.5042796576273898,
      "grad_norm": 0.0062368945218622684,
      "learning_rate": 0.00014884836852207292,
      "loss": 0.0003,
      "step": 1576
    },
    {
      "epoch": 0.5045996320294377,
      "grad_norm": 0.0030578894075006247,
      "learning_rate": 0.00014875239923224567,
      "loss": 0.0002,
      "step": 1577
    },
    {
      "epoch": 0.5049196064314855,
      "grad_norm": 8.939224243164062,
      "learning_rate": 0.0001486564299424184,
      "loss": 0.2451,
      "step": 1578
    },
    {
      "epoch": 0.5052395808335333,
      "grad_norm": 11.16843032836914,
      "learning_rate": 0.00014856046065259116,
      "loss": 0.3462,
      "step": 1579
    },
    {
      "epoch": 0.5055595552355812,
      "grad_norm": 5.653203964233398,
      "learning_rate": 0.0001484644913627639,
      "loss": 0.0179,
      "step": 1580
    },
    {
      "epoch": 0.5058795296376289,
      "grad_norm": 0.004694913513958454,
      "learning_rate": 0.00014836852207293665,
      "loss": 0.0002,
      "step": 1581
    },
    {
      "epoch": 0.5061995040396768,
      "grad_norm": 10.82114315032959,
      "learning_rate": 0.0001482725527831094,
      "loss": 0.0867,
      "step": 1582
    },
    {
      "epoch": 0.5065194784417246,
      "grad_norm": 3.531050205230713,
      "learning_rate": 0.00014817658349328215,
      "loss": 0.15,
      "step": 1583
    },
    {
      "epoch": 0.5068394528437725,
      "grad_norm": 0.05490463972091675,
      "learning_rate": 0.00014808061420345487,
      "loss": 0.0005,
      "step": 1584
    },
    {
      "epoch": 0.5071594272458203,
      "grad_norm": 0.009017602540552616,
      "learning_rate": 0.0001479846449136276,
      "loss": 0.0002,
      "step": 1585
    },
    {
      "epoch": 0.5074794016478682,
      "grad_norm": 1.4319508075714111,
      "learning_rate": 0.00014788867562380039,
      "loss": 0.1498,
      "step": 1586
    },
    {
      "epoch": 0.507799376049916,
      "grad_norm": 0.004782485775649548,
      "learning_rate": 0.00014779270633397313,
      "loss": 0.0002,
      "step": 1587
    },
    {
      "epoch": 0.5081193504519639,
      "grad_norm": 1.1261461973190308,
      "learning_rate": 0.00014769673704414585,
      "loss": 0.1184,
      "step": 1588
    },
    {
      "epoch": 0.5084393248540117,
      "grad_norm": 3.9439754486083984,
      "learning_rate": 0.0001476007677543186,
      "loss": 0.0569,
      "step": 1589
    },
    {
      "epoch": 0.5087592992560596,
      "grad_norm": 0.41243717074394226,
      "learning_rate": 0.00014750479846449135,
      "loss": 0.009,
      "step": 1590
    },
    {
      "epoch": 0.5090792736581073,
      "grad_norm": 4.620717525482178,
      "learning_rate": 0.0001474088291746641,
      "loss": 0.0437,
      "step": 1591
    },
    {
      "epoch": 0.5093992480601552,
      "grad_norm": 1.1361403465270996,
      "learning_rate": 0.00014731285988483684,
      "loss": 0.0691,
      "step": 1592
    },
    {
      "epoch": 0.509719222462203,
      "grad_norm": 0.0766204372048378,
      "learning_rate": 0.00014721689059500959,
      "loss": 0.0004,
      "step": 1593
    },
    {
      "epoch": 0.5100391968642508,
      "grad_norm": 7.306025981903076,
      "learning_rate": 0.00014712092130518233,
      "loss": 0.3031,
      "step": 1594
    },
    {
      "epoch": 0.5103591712662987,
      "grad_norm": 1.6679935455322266,
      "learning_rate": 0.00014702495201535508,
      "loss": 0.1417,
      "step": 1595
    },
    {
      "epoch": 0.5106791456683465,
      "grad_norm": 4.3549604415893555,
      "learning_rate": 0.00014692898272552783,
      "loss": 0.0649,
      "step": 1596
    },
    {
      "epoch": 0.5109991200703944,
      "grad_norm": 1.1499457359313965,
      "learning_rate": 0.00014683301343570055,
      "loss": 0.0105,
      "step": 1597
    },
    {
      "epoch": 0.5113190944724422,
      "grad_norm": 1.607157588005066,
      "learning_rate": 0.0001467370441458733,
      "loss": 0.0822,
      "step": 1598
    },
    {
      "epoch": 0.5116390688744901,
      "grad_norm": 1.9613879919052124,
      "learning_rate": 0.00014664107485604607,
      "loss": 0.0679,
      "step": 1599
    },
    {
      "epoch": 0.5119590432765379,
      "grad_norm": 0.01666875369846821,
      "learning_rate": 0.0001465451055662188,
      "loss": 0.0003,
      "step": 1600
    },
    {
      "epoch": 0.5122790176785857,
      "grad_norm": 1.5107568502426147,
      "learning_rate": 0.00014644913627639153,
      "loss": 0.051,
      "step": 1601
    },
    {
      "epoch": 0.5125989920806335,
      "grad_norm": 0.005457471590489149,
      "learning_rate": 0.00014635316698656428,
      "loss": 0.0002,
      "step": 1602
    },
    {
      "epoch": 0.5129189664826814,
      "grad_norm": 1.5320748090744019,
      "learning_rate": 0.00014625719769673703,
      "loss": 0.0789,
      "step": 1603
    },
    {
      "epoch": 0.5132389408847292,
      "grad_norm": 2.016787528991699,
      "learning_rate": 0.00014616122840690977,
      "loss": 0.1024,
      "step": 1604
    },
    {
      "epoch": 0.5135589152867771,
      "grad_norm": 0.0037127744872123003,
      "learning_rate": 0.00014606525911708252,
      "loss": 0.0002,
      "step": 1605
    },
    {
      "epoch": 0.5138788896888249,
      "grad_norm": 1.7510360479354858,
      "learning_rate": 0.00014596928982725527,
      "loss": 0.1215,
      "step": 1606
    },
    {
      "epoch": 0.5141988640908727,
      "grad_norm": 3.177006721496582,
      "learning_rate": 0.000145873320537428,
      "loss": 0.0887,
      "step": 1607
    },
    {
      "epoch": 0.5145188384929206,
      "grad_norm": 0.26115697622299194,
      "learning_rate": 0.00014577735124760076,
      "loss": 0.0031,
      "step": 1608
    },
    {
      "epoch": 0.5148388128949684,
      "grad_norm": 1.5554124116897583,
      "learning_rate": 0.0001456813819577735,
      "loss": 0.1062,
      "step": 1609
    },
    {
      "epoch": 0.5151587872970163,
      "grad_norm": 0.0022486259695142508,
      "learning_rate": 0.00014558541266794625,
      "loss": 0.0001,
      "step": 1610
    },
    {
      "epoch": 0.515478761699064,
      "grad_norm": 0.0013083532685413957,
      "learning_rate": 0.000145489443378119,
      "loss": 0.0001,
      "step": 1611
    },
    {
      "epoch": 0.5157987361011119,
      "grad_norm": 0.2122839093208313,
      "learning_rate": 0.00014539347408829174,
      "loss": 0.0016,
      "step": 1612
    },
    {
      "epoch": 0.5161187105031597,
      "grad_norm": 1.030828833580017,
      "learning_rate": 0.0001452975047984645,
      "loss": 0.0039,
      "step": 1613
    },
    {
      "epoch": 0.5164386849052076,
      "grad_norm": 0.9225738644599915,
      "learning_rate": 0.0001452015355086372,
      "loss": 0.0236,
      "step": 1614
    },
    {
      "epoch": 0.5167586593072554,
      "grad_norm": 0.0017402060329914093,
      "learning_rate": 0.00014510556621880996,
      "loss": 0.0001,
      "step": 1615
    },
    {
      "epoch": 0.5170786337093033,
      "grad_norm": 0.22222255170345306,
      "learning_rate": 0.0001450095969289827,
      "loss": 0.002,
      "step": 1616
    },
    {
      "epoch": 0.5173986081113511,
      "grad_norm": 0.002595271449536085,
      "learning_rate": 0.00014491362763915545,
      "loss": 0.0001,
      "step": 1617
    },
    {
      "epoch": 0.517718582513399,
      "grad_norm": 0.10481245815753937,
      "learning_rate": 0.0001448176583493282,
      "loss": 0.0012,
      "step": 1618
    },
    {
      "epoch": 0.5180385569154468,
      "grad_norm": 4.003936290740967,
      "learning_rate": 0.00014472168905950094,
      "loss": 0.1892,
      "step": 1619
    },
    {
      "epoch": 0.5183585313174947,
      "grad_norm": 0.8797054886817932,
      "learning_rate": 0.0001446257197696737,
      "loss": 0.0303,
      "step": 1620
    },
    {
      "epoch": 0.5186785057195424,
      "grad_norm": 1.5763262510299683,
      "learning_rate": 0.00014452975047984644,
      "loss": 0.185,
      "step": 1621
    },
    {
      "epoch": 0.5189984801215902,
      "grad_norm": 0.0020777073223143816,
      "learning_rate": 0.00014443378119001918,
      "loss": 0.0001,
      "step": 1622
    },
    {
      "epoch": 0.5193184545236381,
      "grad_norm": 0.026840604841709137,
      "learning_rate": 0.00014433781190019193,
      "loss": 0.0003,
      "step": 1623
    },
    {
      "epoch": 0.5196384289256859,
      "grad_norm": 0.0014278115704655647,
      "learning_rate": 0.00014424184261036468,
      "loss": 0.0001,
      "step": 1624
    },
    {
      "epoch": 0.5199584033277338,
      "grad_norm": 7.184865474700928,
      "learning_rate": 0.00014414587332053742,
      "loss": 0.0832,
      "step": 1625
    },
    {
      "epoch": 0.5202783777297816,
      "grad_norm": 0.1471063196659088,
      "learning_rate": 0.00014404990403071017,
      "loss": 0.0008,
      "step": 1626
    },
    {
      "epoch": 0.5205983521318295,
      "grad_norm": 0.004223930183798075,
      "learning_rate": 0.0001439539347408829,
      "loss": 0.0002,
      "step": 1627
    },
    {
      "epoch": 0.5209183265338773,
      "grad_norm": 0.18873046338558197,
      "learning_rate": 0.00014385796545105564,
      "loss": 0.0032,
      "step": 1628
    },
    {
      "epoch": 0.5212383009359252,
      "grad_norm": 0.22162052989006042,
      "learning_rate": 0.00014376199616122838,
      "loss": 0.001,
      "step": 1629
    },
    {
      "epoch": 0.5215582753379729,
      "grad_norm": 0.0011099963448941708,
      "learning_rate": 0.00014366602687140116,
      "loss": 0.0001,
      "step": 1630
    },
    {
      "epoch": 0.5218782497400208,
      "grad_norm": 2.2158048152923584,
      "learning_rate": 0.00014357005758157388,
      "loss": 0.0957,
      "step": 1631
    },
    {
      "epoch": 0.5221982241420686,
      "grad_norm": 2.4234554767608643,
      "learning_rate": 0.00014347408829174662,
      "loss": 0.0111,
      "step": 1632
    },
    {
      "epoch": 0.5225181985441165,
      "grad_norm": 2.160895586013794,
      "learning_rate": 0.00014337811900191937,
      "loss": 0.1387,
      "step": 1633
    },
    {
      "epoch": 0.5228381729461643,
      "grad_norm": 1.0636026859283447,
      "learning_rate": 0.00014328214971209212,
      "loss": 0.0591,
      "step": 1634
    },
    {
      "epoch": 0.5231581473482121,
      "grad_norm": 0.003001208184286952,
      "learning_rate": 0.00014318618042226486,
      "loss": 0.0002,
      "step": 1635
    },
    {
      "epoch": 0.52347812175026,
      "grad_norm": 0.009962036274373531,
      "learning_rate": 0.0001430902111324376,
      "loss": 0.0004,
      "step": 1636
    },
    {
      "epoch": 0.5237980961523078,
      "grad_norm": 0.7470145225524902,
      "learning_rate": 0.00014299424184261036,
      "loss": 0.0987,
      "step": 1637
    },
    {
      "epoch": 0.5241180705543557,
      "grad_norm": 0.3355960249900818,
      "learning_rate": 0.0001428982725527831,
      "loss": 0.0025,
      "step": 1638
    },
    {
      "epoch": 0.5244380449564034,
      "grad_norm": 0.9091877937316895,
      "learning_rate": 0.00014280230326295585,
      "loss": 0.0285,
      "step": 1639
    },
    {
      "epoch": 0.5247580193584513,
      "grad_norm": 1.5284271240234375,
      "learning_rate": 0.0001427063339731286,
      "loss": 0.0063,
      "step": 1640
    },
    {
      "epoch": 0.5250779937604991,
      "grad_norm": 3.075327157974243,
      "learning_rate": 0.00014261036468330132,
      "loss": 0.0642,
      "step": 1641
    },
    {
      "epoch": 0.525397968162547,
      "grad_norm": 0.048519156873226166,
      "learning_rate": 0.00014251439539347406,
      "loss": 0.0005,
      "step": 1642
    },
    {
      "epoch": 0.5257179425645948,
      "grad_norm": 0.03758132457733154,
      "learning_rate": 0.00014241842610364684,
      "loss": 0.001,
      "step": 1643
    },
    {
      "epoch": 0.5260379169666427,
      "grad_norm": 0.14865045249462128,
      "learning_rate": 0.00014232245681381956,
      "loss": 0.0012,
      "step": 1644
    },
    {
      "epoch": 0.5263578913686905,
      "grad_norm": 0.061579398810863495,
      "learning_rate": 0.0001422264875239923,
      "loss": 0.0016,
      "step": 1645
    },
    {
      "epoch": 0.5266778657707384,
      "grad_norm": 0.002736499300226569,
      "learning_rate": 0.00014213051823416505,
      "loss": 0.0002,
      "step": 1646
    },
    {
      "epoch": 0.5269978401727862,
      "grad_norm": 2.5992698669433594,
      "learning_rate": 0.0001420345489443378,
      "loss": 0.0507,
      "step": 1647
    },
    {
      "epoch": 0.5273178145748341,
      "grad_norm": 0.06917569041252136,
      "learning_rate": 0.00014193857965451054,
      "loss": 0.0003,
      "step": 1648
    },
    {
      "epoch": 0.5276377889768818,
      "grad_norm": 1.717184066772461,
      "learning_rate": 0.0001418426103646833,
      "loss": 0.1632,
      "step": 1649
    },
    {
      "epoch": 0.5279577633789296,
      "grad_norm": 0.005965785123407841,
      "learning_rate": 0.00014174664107485604,
      "loss": 0.0002,
      "step": 1650
    },
    {
      "epoch": 0.5282777377809775,
      "grad_norm": 0.2219126969575882,
      "learning_rate": 0.00014165067178502878,
      "loss": 0.0032,
      "step": 1651
    },
    {
      "epoch": 0.5285977121830253,
      "grad_norm": 0.7217652201652527,
      "learning_rate": 0.00014155470249520153,
      "loss": 0.0428,
      "step": 1652
    },
    {
      "epoch": 0.5289176865850732,
      "grad_norm": 0.015204446390271187,
      "learning_rate": 0.00014145873320537428,
      "loss": 0.0004,
      "step": 1653
    },
    {
      "epoch": 0.529237660987121,
      "grad_norm": 0.0041733053512871265,
      "learning_rate": 0.000141362763915547,
      "loss": 0.0002,
      "step": 1654
    },
    {
      "epoch": 0.5295576353891689,
      "grad_norm": 0.023012056946754456,
      "learning_rate": 0.00014126679462571977,
      "loss": 0.0006,
      "step": 1655
    },
    {
      "epoch": 0.5298776097912167,
      "grad_norm": 0.0017456089844927192,
      "learning_rate": 0.00014117082533589252,
      "loss": 0.0001,
      "step": 1656
    },
    {
      "epoch": 0.5301975841932646,
      "grad_norm": 0.25099772214889526,
      "learning_rate": 0.00014107485604606524,
      "loss": 0.0043,
      "step": 1657
    },
    {
      "epoch": 0.5305175585953124,
      "grad_norm": 0.006597273051738739,
      "learning_rate": 0.00014097888675623798,
      "loss": 0.0002,
      "step": 1658
    },
    {
      "epoch": 0.5308375329973603,
      "grad_norm": 2.9859025478363037,
      "learning_rate": 0.00014088291746641073,
      "loss": 0.0125,
      "step": 1659
    },
    {
      "epoch": 0.531157507399408,
      "grad_norm": 5.082497596740723,
      "learning_rate": 0.00014078694817658348,
      "loss": 0.033,
      "step": 1660
    },
    {
      "epoch": 0.5314774818014559,
      "grad_norm": 0.6732354164123535,
      "learning_rate": 0.00014069097888675622,
      "loss": 0.0394,
      "step": 1661
    },
    {
      "epoch": 0.5317974562035037,
      "grad_norm": 0.01615975983440876,
      "learning_rate": 0.00014059500959692897,
      "loss": 0.0003,
      "step": 1662
    },
    {
      "epoch": 0.5321174306055516,
      "grad_norm": 0.24174778163433075,
      "learning_rate": 0.00014049904030710172,
      "loss": 0.0096,
      "step": 1663
    },
    {
      "epoch": 0.5324374050075994,
      "grad_norm": 0.12253045290708542,
      "learning_rate": 0.00014040307101727446,
      "loss": 0.0014,
      "step": 1664
    },
    {
      "epoch": 0.5327573794096472,
      "grad_norm": 5.175212383270264,
      "learning_rate": 0.0001403071017274472,
      "loss": 0.0786,
      "step": 1665
    },
    {
      "epoch": 0.5330773538116951,
      "grad_norm": 0.004720807541161776,
      "learning_rate": 0.00014021113243761996,
      "loss": 0.0002,
      "step": 1666
    },
    {
      "epoch": 0.5333973282137429,
      "grad_norm": 0.0059380666352808475,
      "learning_rate": 0.00014011516314779268,
      "loss": 0.0002,
      "step": 1667
    },
    {
      "epoch": 0.5337173026157908,
      "grad_norm": 0.0009945554193109274,
      "learning_rate": 0.00014001919385796545,
      "loss": 0.0001,
      "step": 1668
    },
    {
      "epoch": 0.5340372770178385,
      "grad_norm": 0.000518294982612133,
      "learning_rate": 0.0001399232245681382,
      "loss": 0.0,
      "step": 1669
    },
    {
      "epoch": 0.5343572514198864,
      "grad_norm": 2.3985579013824463,
      "learning_rate": 0.00013982725527831094,
      "loss": 0.0567,
      "step": 1670
    },
    {
      "epoch": 0.5346772258219342,
      "grad_norm": 0.03546752780675888,
      "learning_rate": 0.00013973128598848366,
      "loss": 0.0009,
      "step": 1671
    },
    {
      "epoch": 0.5349972002239821,
      "grad_norm": 1.4750498533248901,
      "learning_rate": 0.0001396353166986564,
      "loss": 0.0787,
      "step": 1672
    },
    {
      "epoch": 0.5353171746260299,
      "grad_norm": 2.287147283554077,
      "learning_rate": 0.00013953934740882916,
      "loss": 0.0352,
      "step": 1673
    },
    {
      "epoch": 0.5356371490280778,
      "grad_norm": 1.0974987745285034,
      "learning_rate": 0.0001394433781190019,
      "loss": 0.0186,
      "step": 1674
    },
    {
      "epoch": 0.5359571234301256,
      "grad_norm": 0.00690396036952734,
      "learning_rate": 0.00013934740882917465,
      "loss": 0.0002,
      "step": 1675
    },
    {
      "epoch": 0.5362770978321735,
      "grad_norm": 0.0004863936628680676,
      "learning_rate": 0.0001392514395393474,
      "loss": 0.0,
      "step": 1676
    },
    {
      "epoch": 0.5365970722342213,
      "grad_norm": 0.020099133253097534,
      "learning_rate": 0.00013915547024952014,
      "loss": 0.0002,
      "step": 1677
    },
    {
      "epoch": 0.536917046636269,
      "grad_norm": 0.9688081741333008,
      "learning_rate": 0.0001390595009596929,
      "loss": 0.013,
      "step": 1678
    },
    {
      "epoch": 0.5372370210383169,
      "grad_norm": 0.00041143130511045456,
      "learning_rate": 0.00013896353166986564,
      "loss": 0.0,
      "step": 1679
    },
    {
      "epoch": 0.5375569954403647,
      "grad_norm": 0.047914501279592514,
      "learning_rate": 0.00013886756238003838,
      "loss": 0.0002,
      "step": 1680
    },
    {
      "epoch": 0.5378769698424126,
      "grad_norm": 5.715567588806152,
      "learning_rate": 0.00013877159309021113,
      "loss": 0.0629,
      "step": 1681
    },
    {
      "epoch": 0.5381969442444604,
      "grad_norm": 0.09304898232221603,
      "learning_rate": 0.00013867562380038388,
      "loss": 0.0007,
      "step": 1682
    },
    {
      "epoch": 0.5385169186465083,
      "grad_norm": 0.0005079375114291906,
      "learning_rate": 0.00013857965451055662,
      "loss": 0.0,
      "step": 1683
    },
    {
      "epoch": 0.5388368930485561,
      "grad_norm": 0.35172131657600403,
      "learning_rate": 0.00013848368522072934,
      "loss": 0.004,
      "step": 1684
    },
    {
      "epoch": 0.539156867450604,
      "grad_norm": 0.0009658486815169454,
      "learning_rate": 0.0001383877159309021,
      "loss": 0.0,
      "step": 1685
    },
    {
      "epoch": 0.5394768418526518,
      "grad_norm": 0.0028199832886457443,
      "learning_rate": 0.00013829174664107486,
      "loss": 0.0001,
      "step": 1686
    },
    {
      "epoch": 0.5397968162546997,
      "grad_norm": 2.6637678146362305,
      "learning_rate": 0.00013819577735124758,
      "loss": 0.0633,
      "step": 1687
    },
    {
      "epoch": 0.5401167906567474,
      "grad_norm": 0.0010297228582203388,
      "learning_rate": 0.00013809980806142033,
      "loss": 0.0,
      "step": 1688
    },
    {
      "epoch": 0.5404367650587953,
      "grad_norm": 0.000523166439961642,
      "learning_rate": 0.00013800383877159307,
      "loss": 0.0,
      "step": 1689
    },
    {
      "epoch": 0.5407567394608431,
      "grad_norm": 0.2918991148471832,
      "learning_rate": 0.00013790786948176582,
      "loss": 0.0034,
      "step": 1690
    },
    {
      "epoch": 0.541076713862891,
      "grad_norm": 0.0011092342901974916,
      "learning_rate": 0.00013781190019193857,
      "loss": 0.0,
      "step": 1691
    },
    {
      "epoch": 0.5413966882649388,
      "grad_norm": 5.779646873474121,
      "learning_rate": 0.00013771593090211131,
      "loss": 0.0837,
      "step": 1692
    },
    {
      "epoch": 0.5417166626669866,
      "grad_norm": 0.0026553203351795673,
      "learning_rate": 0.00013761996161228406,
      "loss": 0.0001,
      "step": 1693
    },
    {
      "epoch": 0.5420366370690345,
      "grad_norm": 0.11377769708633423,
      "learning_rate": 0.0001375239923224568,
      "loss": 0.0018,
      "step": 1694
    },
    {
      "epoch": 0.5423566114710823,
      "grad_norm": 0.05242648348212242,
      "learning_rate": 0.00013742802303262955,
      "loss": 0.0004,
      "step": 1695
    },
    {
      "epoch": 0.5426765858731302,
      "grad_norm": 0.5283592939376831,
      "learning_rate": 0.0001373320537428023,
      "loss": 0.0089,
      "step": 1696
    },
    {
      "epoch": 0.542996560275178,
      "grad_norm": 0.057499807327985764,
      "learning_rate": 0.00013723608445297502,
      "loss": 0.0005,
      "step": 1697
    },
    {
      "epoch": 0.5433165346772258,
      "grad_norm": 0.05809791013598442,
      "learning_rate": 0.00013714011516314777,
      "loss": 0.0005,
      "step": 1698
    },
    {
      "epoch": 0.5436365090792736,
      "grad_norm": 0.0006437416304834187,
      "learning_rate": 0.00013704414587332054,
      "loss": 0.0,
      "step": 1699
    },
    {
      "epoch": 0.5439564834813215,
      "grad_norm": 2.8337790966033936,
      "learning_rate": 0.00013694817658349326,
      "loss": 0.0791,
      "step": 1700
    },
    {
      "epoch": 0.5442764578833693,
      "grad_norm": 0.08761203289031982,
      "learning_rate": 0.000136852207293666,
      "loss": 0.0014,
      "step": 1701
    },
    {
      "epoch": 0.5445964322854172,
      "grad_norm": 0.030965955927968025,
      "learning_rate": 0.00013675623800383875,
      "loss": 0.0002,
      "step": 1702
    },
    {
      "epoch": 0.544916406687465,
      "grad_norm": 8.523344039916992,
      "learning_rate": 0.0001366602687140115,
      "loss": 0.1505,
      "step": 1703
    },
    {
      "epoch": 0.5452363810895129,
      "grad_norm": 0.0013717388501390815,
      "learning_rate": 0.00013656429942418425,
      "loss": 0.0,
      "step": 1704
    },
    {
      "epoch": 0.5455563554915607,
      "grad_norm": 0.0016305956523865461,
      "learning_rate": 0.000136468330134357,
      "loss": 0.0001,
      "step": 1705
    },
    {
      "epoch": 0.5458763298936085,
      "grad_norm": 0.0007597400108352304,
      "learning_rate": 0.00013637236084452974,
      "loss": 0.0,
      "step": 1706
    },
    {
      "epoch": 0.5461963042956564,
      "grad_norm": 0.0006356722442433238,
      "learning_rate": 0.0001362763915547025,
      "loss": 0.0,
      "step": 1707
    },
    {
      "epoch": 0.5465162786977041,
      "grad_norm": 2.4985506534576416,
      "learning_rate": 0.00013618042226487523,
      "loss": 0.0496,
      "step": 1708
    },
    {
      "epoch": 0.546836253099752,
      "grad_norm": 0.0018741961102932692,
      "learning_rate": 0.00013608445297504798,
      "loss": 0.0001,
      "step": 1709
    },
    {
      "epoch": 0.5471562275017998,
      "grad_norm": 0.2287474423646927,
      "learning_rate": 0.0001359884836852207,
      "loss": 0.0019,
      "step": 1710
    },
    {
      "epoch": 0.5474762019038477,
      "grad_norm": 1.2455459833145142,
      "learning_rate": 0.00013589251439539347,
      "loss": 0.0122,
      "step": 1711
    },
    {
      "epoch": 0.5477961763058955,
      "grad_norm": 0.0004653374198824167,
      "learning_rate": 0.00013579654510556622,
      "loss": 0.0,
      "step": 1712
    },
    {
      "epoch": 0.5481161507079434,
      "grad_norm": 2.4564545154571533,
      "learning_rate": 0.00013570057581573897,
      "loss": 0.0971,
      "step": 1713
    },
    {
      "epoch": 0.5484361251099912,
      "grad_norm": 0.0006202290533110499,
      "learning_rate": 0.0001356046065259117,
      "loss": 0.0,
      "step": 1714
    },
    {
      "epoch": 0.5487560995120391,
      "grad_norm": 0.3985626995563507,
      "learning_rate": 0.00013550863723608443,
      "loss": 0.0051,
      "step": 1715
    },
    {
      "epoch": 0.5490760739140869,
      "grad_norm": 0.017923716455698013,
      "learning_rate": 0.00013541266794625718,
      "loss": 0.0001,
      "step": 1716
    },
    {
      "epoch": 0.5493960483161348,
      "grad_norm": 0.0005349753773771226,
      "learning_rate": 0.00013531669865642993,
      "loss": 0.0,
      "step": 1717
    },
    {
      "epoch": 0.5497160227181825,
      "grad_norm": 0.0007964270771481097,
      "learning_rate": 0.00013522072936660267,
      "loss": 0.0,
      "step": 1718
    },
    {
      "epoch": 0.5500359971202304,
      "grad_norm": 0.016173597425222397,
      "learning_rate": 0.00013512476007677542,
      "loss": 0.0002,
      "step": 1719
    },
    {
      "epoch": 0.5503559715222782,
      "grad_norm": 0.0007007124950177968,
      "learning_rate": 0.00013502879078694817,
      "loss": 0.0,
      "step": 1720
    },
    {
      "epoch": 0.550675945924326,
      "grad_norm": 0.532397449016571,
      "learning_rate": 0.0001349328214971209,
      "loss": 0.008,
      "step": 1721
    },
    {
      "epoch": 0.5509959203263739,
      "grad_norm": 0.011652995832264423,
      "learning_rate": 0.00013483685220729366,
      "loss": 0.0002,
      "step": 1722
    },
    {
      "epoch": 0.5513158947284217,
      "grad_norm": 2.380519390106201,
      "learning_rate": 0.00013474088291746638,
      "loss": 0.0099,
      "step": 1723
    },
    {
      "epoch": 0.5516358691304696,
      "grad_norm": 0.0015738557558506727,
      "learning_rate": 0.00013464491362763915,
      "loss": 0.0,
      "step": 1724
    },
    {
      "epoch": 0.5519558435325174,
      "grad_norm": 0.05424626171588898,
      "learning_rate": 0.0001345489443378119,
      "loss": 0.0009,
      "step": 1725
    },
    {
      "epoch": 0.5522758179345653,
      "grad_norm": 0.018491044640541077,
      "learning_rate": 0.00013445297504798465,
      "loss": 0.0003,
      "step": 1726
    },
    {
      "epoch": 0.552595792336613,
      "grad_norm": 0.000528522883541882,
      "learning_rate": 0.00013435700575815737,
      "loss": 0.0,
      "step": 1727
    },
    {
      "epoch": 0.5529157667386609,
      "grad_norm": 0.0005035708309151232,
      "learning_rate": 0.0001342610364683301,
      "loss": 0.0,
      "step": 1728
    },
    {
      "epoch": 0.5532357411407087,
      "grad_norm": 3.5630557537078857,
      "learning_rate": 0.00013416506717850286,
      "loss": 0.027,
      "step": 1729
    },
    {
      "epoch": 0.5535557155427566,
      "grad_norm": 0.0003785856533795595,
      "learning_rate": 0.0001340690978886756,
      "loss": 0.0,
      "step": 1730
    },
    {
      "epoch": 0.5538756899448044,
      "grad_norm": 0.0004998815129511058,
      "learning_rate": 0.00013397312859884835,
      "loss": 0.0,
      "step": 1731
    },
    {
      "epoch": 0.5541956643468523,
      "grad_norm": 0.010394449345767498,
      "learning_rate": 0.0001338771593090211,
      "loss": 0.0002,
      "step": 1732
    },
    {
      "epoch": 0.5545156387489001,
      "grad_norm": 0.0003724672133103013,
      "learning_rate": 0.00013378119001919385,
      "loss": 0.0,
      "step": 1733
    },
    {
      "epoch": 0.5548356131509479,
      "grad_norm": 0.000257293286267668,
      "learning_rate": 0.0001336852207293666,
      "loss": 0.0,
      "step": 1734
    },
    {
      "epoch": 0.5551555875529958,
      "grad_norm": 0.007768060080707073,
      "learning_rate": 0.00013358925143953934,
      "loss": 0.0001,
      "step": 1735
    },
    {
      "epoch": 0.5554755619550436,
      "grad_norm": 0.0073264287784695625,
      "learning_rate": 0.00013349328214971209,
      "loss": 0.0001,
      "step": 1736
    },
    {
      "epoch": 0.5557955363570914,
      "grad_norm": 0.8596138954162598,
      "learning_rate": 0.00013339731285988483,
      "loss": 0.0029,
      "step": 1737
    },
    {
      "epoch": 0.5561155107591392,
      "grad_norm": 0.14322616159915924,
      "learning_rate": 0.00013330134357005758,
      "loss": 0.0016,
      "step": 1738
    },
    {
      "epoch": 0.5564354851611871,
      "grad_norm": 0.014755451120436192,
      "learning_rate": 0.00013320537428023033,
      "loss": 0.0002,
      "step": 1739
    },
    {
      "epoch": 0.5567554595632349,
      "grad_norm": 0.06466503441333771,
      "learning_rate": 0.00013310940499040305,
      "loss": 0.0007,
      "step": 1740
    },
    {
      "epoch": 0.5570754339652828,
      "grad_norm": 0.0003167525283060968,
      "learning_rate": 0.0001330134357005758,
      "loss": 0.0,
      "step": 1741
    },
    {
      "epoch": 0.5573954083673306,
      "grad_norm": 0.0004247903998475522,
      "learning_rate": 0.00013291746641074854,
      "loss": 0.0,
      "step": 1742
    },
    {
      "epoch": 0.5577153827693785,
      "grad_norm": 0.8256638646125793,
      "learning_rate": 0.0001328214971209213,
      "loss": 0.0119,
      "step": 1743
    },
    {
      "epoch": 0.5580353571714263,
      "grad_norm": 6.02445125579834,
      "learning_rate": 0.00013272552783109403,
      "loss": 0.1092,
      "step": 1744
    },
    {
      "epoch": 0.5583553315734742,
      "grad_norm": 0.005202178377658129,
      "learning_rate": 0.00013262955854126678,
      "loss": 0.0001,
      "step": 1745
    },
    {
      "epoch": 0.558675305975522,
      "grad_norm": 0.2785274088382721,
      "learning_rate": 0.00013253358925143953,
      "loss": 0.0014,
      "step": 1746
    },
    {
      "epoch": 0.5589952803775698,
      "grad_norm": 0.0003316444344818592,
      "learning_rate": 0.00013243761996161227,
      "loss": 0.0,
      "step": 1747
    },
    {
      "epoch": 0.5593152547796176,
      "grad_norm": 0.01575915701687336,
      "learning_rate": 0.00013234165067178502,
      "loss": 0.0002,
      "step": 1748
    },
    {
      "epoch": 0.5596352291816654,
      "grad_norm": 0.4523250162601471,
      "learning_rate": 0.00013224568138195777,
      "loss": 0.0028,
      "step": 1749
    },
    {
      "epoch": 0.5599552035837133,
      "grad_norm": 0.05489974096417427,
      "learning_rate": 0.0001321497120921305,
      "loss": 0.0006,
      "step": 1750
    },
    {
      "epoch": 0.5602751779857611,
      "grad_norm": 0.007325052283704281,
      "learning_rate": 0.00013205374280230326,
      "loss": 0.0001,
      "step": 1751
    },
    {
      "epoch": 0.560595152387809,
      "grad_norm": 0.019373370334506035,
      "learning_rate": 0.000131957773512476,
      "loss": 0.0003,
      "step": 1752
    },
    {
      "epoch": 0.5609151267898568,
      "grad_norm": 0.03720136731863022,
      "learning_rate": 0.00013186180422264873,
      "loss": 0.0006,
      "step": 1753
    },
    {
      "epoch": 0.5612351011919047,
      "grad_norm": 0.0011706502409651875,
      "learning_rate": 0.00013176583493282147,
      "loss": 0.0,
      "step": 1754
    },
    {
      "epoch": 0.5615550755939525,
      "grad_norm": 0.009669721126556396,
      "learning_rate": 0.00013166986564299425,
      "loss": 0.0001,
      "step": 1755
    },
    {
      "epoch": 0.5618750499960004,
      "grad_norm": 0.0006162982899695635,
      "learning_rate": 0.000131573896353167,
      "loss": 0.0,
      "step": 1756
    },
    {
      "epoch": 0.5621950243980481,
      "grad_norm": 0.011176763102412224,
      "learning_rate": 0.0001314779270633397,
      "loss": 0.0002,
      "step": 1757
    },
    {
      "epoch": 0.562514998800096,
      "grad_norm": 0.14087821543216705,
      "learning_rate": 0.00013138195777351246,
      "loss": 0.0011,
      "step": 1758
    },
    {
      "epoch": 0.5628349732021438,
      "grad_norm": 0.0002506460004951805,
      "learning_rate": 0.0001312859884836852,
      "loss": 0.0,
      "step": 1759
    },
    {
      "epoch": 0.5631549476041917,
      "grad_norm": 1.3763397932052612,
      "learning_rate": 0.00013119001919385795,
      "loss": 0.0117,
      "step": 1760
    },
    {
      "epoch": 0.5634749220062395,
      "grad_norm": 0.0005909897154197097,
      "learning_rate": 0.0001310940499040307,
      "loss": 0.0,
      "step": 1761
    },
    {
      "epoch": 0.5637948964082873,
      "grad_norm": 0.8474330306053162,
      "learning_rate": 0.00013099808061420345,
      "loss": 0.005,
      "step": 1762
    },
    {
      "epoch": 0.5641148708103352,
      "grad_norm": 0.033241137862205505,
      "learning_rate": 0.0001309021113243762,
      "loss": 0.0004,
      "step": 1763
    },
    {
      "epoch": 0.564434845212383,
      "grad_norm": 0.0003394332597963512,
      "learning_rate": 0.00013080614203454894,
      "loss": 0.0,
      "step": 1764
    },
    {
      "epoch": 0.5647548196144309,
      "grad_norm": 0.032507963478565216,
      "learning_rate": 0.00013071017274472169,
      "loss": 0.0003,
      "step": 1765
    },
    {
      "epoch": 0.5650747940164786,
      "grad_norm": 0.0016079717315733433,
      "learning_rate": 0.0001306142034548944,
      "loss": 0.0,
      "step": 1766
    },
    {
      "epoch": 0.5653947684185265,
      "grad_norm": 2.770850419998169,
      "learning_rate": 0.00013051823416506715,
      "loss": 0.1766,
      "step": 1767
    },
    {
      "epoch": 0.5657147428205743,
      "grad_norm": 0.0003440242144279182,
      "learning_rate": 0.00013042226487523993,
      "loss": 0.0,
      "step": 1768
    },
    {
      "epoch": 0.5660347172226222,
      "grad_norm": 0.03079724684357643,
      "learning_rate": 0.00013032629558541267,
      "loss": 0.0005,
      "step": 1769
    },
    {
      "epoch": 0.56635469162467,
      "grad_norm": 4.656531810760498,
      "learning_rate": 0.0001302303262955854,
      "loss": 0.0824,
      "step": 1770
    },
    {
      "epoch": 0.5666746660267179,
      "grad_norm": 0.00022544909734278917,
      "learning_rate": 0.00013013435700575814,
      "loss": 0.0,
      "step": 1771
    },
    {
      "epoch": 0.5669946404287657,
      "grad_norm": 0.0008079635445028543,
      "learning_rate": 0.00013003838771593088,
      "loss": 0.0,
      "step": 1772
    },
    {
      "epoch": 0.5673146148308136,
      "grad_norm": 0.00031159279751591384,
      "learning_rate": 0.00012994241842610363,
      "loss": 0.0,
      "step": 1773
    },
    {
      "epoch": 0.5676345892328614,
      "grad_norm": 0.0506385937333107,
      "learning_rate": 0.00012984644913627638,
      "loss": 0.0004,
      "step": 1774
    },
    {
      "epoch": 0.5679545636349093,
      "grad_norm": 0.16896069049835205,
      "learning_rate": 0.00012975047984644912,
      "loss": 0.0016,
      "step": 1775
    },
    {
      "epoch": 0.568274538036957,
      "grad_norm": 6.453093528747559,
      "learning_rate": 0.00012965451055662187,
      "loss": 0.0552,
      "step": 1776
    },
    {
      "epoch": 0.5685945124390048,
      "grad_norm": 1.9914430379867554,
      "learning_rate": 0.00012955854126679462,
      "loss": 0.046,
      "step": 1777
    },
    {
      "epoch": 0.5689144868410527,
      "grad_norm": 0.0002182815660489723,
      "learning_rate": 0.00012946257197696736,
      "loss": 0.0,
      "step": 1778
    },
    {
      "epoch": 0.5692344612431005,
      "grad_norm": 0.000283414323348552,
      "learning_rate": 0.00012936660268714008,
      "loss": 0.0,
      "step": 1779
    },
    {
      "epoch": 0.5695544356451484,
      "grad_norm": 0.06313401460647583,
      "learning_rate": 0.00012927063339731286,
      "loss": 0.0009,
      "step": 1780
    },
    {
      "epoch": 0.5698744100471962,
      "grad_norm": 1.0928484201431274,
      "learning_rate": 0.0001291746641074856,
      "loss": 0.0433,
      "step": 1781
    },
    {
      "epoch": 0.5701943844492441,
      "grad_norm": 0.0002756620815489441,
      "learning_rate": 0.00012907869481765835,
      "loss": 0.0,
      "step": 1782
    },
    {
      "epoch": 0.5705143588512919,
      "grad_norm": 0.0013244114816188812,
      "learning_rate": 0.00012898272552783107,
      "loss": 0.0,
      "step": 1783
    },
    {
      "epoch": 0.5708343332533398,
      "grad_norm": 1.4057763814926147,
      "learning_rate": 0.00012888675623800382,
      "loss": 0.0063,
      "step": 1784
    },
    {
      "epoch": 0.5711543076553875,
      "grad_norm": 0.1331632435321808,
      "learning_rate": 0.00012879078694817656,
      "loss": 0.0026,
      "step": 1785
    },
    {
      "epoch": 0.5714742820574354,
      "grad_norm": 0.00446115480735898,
      "learning_rate": 0.00012869481765834934,
      "loss": 0.0001,
      "step": 1786
    },
    {
      "epoch": 0.5717942564594832,
      "grad_norm": 0.13408522307872772,
      "learning_rate": 0.00012859884836852206,
      "loss": 0.0017,
      "step": 1787
    },
    {
      "epoch": 0.5721142308615311,
      "grad_norm": 0.7654765844345093,
      "learning_rate": 0.0001285028790786948,
      "loss": 0.0065,
      "step": 1788
    },
    {
      "epoch": 0.5724342052635789,
      "grad_norm": 0.01169403363019228,
      "learning_rate": 0.00012840690978886755,
      "loss": 0.0002,
      "step": 1789
    },
    {
      "epoch": 0.5727541796656267,
      "grad_norm": 0.00027009763289242983,
      "learning_rate": 0.0001283109404990403,
      "loss": 0.0,
      "step": 1790
    },
    {
      "epoch": 0.5730741540676746,
      "grad_norm": 0.0007853878196328878,
      "learning_rate": 0.00012821497120921304,
      "loss": 0.0,
      "step": 1791
    },
    {
      "epoch": 0.5733941284697224,
      "grad_norm": 0.00026294676354154944,
      "learning_rate": 0.0001281190019193858,
      "loss": 0.0,
      "step": 1792
    },
    {
      "epoch": 0.5737141028717703,
      "grad_norm": 0.00028945875237695873,
      "learning_rate": 0.00012802303262955854,
      "loss": 0.0,
      "step": 1793
    },
    {
      "epoch": 0.574034077273818,
      "grad_norm": 0.010525941848754883,
      "learning_rate": 0.00012792706333973128,
      "loss": 0.0001,
      "step": 1794
    },
    {
      "epoch": 0.574354051675866,
      "grad_norm": 0.006398825906217098,
      "learning_rate": 0.00012783109404990403,
      "loss": 0.0001,
      "step": 1795
    },
    {
      "epoch": 0.5746740260779137,
      "grad_norm": 0.008861334063112736,
      "learning_rate": 0.00012773512476007675,
      "loss": 0.0001,
      "step": 1796
    },
    {
      "epoch": 0.5749940004799616,
      "grad_norm": 0.14912405610084534,
      "learning_rate": 0.0001276391554702495,
      "loss": 0.001,
      "step": 1797
    },
    {
      "epoch": 0.5753139748820094,
      "grad_norm": 0.0004052469739690423,
      "learning_rate": 0.00012754318618042224,
      "loss": 0.0,
      "step": 1798
    },
    {
      "epoch": 0.5756339492840573,
      "grad_norm": 0.0018462319858372211,
      "learning_rate": 0.00012744721689059502,
      "loss": 0.0001,
      "step": 1799
    },
    {
      "epoch": 0.5759539236861051,
      "grad_norm": 0.000329028902342543,
      "learning_rate": 0.00012735124760076774,
      "loss": 0.0,
      "step": 1800
    },
    {
      "epoch": 0.17067045723762142,
      "grad_norm": 14.396268844604492,
      "learning_rate": 0.00012725527831094048,
      "loss": 3.011,
      "step": 1801
    },
    {
      "epoch": 0.17076522151149018,
      "grad_norm": 20.681581497192383,
      "learning_rate": 0.00024880128873306165,
      "loss": 3.9588,
      "step": 1802
    },
    {
      "epoch": 0.1708599857853589,
      "grad_norm": 19.0582275390625,
      "learning_rate": 0.00024877286079787734,
      "loss": 1.5824,
      "step": 1803
    },
    {
      "epoch": 0.17095475005922767,
      "grad_norm": 9.185111999511719,
      "learning_rate": 0.000248744432862693,
      "loss": 1.562,
      "step": 1804
    },
    {
      "epoch": 0.17104951433309643,
      "grad_norm": 10.08692455291748,
      "learning_rate": 0.0002487160049275087,
      "loss": 0.8832,
      "step": 1805
    },
    {
      "epoch": 0.17114427860696518,
      "grad_norm": 6.596438407897949,
      "learning_rate": 0.00024868757699232446,
      "loss": 1.6347,
      "step": 1806
    },
    {
      "epoch": 0.1712390428808339,
      "grad_norm": 5.371668338775635,
      "learning_rate": 0.00024865914905714014,
      "loss": 1.8107,
      "step": 1807
    },
    {
      "epoch": 0.17133380715470267,
      "grad_norm": 3.0702571868896484,
      "learning_rate": 0.00024863072112195583,
      "loss": 1.119,
      "step": 1808
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 2.9739644527435303,
      "learning_rate": 0.0002486022931867715,
      "loss": 0.6,
      "step": 1809
    },
    {
      "epoch": 0.1715233357024402,
      "grad_norm": 3.2341840267181396,
      "learning_rate": 0.0002485738652515872,
      "loss": 0.5952,
      "step": 1810
    },
    {
      "epoch": 0.17161809997630892,
      "grad_norm": 2.327609062194824,
      "learning_rate": 0.0002485454373164029,
      "loss": 0.6035,
      "step": 1811
    },
    {
      "epoch": 0.17171286425017768,
      "grad_norm": 15.659128189086914,
      "learning_rate": 0.0002485170093812186,
      "loss": 0.7906,
      "step": 1812
    },
    {
      "epoch": 0.17180762852404644,
      "grad_norm": 2.837716579437256,
      "learning_rate": 0.0002484885814460343,
      "loss": 0.6136,
      "step": 1813
    },
    {
      "epoch": 0.1719023927979152,
      "grad_norm": 1.2581077814102173,
      "learning_rate": 0.00024846015351084996,
      "loss": 0.3561,
      "step": 1814
    },
    {
      "epoch": 0.17199715707178392,
      "grad_norm": 1.2128487825393677,
      "learning_rate": 0.00024843172557566565,
      "loss": 0.5804,
      "step": 1815
    },
    {
      "epoch": 0.17209192134565268,
      "grad_norm": 3.7284348011016846,
      "learning_rate": 0.00024840329764048134,
      "loss": 0.6351,
      "step": 1816
    },
    {
      "epoch": 0.17218668561952144,
      "grad_norm": 5.551874160766602,
      "learning_rate": 0.00024837486970529703,
      "loss": 0.8628,
      "step": 1817
    },
    {
      "epoch": 0.1722814498933902,
      "grad_norm": 7.03966760635376,
      "learning_rate": 0.0002483464417701127,
      "loss": 0.588,
      "step": 1818
    },
    {
      "epoch": 0.17237621416725896,
      "grad_norm": 1.4379981756210327,
      "learning_rate": 0.0002483180138349284,
      "loss": 0.7078,
      "step": 1819
    },
    {
      "epoch": 0.1724709784411277,
      "grad_norm": 1.9608396291732788,
      "learning_rate": 0.00024828958589974415,
      "loss": 0.6674,
      "step": 1820
    },
    {
      "epoch": 0.17256574271499645,
      "grad_norm": 3.20621657371521,
      "learning_rate": 0.00024826115796455984,
      "loss": 0.4816,
      "step": 1821
    },
    {
      "epoch": 0.1726605069888652,
      "grad_norm": 13.294296264648438,
      "learning_rate": 0.0002482327300293755,
      "loss": 1.1096,
      "step": 1822
    },
    {
      "epoch": 0.17275527126273396,
      "grad_norm": 1.6514341831207275,
      "learning_rate": 0.0002482043020941912,
      "loss": 0.6909,
      "step": 1823
    },
    {
      "epoch": 0.1728500355366027,
      "grad_norm": 3.615298271179199,
      "learning_rate": 0.0002481758741590069,
      "loss": 0.8789,
      "step": 1824
    },
    {
      "epoch": 0.17294479981047145,
      "grad_norm": 2.236722469329834,
      "learning_rate": 0.0002481474462238226,
      "loss": 0.3761,
      "step": 1825
    },
    {
      "epoch": 0.1730395640843402,
      "grad_norm": 2.2017602920532227,
      "learning_rate": 0.0002481190182886383,
      "loss": 0.3538,
      "step": 1826
    },
    {
      "epoch": 0.17313432835820897,
      "grad_norm": 6.892513751983643,
      "learning_rate": 0.00024809059035345397,
      "loss": 0.3499,
      "step": 1827
    },
    {
      "epoch": 0.1732290926320777,
      "grad_norm": 2.5903563499450684,
      "learning_rate": 0.00024806216241826965,
      "loss": 0.4649,
      "step": 1828
    },
    {
      "epoch": 0.17332385690594646,
      "grad_norm": 1.7109181880950928,
      "learning_rate": 0.00024803373448308534,
      "loss": 0.6006,
      "step": 1829
    },
    {
      "epoch": 0.1734186211798152,
      "grad_norm": 2.580731153488159,
      "learning_rate": 0.00024800530654790103,
      "loss": 0.8567,
      "step": 1830
    },
    {
      "epoch": 0.17351338545368397,
      "grad_norm": 2.065757989883423,
      "learning_rate": 0.0002479768786127167,
      "loss": 0.3333,
      "step": 1831
    },
    {
      "epoch": 0.1736081497275527,
      "grad_norm": 3.1779212951660156,
      "learning_rate": 0.0002479484506775324,
      "loss": 0.5554,
      "step": 1832
    },
    {
      "epoch": 0.17370291400142146,
      "grad_norm": 3.44520902633667,
      "learning_rate": 0.00024792002274234815,
      "loss": 0.5491,
      "step": 1833
    },
    {
      "epoch": 0.17379767827529022,
      "grad_norm": 7.103194236755371,
      "learning_rate": 0.00024789159480716384,
      "loss": 0.433,
      "step": 1834
    },
    {
      "epoch": 0.17389244254915898,
      "grad_norm": 5.56456184387207,
      "learning_rate": 0.0002478631668719795,
      "loss": 0.5251,
      "step": 1835
    },
    {
      "epoch": 0.1739872068230277,
      "grad_norm": 8.09730339050293,
      "learning_rate": 0.0002478347389367952,
      "loss": 0.8308,
      "step": 1836
    },
    {
      "epoch": 0.17408197109689647,
      "grad_norm": 11.399048805236816,
      "learning_rate": 0.0002478063110016109,
      "loss": 0.4713,
      "step": 1837
    },
    {
      "epoch": 0.17417673537076522,
      "grad_norm": 5.6493916511535645,
      "learning_rate": 0.0002477778830664266,
      "loss": 0.4609,
      "step": 1838
    },
    {
      "epoch": 0.17427149964463398,
      "grad_norm": 7.244866847991943,
      "learning_rate": 0.0002477494551312423,
      "loss": 0.4033,
      "step": 1839
    },
    {
      "epoch": 0.1743662639185027,
      "grad_norm": 2.046393632888794,
      "learning_rate": 0.00024772102719605797,
      "loss": 0.2653,
      "step": 1840
    },
    {
      "epoch": 0.17446102819237147,
      "grad_norm": 1.9695407152175903,
      "learning_rate": 0.00024769259926087366,
      "loss": 0.272,
      "step": 1841
    },
    {
      "epoch": 0.17455579246624023,
      "grad_norm": 3.551815986633301,
      "learning_rate": 0.00024766417132568935,
      "loss": 0.1893,
      "step": 1842
    },
    {
      "epoch": 0.174650556740109,
      "grad_norm": 1.0299172401428223,
      "learning_rate": 0.00024763574339050503,
      "loss": 0.1631,
      "step": 1843
    },
    {
      "epoch": 0.17474532101397772,
      "grad_norm": 2.058551073074341,
      "learning_rate": 0.0002476073154553207,
      "loss": 0.1919,
      "step": 1844
    },
    {
      "epoch": 0.17484008528784648,
      "grad_norm": 2.226614475250244,
      "learning_rate": 0.0002475788875201364,
      "loss": 0.107,
      "step": 1845
    },
    {
      "epoch": 0.17493484956171523,
      "grad_norm": 4.86137580871582,
      "learning_rate": 0.0002475504595849521,
      "loss": 0.1349,
      "step": 1846
    },
    {
      "epoch": 0.175029613835584,
      "grad_norm": 2.714290142059326,
      "learning_rate": 0.00024752203164976784,
      "loss": 0.5637,
      "step": 1847
    },
    {
      "epoch": 0.17512437810945275,
      "grad_norm": 2.0581328868865967,
      "learning_rate": 0.00024749360371458353,
      "loss": 0.2902,
      "step": 1848
    },
    {
      "epoch": 0.17521914238332148,
      "grad_norm": 0.41836073994636536,
      "learning_rate": 0.0002474651757793992,
      "loss": 0.0267,
      "step": 1849
    },
    {
      "epoch": 0.17531390665719024,
      "grad_norm": 1.5923798084259033,
      "learning_rate": 0.0002474367478442149,
      "loss": 0.1941,
      "step": 1850
    },
    {
      "epoch": 0.175408670931059,
      "grad_norm": 1.5815764665603638,
      "learning_rate": 0.0002474083199090306,
      "loss": 0.2829,
      "step": 1851
    },
    {
      "epoch": 0.17550343520492775,
      "grad_norm": 1.3651779890060425,
      "learning_rate": 0.0002473798919738463,
      "loss": 0.123,
      "step": 1852
    },
    {
      "epoch": 0.17559819947879649,
      "grad_norm": 0.9363512992858887,
      "learning_rate": 0.00024735146403866197,
      "loss": 0.1406,
      "step": 1853
    },
    {
      "epoch": 0.17569296375266524,
      "grad_norm": 6.170863151550293,
      "learning_rate": 0.00024732303610347766,
      "loss": 0.63,
      "step": 1854
    },
    {
      "epoch": 0.175787728026534,
      "grad_norm": 2.919677257537842,
      "learning_rate": 0.00024729460816829335,
      "loss": 0.2528,
      "step": 1855
    },
    {
      "epoch": 0.17588249230040276,
      "grad_norm": 4.371783256530762,
      "learning_rate": 0.00024726618023310904,
      "loss": 0.3598,
      "step": 1856
    },
    {
      "epoch": 0.1759772565742715,
      "grad_norm": 2.023155450820923,
      "learning_rate": 0.0002472377522979247,
      "loss": 0.2449,
      "step": 1857
    },
    {
      "epoch": 0.17607202084814025,
      "grad_norm": 3.851038932800293,
      "learning_rate": 0.0002472093243627404,
      "loss": 0.1118,
      "step": 1858
    },
    {
      "epoch": 0.176166785122009,
      "grad_norm": 3.6585447788238525,
      "learning_rate": 0.0002471808964275561,
      "loss": 0.1499,
      "step": 1859
    },
    {
      "epoch": 0.17626154939587776,
      "grad_norm": 0.8170350790023804,
      "learning_rate": 0.0002471524684923718,
      "loss": 0.0715,
      "step": 1860
    },
    {
      "epoch": 0.1763563136697465,
      "grad_norm": 2.5178143978118896,
      "learning_rate": 0.00024712404055718753,
      "loss": 0.1072,
      "step": 1861
    },
    {
      "epoch": 0.17645107794361525,
      "grad_norm": 1.6237351894378662,
      "learning_rate": 0.0002470956126220032,
      "loss": 0.1435,
      "step": 1862
    },
    {
      "epoch": 0.176545842217484,
      "grad_norm": 9.5008544921875,
      "learning_rate": 0.0002470671846868189,
      "loss": 0.2784,
      "step": 1863
    },
    {
      "epoch": 0.17664060649135277,
      "grad_norm": 8.675446510314941,
      "learning_rate": 0.0002470387567516346,
      "loss": 0.4393,
      "step": 1864
    },
    {
      "epoch": 0.1767353707652215,
      "grad_norm": 1.7574023008346558,
      "learning_rate": 0.0002470103288164503,
      "loss": 0.0465,
      "step": 1865
    },
    {
      "epoch": 0.17683013503909026,
      "grad_norm": 6.427269458770752,
      "learning_rate": 0.000246981900881266,
      "loss": 0.3522,
      "step": 1866
    },
    {
      "epoch": 0.17692489931295902,
      "grad_norm": 3.903805732727051,
      "learning_rate": 0.00024695347294608166,
      "loss": 0.1007,
      "step": 1867
    },
    {
      "epoch": 0.17701966358682777,
      "grad_norm": 1.1469720602035522,
      "learning_rate": 0.00024692504501089735,
      "loss": 0.0915,
      "step": 1868
    },
    {
      "epoch": 0.1771144278606965,
      "grad_norm": 1.2260407209396362,
      "learning_rate": 0.00024689661707571304,
      "loss": 0.0872,
      "step": 1869
    },
    {
      "epoch": 0.17720919213456526,
      "grad_norm": 4.457704544067383,
      "learning_rate": 0.00024686818914052873,
      "loss": 0.1498,
      "step": 1870
    },
    {
      "epoch": 0.17730395640843402,
      "grad_norm": 4.778458595275879,
      "learning_rate": 0.0002468397612053444,
      "loss": 0.2913,
      "step": 1871
    },
    {
      "epoch": 0.17739872068230278,
      "grad_norm": 2.5587098598480225,
      "learning_rate": 0.0002468113332701601,
      "loss": 0.1323,
      "step": 1872
    },
    {
      "epoch": 0.1774934849561715,
      "grad_norm": 5.129818439483643,
      "learning_rate": 0.0002467829053349758,
      "loss": 0.1702,
      "step": 1873
    },
    {
      "epoch": 0.17758824923004027,
      "grad_norm": 1.1018590927124023,
      "learning_rate": 0.0002467544773997915,
      "loss": 0.0676,
      "step": 1874
    },
    {
      "epoch": 0.17768301350390903,
      "grad_norm": 4.504040718078613,
      "learning_rate": 0.0002467260494646072,
      "loss": 0.3497,
      "step": 1875
    },
    {
      "epoch": 0.17777777777777778,
      "grad_norm": 3.1604950428009033,
      "learning_rate": 0.0002466976215294229,
      "loss": 0.1612,
      "step": 1876
    },
    {
      "epoch": 0.17787254205164654,
      "grad_norm": 5.850978374481201,
      "learning_rate": 0.0002466691935942386,
      "loss": 0.3548,
      "step": 1877
    },
    {
      "epoch": 0.17796730632551527,
      "grad_norm": 2.1647260189056396,
      "learning_rate": 0.0002466407656590543,
      "loss": 0.1386,
      "step": 1878
    },
    {
      "epoch": 0.17806207059938403,
      "grad_norm": 3.6834006309509277,
      "learning_rate": 0.00024661233772387,
      "loss": 0.1171,
      "step": 1879
    },
    {
      "epoch": 0.1781568348732528,
      "grad_norm": 1.6962182521820068,
      "learning_rate": 0.00024658390978868567,
      "loss": 0.175,
      "step": 1880
    },
    {
      "epoch": 0.17825159914712155,
      "grad_norm": 4.227386951446533,
      "learning_rate": 0.00024655548185350135,
      "loss": 0.1043,
      "step": 1881
    },
    {
      "epoch": 0.17834636342099028,
      "grad_norm": 8.529154777526855,
      "learning_rate": 0.00024652705391831704,
      "loss": 0.2803,
      "step": 1882
    },
    {
      "epoch": 0.17844112769485904,
      "grad_norm": 6.779361724853516,
      "learning_rate": 0.00024649862598313273,
      "loss": 0.3158,
      "step": 1883
    },
    {
      "epoch": 0.1785358919687278,
      "grad_norm": 2.2907426357269287,
      "learning_rate": 0.0002464701980479484,
      "loss": 0.1042,
      "step": 1884
    },
    {
      "epoch": 0.17863065624259655,
      "grad_norm": 4.767274856567383,
      "learning_rate": 0.0002464417701127641,
      "loss": 0.2081,
      "step": 1885
    },
    {
      "epoch": 0.17872542051646528,
      "grad_norm": 1.403396487236023,
      "learning_rate": 0.0002464133421775798,
      "loss": 0.1291,
      "step": 1886
    },
    {
      "epoch": 0.17882018479033404,
      "grad_norm": 7.407222270965576,
      "learning_rate": 0.0002463849142423955,
      "loss": 0.4648,
      "step": 1887
    },
    {
      "epoch": 0.1789149490642028,
      "grad_norm": 5.206277370452881,
      "learning_rate": 0.00024635648630721117,
      "loss": 0.2088,
      "step": 1888
    },
    {
      "epoch": 0.17900971333807156,
      "grad_norm": 14.376751899719238,
      "learning_rate": 0.0002463280583720269,
      "loss": 0.3792,
      "step": 1889
    },
    {
      "epoch": 0.1791044776119403,
      "grad_norm": 0.7845385670661926,
      "learning_rate": 0.0002462996304368426,
      "loss": 0.0829,
      "step": 1890
    },
    {
      "epoch": 0.17919924188580905,
      "grad_norm": 2.7926814556121826,
      "learning_rate": 0.0002462712025016583,
      "loss": 0.1516,
      "step": 1891
    },
    {
      "epoch": 0.1792940061596778,
      "grad_norm": 2.823728561401367,
      "learning_rate": 0.000246242774566474,
      "loss": 0.2025,
      "step": 1892
    },
    {
      "epoch": 0.17938877043354656,
      "grad_norm": 8.105494499206543,
      "learning_rate": 0.00024621434663128967,
      "loss": 0.1749,
      "step": 1893
    },
    {
      "epoch": 0.1794835347074153,
      "grad_norm": 1.9601823091506958,
      "learning_rate": 0.00024618591869610536,
      "loss": 0.1456,
      "step": 1894
    },
    {
      "epoch": 0.17957829898128405,
      "grad_norm": 1.4629584550857544,
      "learning_rate": 0.00024615749076092105,
      "loss": 0.0906,
      "step": 1895
    },
    {
      "epoch": 0.1796730632551528,
      "grad_norm": 10.510485649108887,
      "learning_rate": 0.00024612906282573673,
      "loss": 0.1352,
      "step": 1896
    },
    {
      "epoch": 0.17976782752902157,
      "grad_norm": 2.8952231407165527,
      "learning_rate": 0.0002461006348905524,
      "loss": 0.1538,
      "step": 1897
    },
    {
      "epoch": 0.1798625918028903,
      "grad_norm": 3.3004794120788574,
      "learning_rate": 0.0002460722069553681,
      "loss": 0.1555,
      "step": 1898
    },
    {
      "epoch": 0.17995735607675906,
      "grad_norm": 3.4710047245025635,
      "learning_rate": 0.0002460437790201838,
      "loss": 0.1384,
      "step": 1899
    },
    {
      "epoch": 0.18005212035062781,
      "grad_norm": 7.427880764007568,
      "learning_rate": 0.0002460153510849995,
      "loss": 0.2893,
      "step": 1900
    },
    {
      "epoch": 0.18014688462449657,
      "grad_norm": 6.95758056640625,
      "learning_rate": 0.0002459869231498152,
      "loss": 0.1949,
      "step": 1901
    },
    {
      "epoch": 0.1802416488983653,
      "grad_norm": 2.5763626098632812,
      "learning_rate": 0.00024595849521463086,
      "loss": 0.1302,
      "step": 1902
    },
    {
      "epoch": 0.18033641317223406,
      "grad_norm": 2.3613803386688232,
      "learning_rate": 0.0002459300672794466,
      "loss": 0.1491,
      "step": 1903
    },
    {
      "epoch": 0.18043117744610282,
      "grad_norm": 4.541196346282959,
      "learning_rate": 0.0002459016393442623,
      "loss": 0.2004,
      "step": 1904
    },
    {
      "epoch": 0.18052594171997158,
      "grad_norm": 1.3785854578018188,
      "learning_rate": 0.000245873211409078,
      "loss": 0.0402,
      "step": 1905
    },
    {
      "epoch": 0.18062070599384034,
      "grad_norm": 3.36641001701355,
      "learning_rate": 0.00024584478347389367,
      "loss": 0.123,
      "step": 1906
    },
    {
      "epoch": 0.18071547026770907,
      "grad_norm": 5.386262893676758,
      "learning_rate": 0.00024581635553870936,
      "loss": 0.2045,
      "step": 1907
    },
    {
      "epoch": 0.18081023454157782,
      "grad_norm": 4.663823127746582,
      "learning_rate": 0.00024578792760352505,
      "loss": 0.2438,
      "step": 1908
    },
    {
      "epoch": 0.18090499881544658,
      "grad_norm": 3.0804076194763184,
      "learning_rate": 0.00024575949966834074,
      "loss": 0.088,
      "step": 1909
    },
    {
      "epoch": 0.18099976308931534,
      "grad_norm": 3.231934070587158,
      "learning_rate": 0.0002457310717331564,
      "loss": 0.2001,
      "step": 1910
    },
    {
      "epoch": 0.18109452736318407,
      "grad_norm": 7.351112365722656,
      "learning_rate": 0.0002457026437979721,
      "loss": 0.3349,
      "step": 1911
    },
    {
      "epoch": 0.18118929163705283,
      "grad_norm": 1.2794517278671265,
      "learning_rate": 0.0002456742158627878,
      "loss": 0.0867,
      "step": 1912
    },
    {
      "epoch": 0.1812840559109216,
      "grad_norm": 4.114756107330322,
      "learning_rate": 0.0002456457879276035,
      "loss": 0.2432,
      "step": 1913
    },
    {
      "epoch": 0.18137882018479035,
      "grad_norm": 7.568336009979248,
      "learning_rate": 0.0002456173599924192,
      "loss": 0.1782,
      "step": 1914
    },
    {
      "epoch": 0.18147358445865908,
      "grad_norm": 4.625567436218262,
      "learning_rate": 0.00024558893205723487,
      "loss": 0.1599,
      "step": 1915
    },
    {
      "epoch": 0.18156834873252783,
      "grad_norm": 2.828217029571533,
      "learning_rate": 0.0002455605041220506,
      "loss": 0.1168,
      "step": 1916
    },
    {
      "epoch": 0.1816631130063966,
      "grad_norm": 1.8201031684875488,
      "learning_rate": 0.0002455320761868663,
      "loss": 0.1253,
      "step": 1917
    },
    {
      "epoch": 0.18175787728026535,
      "grad_norm": 2.9209678173065186,
      "learning_rate": 0.000245503648251682,
      "loss": 0.2559,
      "step": 1918
    },
    {
      "epoch": 0.18185264155413408,
      "grad_norm": 7.0372138023376465,
      "learning_rate": 0.0002454752203164977,
      "loss": 0.5741,
      "step": 1919
    },
    {
      "epoch": 0.18194740582800284,
      "grad_norm": 6.7423882484436035,
      "learning_rate": 0.00024544679238131336,
      "loss": 0.2762,
      "step": 1920
    },
    {
      "epoch": 0.1820421701018716,
      "grad_norm": 6.158392906188965,
      "learning_rate": 0.00024541836444612905,
      "loss": 0.2394,
      "step": 1921
    },
    {
      "epoch": 0.18213693437574036,
      "grad_norm": 2.3992760181427,
      "learning_rate": 0.00024538993651094474,
      "loss": 0.1141,
      "step": 1922
    },
    {
      "epoch": 0.1822316986496091,
      "grad_norm": 4.596920013427734,
      "learning_rate": 0.00024536150857576043,
      "loss": 0.4077,
      "step": 1923
    },
    {
      "epoch": 0.18232646292347784,
      "grad_norm": 9.737066268920898,
      "learning_rate": 0.0002453330806405761,
      "loss": 0.3415,
      "step": 1924
    },
    {
      "epoch": 0.1824212271973466,
      "grad_norm": 2.630352258682251,
      "learning_rate": 0.0002453046527053918,
      "loss": 0.1591,
      "step": 1925
    },
    {
      "epoch": 0.18251599147121536,
      "grad_norm": 4.387158393859863,
      "learning_rate": 0.0002452762247702075,
      "loss": 0.1854,
      "step": 1926
    },
    {
      "epoch": 0.1826107557450841,
      "grad_norm": 1.2478712797164917,
      "learning_rate": 0.0002452477968350232,
      "loss": 0.0907,
      "step": 1927
    },
    {
      "epoch": 0.18270552001895285,
      "grad_norm": 2.6121628284454346,
      "learning_rate": 0.00024521936889983887,
      "loss": 0.1094,
      "step": 1928
    },
    {
      "epoch": 0.1828002842928216,
      "grad_norm": 6.043979167938232,
      "learning_rate": 0.00024519094096465456,
      "loss": 0.3923,
      "step": 1929
    },
    {
      "epoch": 0.18289504856669037,
      "grad_norm": 2.281500816345215,
      "learning_rate": 0.0002451625130294703,
      "loss": 0.1211,
      "step": 1930
    },
    {
      "epoch": 0.1829898128405591,
      "grad_norm": 4.6634416580200195,
      "learning_rate": 0.000245134085094286,
      "loss": 0.0665,
      "step": 1931
    },
    {
      "epoch": 0.18308457711442785,
      "grad_norm": 1.6474153995513916,
      "learning_rate": 0.0002451056571591017,
      "loss": 0.0977,
      "step": 1932
    },
    {
      "epoch": 0.1831793413882966,
      "grad_norm": 8.077548027038574,
      "learning_rate": 0.00024507722922391737,
      "loss": 0.1861,
      "step": 1933
    },
    {
      "epoch": 0.18327410566216537,
      "grad_norm": 2.6433796882629395,
      "learning_rate": 0.00024504880128873305,
      "loss": 0.149,
      "step": 1934
    },
    {
      "epoch": 0.18336886993603413,
      "grad_norm": 4.020512104034424,
      "learning_rate": 0.00024502037335354874,
      "loss": 0.1453,
      "step": 1935
    },
    {
      "epoch": 0.18346363420990286,
      "grad_norm": 0.6317743062973022,
      "learning_rate": 0.00024499194541836443,
      "loss": 0.0263,
      "step": 1936
    },
    {
      "epoch": 0.18355839848377162,
      "grad_norm": 1.0435024499893188,
      "learning_rate": 0.0002449635174831801,
      "loss": 0.0946,
      "step": 1937
    },
    {
      "epoch": 0.18365316275764038,
      "grad_norm": 6.7400946617126465,
      "learning_rate": 0.0002449350895479958,
      "loss": 0.6024,
      "step": 1938
    },
    {
      "epoch": 0.18374792703150913,
      "grad_norm": 3.520089864730835,
      "learning_rate": 0.0002449066616128115,
      "loss": 0.2612,
      "step": 1939
    },
    {
      "epoch": 0.18384269130537786,
      "grad_norm": 2.4548540115356445,
      "learning_rate": 0.0002448782336776272,
      "loss": 0.2055,
      "step": 1940
    },
    {
      "epoch": 0.18393745557924662,
      "grad_norm": 1.7812738418579102,
      "learning_rate": 0.00024484980574244287,
      "loss": 0.0839,
      "step": 1941
    },
    {
      "epoch": 0.18403221985311538,
      "grad_norm": 13.217529296875,
      "learning_rate": 0.00024482137780725856,
      "loss": 0.2289,
      "step": 1942
    },
    {
      "epoch": 0.18412698412698414,
      "grad_norm": 4.658334732055664,
      "learning_rate": 0.00024479294987207425,
      "loss": 0.1919,
      "step": 1943
    },
    {
      "epoch": 0.18422174840085287,
      "grad_norm": 0.9784828424453735,
      "learning_rate": 0.00024476452193689,
      "loss": 0.053,
      "step": 1944
    },
    {
      "epoch": 0.18431651267472163,
      "grad_norm": 4.521825313568115,
      "learning_rate": 0.0002447360940017057,
      "loss": 0.3526,
      "step": 1945
    },
    {
      "epoch": 0.18441127694859039,
      "grad_norm": 3.0105645656585693,
      "learning_rate": 0.00024470766606652137,
      "loss": 0.1149,
      "step": 1946
    },
    {
      "epoch": 0.18450604122245914,
      "grad_norm": 1.6714709997177124,
      "learning_rate": 0.00024467923813133706,
      "loss": 0.1381,
      "step": 1947
    },
    {
      "epoch": 0.18460080549632787,
      "grad_norm": 6.078942775726318,
      "learning_rate": 0.00024465081019615275,
      "loss": 0.1654,
      "step": 1948
    },
    {
      "epoch": 0.18469556977019663,
      "grad_norm": 5.009575843811035,
      "learning_rate": 0.00024462238226096843,
      "loss": 0.1758,
      "step": 1949
    },
    {
      "epoch": 0.1847903340440654,
      "grad_norm": 2.8257107734680176,
      "learning_rate": 0.0002445939543257841,
      "loss": 0.2074,
      "step": 1950
    },
    {
      "epoch": 0.18488509831793415,
      "grad_norm": 1.2085832357406616,
      "learning_rate": 0.0002445655263905998,
      "loss": 0.1117,
      "step": 1951
    },
    {
      "epoch": 0.18497986259180288,
      "grad_norm": 5.033059120178223,
      "learning_rate": 0.0002445370984554155,
      "loss": 0.2308,
      "step": 1952
    },
    {
      "epoch": 0.18507462686567164,
      "grad_norm": 5.040371894836426,
      "learning_rate": 0.0002445086705202312,
      "loss": 0.1409,
      "step": 1953
    },
    {
      "epoch": 0.1851693911395404,
      "grad_norm": 4.679244041442871,
      "learning_rate": 0.0002444802425850469,
      "loss": 0.1576,
      "step": 1954
    },
    {
      "epoch": 0.18526415541340915,
      "grad_norm": 1.3767057657241821,
      "learning_rate": 0.00024445181464986256,
      "loss": 0.0661,
      "step": 1955
    },
    {
      "epoch": 0.18535891968727788,
      "grad_norm": 2.767559766769409,
      "learning_rate": 0.00024442338671467825,
      "loss": 0.1447,
      "step": 1956
    },
    {
      "epoch": 0.18545368396114664,
      "grad_norm": 4.719747543334961,
      "learning_rate": 0.00024439495877949394,
      "loss": 0.1295,
      "step": 1957
    },
    {
      "epoch": 0.1855484482350154,
      "grad_norm": 2.7330780029296875,
      "learning_rate": 0.0002443665308443097,
      "loss": 0.1122,
      "step": 1958
    },
    {
      "epoch": 0.18564321250888416,
      "grad_norm": 4.220823287963867,
      "learning_rate": 0.00024433810290912537,
      "loss": 0.1195,
      "step": 1959
    },
    {
      "epoch": 0.1857379767827529,
      "grad_norm": 9.43278694152832,
      "learning_rate": 0.00024430967497394106,
      "loss": 0.1888,
      "step": 1960
    },
    {
      "epoch": 0.18583274105662165,
      "grad_norm": 4.32876443862915,
      "learning_rate": 0.00024428124703875675,
      "loss": 0.2427,
      "step": 1961
    },
    {
      "epoch": 0.1859275053304904,
      "grad_norm": 2.2011444568634033,
      "learning_rate": 0.0002442528191035724,
      "loss": 0.1075,
      "step": 1962
    },
    {
      "epoch": 0.18602226960435916,
      "grad_norm": 5.238461971282959,
      "learning_rate": 0.0002442243911683881,
      "loss": 0.3666,
      "step": 1963
    },
    {
      "epoch": 0.1861170338782279,
      "grad_norm": 3.7538185119628906,
      "learning_rate": 0.0002441959632332038,
      "loss": 0.2661,
      "step": 1964
    },
    {
      "epoch": 0.18621179815209665,
      "grad_norm": 3.380436658859253,
      "learning_rate": 0.0002441675352980195,
      "loss": 0.2,
      "step": 1965
    },
    {
      "epoch": 0.1863065624259654,
      "grad_norm": 4.963143825531006,
      "learning_rate": 0.0002441391073628352,
      "loss": 0.1478,
      "step": 1966
    },
    {
      "epoch": 0.18640132669983417,
      "grad_norm": 6.224724292755127,
      "learning_rate": 0.00024411067942765088,
      "loss": 0.0691,
      "step": 1967
    },
    {
      "epoch": 0.18649609097370293,
      "grad_norm": 2.354278802871704,
      "learning_rate": 0.00024408225149246657,
      "loss": 0.2364,
      "step": 1968
    },
    {
      "epoch": 0.18659085524757166,
      "grad_norm": 3.461369037628174,
      "learning_rate": 0.00024405382355728228,
      "loss": 0.3226,
      "step": 1969
    },
    {
      "epoch": 0.18668561952144042,
      "grad_norm": 2.2950494289398193,
      "learning_rate": 0.00024402539562209797,
      "loss": 0.1488,
      "step": 1970
    },
    {
      "epoch": 0.18678038379530917,
      "grad_norm": 3.6219375133514404,
      "learning_rate": 0.00024399696768691366,
      "loss": 0.2061,
      "step": 1971
    },
    {
      "epoch": 0.18687514806917793,
      "grad_norm": 1.8015155792236328,
      "learning_rate": 0.00024396853975172935,
      "loss": 0.1263,
      "step": 1972
    },
    {
      "epoch": 0.18696991234304666,
      "grad_norm": 0.8270602822303772,
      "learning_rate": 0.00024394011181654504,
      "loss": 0.0621,
      "step": 1973
    },
    {
      "epoch": 0.18706467661691542,
      "grad_norm": 3.0818984508514404,
      "learning_rate": 0.00024391168388136075,
      "loss": 0.1116,
      "step": 1974
    },
    {
      "epoch": 0.18715944089078418,
      "grad_norm": 15.92156982421875,
      "learning_rate": 0.00024388325594617644,
      "loss": 0.2164,
      "step": 1975
    },
    {
      "epoch": 0.18725420516465294,
      "grad_norm": 0.9987599849700928,
      "learning_rate": 0.0002438548280109921,
      "loss": 0.0998,
      "step": 1976
    },
    {
      "epoch": 0.18734896943852167,
      "grad_norm": 5.091609001159668,
      "learning_rate": 0.0002438264000758078,
      "loss": 0.1886,
      "step": 1977
    },
    {
      "epoch": 0.18744373371239043,
      "grad_norm": 1.6542603969573975,
      "learning_rate": 0.00024379797214062348,
      "loss": 0.1532,
      "step": 1978
    },
    {
      "epoch": 0.18753849798625918,
      "grad_norm": 1.8150253295898438,
      "learning_rate": 0.0002437695442054392,
      "loss": 0.168,
      "step": 1979
    },
    {
      "epoch": 0.18763326226012794,
      "grad_norm": 5.687280178070068,
      "learning_rate": 0.00024374111627025488,
      "loss": 0.4833,
      "step": 1980
    },
    {
      "epoch": 0.18772802653399667,
      "grad_norm": 1.6885911226272583,
      "learning_rate": 0.00024371268833507057,
      "loss": 0.1104,
      "step": 1981
    },
    {
      "epoch": 0.18782279080786543,
      "grad_norm": 2.3756816387176514,
      "learning_rate": 0.00024368426039988626,
      "loss": 0.2701,
      "step": 1982
    },
    {
      "epoch": 0.1879175550817342,
      "grad_norm": 2.917921304702759,
      "learning_rate": 0.00024365583246470197,
      "loss": 0.2262,
      "step": 1983
    },
    {
      "epoch": 0.18801231935560295,
      "grad_norm": 1.8663039207458496,
      "learning_rate": 0.00024362740452951766,
      "loss": 0.193,
      "step": 1984
    },
    {
      "epoch": 0.18810708362947168,
      "grad_norm": 3.7499799728393555,
      "learning_rate": 0.00024359897659433335,
      "loss": 0.2766,
      "step": 1985
    },
    {
      "epoch": 0.18820184790334044,
      "grad_norm": 2.1673953533172607,
      "learning_rate": 0.00024357054865914904,
      "loss": 0.117,
      "step": 1986
    },
    {
      "epoch": 0.1882966121772092,
      "grad_norm": 1.2572606801986694,
      "learning_rate": 0.00024354212072396473,
      "loss": 0.1652,
      "step": 1987
    },
    {
      "epoch": 0.18839137645107795,
      "grad_norm": 3.1309235095977783,
      "learning_rate": 0.00024351369278878044,
      "loss": 0.3253,
      "step": 1988
    },
    {
      "epoch": 0.18848614072494668,
      "grad_norm": 2.228250503540039,
      "learning_rate": 0.0002434852648535961,
      "loss": 0.2116,
      "step": 1989
    },
    {
      "epoch": 0.18858090499881544,
      "grad_norm": 0.7782718539237976,
      "learning_rate": 0.0002434568369184118,
      "loss": 0.0291,
      "step": 1990
    },
    {
      "epoch": 0.1886756692726842,
      "grad_norm": 4.460618495941162,
      "learning_rate": 0.00024342840898322748,
      "loss": 0.1273,
      "step": 1991
    },
    {
      "epoch": 0.18877043354655296,
      "grad_norm": 1.9372596740722656,
      "learning_rate": 0.00024339998104804317,
      "loss": 0.1805,
      "step": 1992
    },
    {
      "epoch": 0.1888651978204217,
      "grad_norm": 2.2900571823120117,
      "learning_rate": 0.00024337155311285888,
      "loss": 0.214,
      "step": 1993
    },
    {
      "epoch": 0.18895996209429045,
      "grad_norm": 9.657102584838867,
      "learning_rate": 0.00024334312517767457,
      "loss": 0.2459,
      "step": 1994
    },
    {
      "epoch": 0.1890547263681592,
      "grad_norm": 1.1512442827224731,
      "learning_rate": 0.00024331469724249026,
      "loss": 0.0655,
      "step": 1995
    },
    {
      "epoch": 0.18914949064202796,
      "grad_norm": 1.3857083320617676,
      "learning_rate": 0.00024328626930730595,
      "loss": 0.1396,
      "step": 1996
    },
    {
      "epoch": 0.18924425491589672,
      "grad_norm": 10.707776069641113,
      "learning_rate": 0.00024325784137212166,
      "loss": 0.2934,
      "step": 1997
    },
    {
      "epoch": 0.18933901918976545,
      "grad_norm": 1.8477126359939575,
      "learning_rate": 0.00024322941343693735,
      "loss": 0.129,
      "step": 1998
    },
    {
      "epoch": 0.1894337834636342,
      "grad_norm": 5.398036479949951,
      "learning_rate": 0.00024320098550175304,
      "loss": 0.298,
      "step": 1999
    },
    {
      "epoch": 0.18952854773750297,
      "grad_norm": 1.618761658668518,
      "learning_rate": 0.00024317255756656873,
      "loss": 0.1198,
      "step": 2000
    },
    {
      "epoch": 0.18962331201137173,
      "grad_norm": 12.167268753051758,
      "learning_rate": 0.00024314412963138445,
      "loss": 0.4855,
      "step": 2001
    },
    {
      "epoch": 0.18971807628524046,
      "grad_norm": 3.3929531574249268,
      "learning_rate": 0.00024311570169620013,
      "loss": 0.1941,
      "step": 2002
    },
    {
      "epoch": 0.1898128405591092,
      "grad_norm": 4.5758771896362305,
      "learning_rate": 0.0002430872737610158,
      "loss": 0.3476,
      "step": 2003
    },
    {
      "epoch": 0.18990760483297797,
      "grad_norm": 8.821910858154297,
      "learning_rate": 0.00024305884582583148,
      "loss": 0.431,
      "step": 2004
    },
    {
      "epoch": 0.19000236910684673,
      "grad_norm": 6.977352619171143,
      "learning_rate": 0.00024303041789064717,
      "loss": 0.2818,
      "step": 2005
    },
    {
      "epoch": 0.19009713338071546,
      "grad_norm": 1.7323713302612305,
      "learning_rate": 0.00024300198995546286,
      "loss": 0.2782,
      "step": 2006
    },
    {
      "epoch": 0.19019189765458422,
      "grad_norm": 0.904279351234436,
      "learning_rate": 0.00024297356202027858,
      "loss": 0.1113,
      "step": 2007
    },
    {
      "epoch": 0.19028666192845298,
      "grad_norm": 2.050956964492798,
      "learning_rate": 0.00024294513408509426,
      "loss": 0.1778,
      "step": 2008
    },
    {
      "epoch": 0.19038142620232174,
      "grad_norm": 1.489538550376892,
      "learning_rate": 0.00024291670614990995,
      "loss": 0.1074,
      "step": 2009
    },
    {
      "epoch": 0.19047619047619047,
      "grad_norm": 3.0207197666168213,
      "learning_rate": 0.00024288827821472564,
      "loss": 0.3422,
      "step": 2010
    },
    {
      "epoch": 0.19057095475005922,
      "grad_norm": 1.800377607345581,
      "learning_rate": 0.00024285985027954136,
      "loss": 0.1171,
      "step": 2011
    },
    {
      "epoch": 0.19066571902392798,
      "grad_norm": 6.320285797119141,
      "learning_rate": 0.00024283142234435704,
      "loss": 0.455,
      "step": 2012
    },
    {
      "epoch": 0.19076048329779674,
      "grad_norm": 3.7758917808532715,
      "learning_rate": 0.00024280299440917273,
      "loss": 0.1452,
      "step": 2013
    },
    {
      "epoch": 0.19085524757166547,
      "grad_norm": 4.736240863800049,
      "learning_rate": 0.00024277456647398842,
      "loss": 0.1993,
      "step": 2014
    },
    {
      "epoch": 0.19095001184553423,
      "grad_norm": 1.2636189460754395,
      "learning_rate": 0.00024274613853880414,
      "loss": 0.0926,
      "step": 2015
    },
    {
      "epoch": 0.191044776119403,
      "grad_norm": 2.051453113555908,
      "learning_rate": 0.0002427177106036198,
      "loss": 0.1795,
      "step": 2016
    },
    {
      "epoch": 0.19113954039327175,
      "grad_norm": 6.534118175506592,
      "learning_rate": 0.00024268928266843549,
      "loss": 0.1687,
      "step": 2017
    },
    {
      "epoch": 0.19123430466714048,
      "grad_norm": 2.361985445022583,
      "learning_rate": 0.00024266085473325117,
      "loss": 0.1415,
      "step": 2018
    },
    {
      "epoch": 0.19132906894100923,
      "grad_norm": 1.626555323600769,
      "learning_rate": 0.00024263242679806686,
      "loss": 0.1298,
      "step": 2019
    },
    {
      "epoch": 0.191423833214878,
      "grad_norm": 2.0065650939941406,
      "learning_rate": 0.00024260399886288258,
      "loss": 0.1793,
      "step": 2020
    },
    {
      "epoch": 0.19151859748874675,
      "grad_norm": 2.3940298557281494,
      "learning_rate": 0.00024257557092769827,
      "loss": 0.2755,
      "step": 2021
    },
    {
      "epoch": 0.19161336176261548,
      "grad_norm": 6.3044047355651855,
      "learning_rate": 0.00024254714299251395,
      "loss": 0.1313,
      "step": 2022
    },
    {
      "epoch": 0.19170812603648424,
      "grad_norm": 1.887359857559204,
      "learning_rate": 0.00024251871505732964,
      "loss": 0.2548,
      "step": 2023
    },
    {
      "epoch": 0.191802890310353,
      "grad_norm": 6.41745662689209,
      "learning_rate": 0.00024249028712214533,
      "loss": 0.1892,
      "step": 2024
    },
    {
      "epoch": 0.19189765458422176,
      "grad_norm": 1.8803348541259766,
      "learning_rate": 0.00024246185918696105,
      "loss": 0.1454,
      "step": 2025
    },
    {
      "epoch": 0.1919924188580905,
      "grad_norm": 12.58386516571045,
      "learning_rate": 0.00024243343125177674,
      "loss": 0.2989,
      "step": 2026
    },
    {
      "epoch": 0.19208718313195924,
      "grad_norm": 11.663904190063477,
      "learning_rate": 0.00024240500331659242,
      "loss": 0.1499,
      "step": 2027
    },
    {
      "epoch": 0.192181947405828,
      "grad_norm": 6.606687068939209,
      "learning_rate": 0.0002423765753814081,
      "loss": 0.148,
      "step": 2028
    },
    {
      "epoch": 0.19227671167969676,
      "grad_norm": 5.390662670135498,
      "learning_rate": 0.00024234814744622383,
      "loss": 0.1631,
      "step": 2029
    },
    {
      "epoch": 0.19237147595356552,
      "grad_norm": 1.9034216403961182,
      "learning_rate": 0.0002423197195110395,
      "loss": 0.1494,
      "step": 2030
    },
    {
      "epoch": 0.19246624022743425,
      "grad_norm": 1.7794950008392334,
      "learning_rate": 0.00024229129157585518,
      "loss": 0.1297,
      "step": 2031
    },
    {
      "epoch": 0.192561004501303,
      "grad_norm": 5.714394569396973,
      "learning_rate": 0.00024226286364067087,
      "loss": 0.1679,
      "step": 2032
    },
    {
      "epoch": 0.19265576877517177,
      "grad_norm": 2.623399257659912,
      "learning_rate": 0.00024223443570548655,
      "loss": 0.1042,
      "step": 2033
    },
    {
      "epoch": 0.19275053304904052,
      "grad_norm": 3.782329797744751,
      "learning_rate": 0.00024220600777030227,
      "loss": 0.2576,
      "step": 2034
    },
    {
      "epoch": 0.19284529732290925,
      "grad_norm": 3.115677833557129,
      "learning_rate": 0.00024217757983511796,
      "loss": 0.2404,
      "step": 2035
    },
    {
      "epoch": 0.192940061596778,
      "grad_norm": 3.0719051361083984,
      "learning_rate": 0.00024214915189993365,
      "loss": 0.2693,
      "step": 2036
    },
    {
      "epoch": 0.19303482587064677,
      "grad_norm": 10.16821575164795,
      "learning_rate": 0.00024212072396474933,
      "loss": 0.3992,
      "step": 2037
    },
    {
      "epoch": 0.19312959014451553,
      "grad_norm": 6.189538478851318,
      "learning_rate": 0.00024209229602956502,
      "loss": 0.1266,
      "step": 2038
    },
    {
      "epoch": 0.19322435441838426,
      "grad_norm": 1.926312804222107,
      "learning_rate": 0.00024206386809438074,
      "loss": 0.1468,
      "step": 2039
    },
    {
      "epoch": 0.19331911869225302,
      "grad_norm": 5.5064568519592285,
      "learning_rate": 0.00024203544015919643,
      "loss": 0.2214,
      "step": 2040
    },
    {
      "epoch": 0.19341388296612178,
      "grad_norm": 3.773036003112793,
      "learning_rate": 0.00024200701222401212,
      "loss": 0.2753,
      "step": 2041
    },
    {
      "epoch": 0.19350864723999053,
      "grad_norm": 4.003133773803711,
      "learning_rate": 0.0002419785842888278,
      "loss": 0.1395,
      "step": 2042
    },
    {
      "epoch": 0.19360341151385926,
      "grad_norm": 5.506394863128662,
      "learning_rate": 0.00024195015635364346,
      "loss": 0.1383,
      "step": 2043
    },
    {
      "epoch": 0.19369817578772802,
      "grad_norm": 1.1209206581115723,
      "learning_rate": 0.00024192172841845918,
      "loss": 0.0761,
      "step": 2044
    },
    {
      "epoch": 0.19379294006159678,
      "grad_norm": 7.259209632873535,
      "learning_rate": 0.00024189330048327487,
      "loss": 0.1869,
      "step": 2045
    },
    {
      "epoch": 0.19388770433546554,
      "grad_norm": 4.815157890319824,
      "learning_rate": 0.00024186487254809056,
      "loss": 0.0872,
      "step": 2046
    },
    {
      "epoch": 0.19398246860933427,
      "grad_norm": 2.3848538398742676,
      "learning_rate": 0.00024183644461290625,
      "loss": 0.1568,
      "step": 2047
    },
    {
      "epoch": 0.19407723288320303,
      "grad_norm": 2.321343183517456,
      "learning_rate": 0.00024180801667772196,
      "loss": 0.1313,
      "step": 2048
    },
    {
      "epoch": 0.19417199715707179,
      "grad_norm": 6.709113121032715,
      "learning_rate": 0.00024177958874253765,
      "loss": 0.2662,
      "step": 2049
    },
    {
      "epoch": 0.19426676143094054,
      "grad_norm": 16.687597274780273,
      "learning_rate": 0.00024175116080735334,
      "loss": 0.3671,
      "step": 2050
    },
    {
      "epoch": 0.19436152570480927,
      "grad_norm": 1.7071723937988281,
      "learning_rate": 0.00024172273287216903,
      "loss": 0.0971,
      "step": 2051
    },
    {
      "epoch": 0.19445628997867803,
      "grad_norm": 2.986996650695801,
      "learning_rate": 0.00024169430493698474,
      "loss": 0.1246,
      "step": 2052
    },
    {
      "epoch": 0.1945510542525468,
      "grad_norm": 3.014120578765869,
      "learning_rate": 0.00024166587700180043,
      "loss": 0.0344,
      "step": 2053
    },
    {
      "epoch": 0.19464581852641555,
      "grad_norm": 2.718644142150879,
      "learning_rate": 0.00024163744906661612,
      "loss": 0.3196,
      "step": 2054
    },
    {
      "epoch": 0.1947405828002843,
      "grad_norm": 1.4720982313156128,
      "learning_rate": 0.0002416090211314318,
      "loss": 0.0948,
      "step": 2055
    },
    {
      "epoch": 0.19483534707415304,
      "grad_norm": 6.653286933898926,
      "learning_rate": 0.0002415805931962475,
      "loss": 0.1583,
      "step": 2056
    },
    {
      "epoch": 0.1949301113480218,
      "grad_norm": 11.176593780517578,
      "learning_rate": 0.00024155216526106316,
      "loss": 0.1225,
      "step": 2057
    },
    {
      "epoch": 0.19502487562189055,
      "grad_norm": 5.056331157684326,
      "learning_rate": 0.00024152373732587887,
      "loss": 0.1112,
      "step": 2058
    },
    {
      "epoch": 0.1951196398957593,
      "grad_norm": 2.0545263290405273,
      "learning_rate": 0.00024149530939069456,
      "loss": 0.1186,
      "step": 2059
    },
    {
      "epoch": 0.19521440416962804,
      "grad_norm": 14.024041175842285,
      "learning_rate": 0.00024146688145551025,
      "loss": 0.1829,
      "step": 2060
    },
    {
      "epoch": 0.1953091684434968,
      "grad_norm": 1.6161901950836182,
      "learning_rate": 0.00024143845352032594,
      "loss": 0.1734,
      "step": 2061
    },
    {
      "epoch": 0.19540393271736556,
      "grad_norm": 1.5174939632415771,
      "learning_rate": 0.00024141002558514165,
      "loss": 0.0771,
      "step": 2062
    },
    {
      "epoch": 0.19549869699123432,
      "grad_norm": 0.888652503490448,
      "learning_rate": 0.00024138159764995734,
      "loss": 0.0266,
      "step": 2063
    },
    {
      "epoch": 0.19559346126510305,
      "grad_norm": 2.4053516387939453,
      "learning_rate": 0.00024135316971477303,
      "loss": 0.2582,
      "step": 2064
    },
    {
      "epoch": 0.1956882255389718,
      "grad_norm": 4.171948432922363,
      "learning_rate": 0.00024132474177958872,
      "loss": 0.141,
      "step": 2065
    },
    {
      "epoch": 0.19578298981284056,
      "grad_norm": 1.7910979986190796,
      "learning_rate": 0.00024129631384440443,
      "loss": 0.1714,
      "step": 2066
    },
    {
      "epoch": 0.19587775408670932,
      "grad_norm": 12.398591041564941,
      "learning_rate": 0.00024126788590922012,
      "loss": 0.2574,
      "step": 2067
    },
    {
      "epoch": 0.19597251836057805,
      "grad_norm": 7.956451416015625,
      "learning_rate": 0.0002412394579740358,
      "loss": 0.2108,
      "step": 2068
    },
    {
      "epoch": 0.1960672826344468,
      "grad_norm": 9.447474479675293,
      "learning_rate": 0.0002412110300388515,
      "loss": 0.2643,
      "step": 2069
    },
    {
      "epoch": 0.19616204690831557,
      "grad_norm": 6.261938571929932,
      "learning_rate": 0.00024118260210366716,
      "loss": 0.2216,
      "step": 2070
    },
    {
      "epoch": 0.19625681118218433,
      "grad_norm": 9.426848411560059,
      "learning_rate": 0.00024115417416848287,
      "loss": 0.1702,
      "step": 2071
    },
    {
      "epoch": 0.19635157545605306,
      "grad_norm": 1.6669304370880127,
      "learning_rate": 0.00024112574623329856,
      "loss": 0.1194,
      "step": 2072
    },
    {
      "epoch": 0.19644633972992182,
      "grad_norm": 2.1070265769958496,
      "learning_rate": 0.00024109731829811425,
      "loss": 0.1313,
      "step": 2073
    },
    {
      "epoch": 0.19654110400379057,
      "grad_norm": 2.9185967445373535,
      "learning_rate": 0.00024106889036292994,
      "loss": 0.2876,
      "step": 2074
    },
    {
      "epoch": 0.19663586827765933,
      "grad_norm": 5.592100620269775,
      "learning_rate": 0.00024104046242774563,
      "loss": 0.2166,
      "step": 2075
    },
    {
      "epoch": 0.19673063255152806,
      "grad_norm": 1.7692656517028809,
      "learning_rate": 0.00024101203449256134,
      "loss": 0.062,
      "step": 2076
    },
    {
      "epoch": 0.19682539682539682,
      "grad_norm": 0.5147013664245605,
      "learning_rate": 0.00024098360655737703,
      "loss": 0.0086,
      "step": 2077
    },
    {
      "epoch": 0.19692016109926558,
      "grad_norm": 6.692766189575195,
      "learning_rate": 0.00024095517862219272,
      "loss": 0.3076,
      "step": 2078
    },
    {
      "epoch": 0.19701492537313434,
      "grad_norm": 1.5965759754180908,
      "learning_rate": 0.0002409267506870084,
      "loss": 0.0739,
      "step": 2079
    },
    {
      "epoch": 0.19710968964700307,
      "grad_norm": 4.565943241119385,
      "learning_rate": 0.00024089832275182412,
      "loss": 0.1413,
      "step": 2080
    },
    {
      "epoch": 0.19720445392087183,
      "grad_norm": 12.58309555053711,
      "learning_rate": 0.0002408698948166398,
      "loss": 0.1448,
      "step": 2081
    },
    {
      "epoch": 0.19729921819474058,
      "grad_norm": 2.701040506362915,
      "learning_rate": 0.0002408414668814555,
      "loss": 0.2295,
      "step": 2082
    },
    {
      "epoch": 0.19739398246860934,
      "grad_norm": 1.8463952541351318,
      "learning_rate": 0.0002408130389462712,
      "loss": 0.1376,
      "step": 2083
    },
    {
      "epoch": 0.1974887467424781,
      "grad_norm": 3.0255072116851807,
      "learning_rate": 0.00024078461101108685,
      "loss": 0.304,
      "step": 2084
    },
    {
      "epoch": 0.19758351101634683,
      "grad_norm": 7.572998046875,
      "learning_rate": 0.00024075618307590257,
      "loss": 0.1933,
      "step": 2085
    },
    {
      "epoch": 0.1976782752902156,
      "grad_norm": 2.272212266921997,
      "learning_rate": 0.00024072775514071825,
      "loss": 0.2195,
      "step": 2086
    },
    {
      "epoch": 0.19777303956408435,
      "grad_norm": 5.834142684936523,
      "learning_rate": 0.00024069932720553394,
      "loss": 0.2898,
      "step": 2087
    },
    {
      "epoch": 0.1978678038379531,
      "grad_norm": 3.263122081756592,
      "learning_rate": 0.00024067089927034963,
      "loss": 0.4063,
      "step": 2088
    },
    {
      "epoch": 0.19796256811182183,
      "grad_norm": 3.574331045150757,
      "learning_rate": 0.00024064247133516532,
      "loss": 0.1827,
      "step": 2089
    },
    {
      "epoch": 0.1980573323856906,
      "grad_norm": 5.337069988250732,
      "learning_rate": 0.00024061404339998103,
      "loss": 0.1074,
      "step": 2090
    },
    {
      "epoch": 0.19815209665955935,
      "grad_norm": 1.2346915006637573,
      "learning_rate": 0.00024058561546479672,
      "loss": 0.1155,
      "step": 2091
    },
    {
      "epoch": 0.1982468609334281,
      "grad_norm": 6.329611301422119,
      "learning_rate": 0.0002405571875296124,
      "loss": 0.2491,
      "step": 2092
    },
    {
      "epoch": 0.19834162520729684,
      "grad_norm": 1.402172565460205,
      "learning_rate": 0.0002405287595944281,
      "loss": 0.0933,
      "step": 2093
    },
    {
      "epoch": 0.1984363894811656,
      "grad_norm": 2.467106342315674,
      "learning_rate": 0.00024050033165924382,
      "loss": 0.1759,
      "step": 2094
    },
    {
      "epoch": 0.19853115375503436,
      "grad_norm": 13.223746299743652,
      "learning_rate": 0.0002404719037240595,
      "loss": 0.1746,
      "step": 2095
    },
    {
      "epoch": 0.19862591802890311,
      "grad_norm": 6.872497081756592,
      "learning_rate": 0.0002404434757888752,
      "loss": 0.183,
      "step": 2096
    },
    {
      "epoch": 0.19872068230277184,
      "grad_norm": 6.140780925750732,
      "learning_rate": 0.00024041504785369085,
      "loss": 0.2333,
      "step": 2097
    },
    {
      "epoch": 0.1988154465766406,
      "grad_norm": 4.3452019691467285,
      "learning_rate": 0.00024038661991850654,
      "loss": 0.279,
      "step": 2098
    },
    {
      "epoch": 0.19891021085050936,
      "grad_norm": 5.239523887634277,
      "learning_rate": 0.00024035819198332226,
      "loss": 0.2311,
      "step": 2099
    },
    {
      "epoch": 0.19900497512437812,
      "grad_norm": 7.677933216094971,
      "learning_rate": 0.00024032976404813795,
      "loss": 0.2445,
      "step": 2100
    },
    {
      "epoch": 0.19909973939824685,
      "grad_norm": 1.6329364776611328,
      "learning_rate": 0.00024030133611295363,
      "loss": 0.1577,
      "step": 2101
    },
    {
      "epoch": 0.1991945036721156,
      "grad_norm": 2.9596409797668457,
      "learning_rate": 0.00024027290817776932,
      "loss": 0.2488,
      "step": 2102
    },
    {
      "epoch": 0.19928926794598437,
      "grad_norm": 2.204075813293457,
      "learning_rate": 0.00024024448024258504,
      "loss": 0.2305,
      "step": 2103
    },
    {
      "epoch": 0.19938403221985312,
      "grad_norm": 1.6795308589935303,
      "learning_rate": 0.00024021605230740073,
      "loss": 0.1033,
      "step": 2104
    },
    {
      "epoch": 0.19947879649372185,
      "grad_norm": 2.5365350246429443,
      "learning_rate": 0.00024018762437221641,
      "loss": 0.1411,
      "step": 2105
    },
    {
      "epoch": 0.1995735607675906,
      "grad_norm": 6.062169551849365,
      "learning_rate": 0.0002401591964370321,
      "loss": 0.306,
      "step": 2106
    },
    {
      "epoch": 0.19966832504145937,
      "grad_norm": 3.333237648010254,
      "learning_rate": 0.0002401307685018478,
      "loss": 0.174,
      "step": 2107
    },
    {
      "epoch": 0.19976308931532813,
      "grad_norm": 2.962510108947754,
      "learning_rate": 0.0002401023405666635,
      "loss": 0.1799,
      "step": 2108
    },
    {
      "epoch": 0.19985785358919686,
      "grad_norm": 2.4432551860809326,
      "learning_rate": 0.0002400739126314792,
      "loss": 0.0788,
      "step": 2109
    },
    {
      "epoch": 0.19995261786306562,
      "grad_norm": 2.265176773071289,
      "learning_rate": 0.00024004548469629488,
      "loss": 0.1301,
      "step": 2110
    },
    {
      "epoch": 0.20004738213693438,
      "grad_norm": 7.501734256744385,
      "learning_rate": 0.00024001705676111054,
      "loss": 0.3199,
      "step": 2111
    },
    {
      "epoch": 0.20014214641080313,
      "grad_norm": 7.202517986297607,
      "learning_rate": 0.00023998862882592623,
      "loss": 0.0966,
      "step": 2112
    },
    {
      "epoch": 0.20023691068467186,
      "grad_norm": 3.1011016368865967,
      "learning_rate": 0.00023996020089074195,
      "loss": 0.086,
      "step": 2113
    },
    {
      "epoch": 0.20033167495854062,
      "grad_norm": 4.658886909484863,
      "learning_rate": 0.00023993177295555764,
      "loss": 0.2693,
      "step": 2114
    },
    {
      "epoch": 0.20042643923240938,
      "grad_norm": 4.82171106338501,
      "learning_rate": 0.00023990334502037332,
      "loss": 0.0854,
      "step": 2115
    },
    {
      "epoch": 0.20052120350627814,
      "grad_norm": 2.113232135772705,
      "learning_rate": 0.000239874917085189,
      "loss": 0.0969,
      "step": 2116
    },
    {
      "epoch": 0.2006159677801469,
      "grad_norm": 8.22176456451416,
      "learning_rate": 0.00023984648915000473,
      "loss": 0.2347,
      "step": 2117
    },
    {
      "epoch": 0.20071073205401563,
      "grad_norm": 2.3856520652770996,
      "learning_rate": 0.00023981806121482042,
      "loss": 0.1179,
      "step": 2118
    },
    {
      "epoch": 0.2008054963278844,
      "grad_norm": 7.103582382202148,
      "learning_rate": 0.0002397896332796361,
      "loss": 0.3693,
      "step": 2119
    },
    {
      "epoch": 0.20090026060175314,
      "grad_norm": 2.6282074451446533,
      "learning_rate": 0.0002397612053444518,
      "loss": 0.1549,
      "step": 2120
    },
    {
      "epoch": 0.2009950248756219,
      "grad_norm": 1.7987258434295654,
      "learning_rate": 0.0002397327774092675,
      "loss": 0.0877,
      "step": 2121
    },
    {
      "epoch": 0.20108978914949063,
      "grad_norm": 20.77861976623535,
      "learning_rate": 0.0002397043494740832,
      "loss": 0.4401,
      "step": 2122
    },
    {
      "epoch": 0.2011845534233594,
      "grad_norm": 3.5685579776763916,
      "learning_rate": 0.00023967592153889889,
      "loss": 0.1715,
      "step": 2123
    },
    {
      "epoch": 0.20127931769722815,
      "grad_norm": 2.7186014652252197,
      "learning_rate": 0.00023964749360371455,
      "loss": 0.1877,
      "step": 2124
    },
    {
      "epoch": 0.2013740819710969,
      "grad_norm": 9.056072235107422,
      "learning_rate": 0.00023961906566853024,
      "loss": 0.2887,
      "step": 2125
    },
    {
      "epoch": 0.20146884624496564,
      "grad_norm": 6.831856727600098,
      "learning_rate": 0.00023959063773334592,
      "loss": 0.2619,
      "step": 2126
    },
    {
      "epoch": 0.2015636105188344,
      "grad_norm": 2.073441505432129,
      "learning_rate": 0.00023956220979816164,
      "loss": 0.0969,
      "step": 2127
    },
    {
      "epoch": 0.20165837479270315,
      "grad_norm": 7.383899688720703,
      "learning_rate": 0.00023953378186297733,
      "loss": 0.2318,
      "step": 2128
    },
    {
      "epoch": 0.2017531390665719,
      "grad_norm": 2.1538827419281006,
      "learning_rate": 0.00023950535392779302,
      "loss": 0.0838,
      "step": 2129
    },
    {
      "epoch": 0.20184790334044064,
      "grad_norm": 3.549422264099121,
      "learning_rate": 0.0002394769259926087,
      "loss": 0.1704,
      "step": 2130
    },
    {
      "epoch": 0.2019426676143094,
      "grad_norm": 12.828312873840332,
      "learning_rate": 0.00023944849805742442,
      "loss": 0.2396,
      "step": 2131
    },
    {
      "epoch": 0.20203743188817816,
      "grad_norm": 2.4229297637939453,
      "learning_rate": 0.0002394200701222401,
      "loss": 0.1522,
      "step": 2132
    },
    {
      "epoch": 0.20213219616204692,
      "grad_norm": 1.8257561922073364,
      "learning_rate": 0.0002393916421870558,
      "loss": 0.1595,
      "step": 2133
    },
    {
      "epoch": 0.20222696043591565,
      "grad_norm": 1.7208056449890137,
      "learning_rate": 0.00023936321425187148,
      "loss": 0.0875,
      "step": 2134
    },
    {
      "epoch": 0.2023217247097844,
      "grad_norm": 2.3659543991088867,
      "learning_rate": 0.0002393347863166872,
      "loss": 0.132,
      "step": 2135
    },
    {
      "epoch": 0.20241648898365316,
      "grad_norm": 3.7689309120178223,
      "learning_rate": 0.0002393063583815029,
      "loss": 0.2656,
      "step": 2136
    },
    {
      "epoch": 0.20251125325752192,
      "grad_norm": 10.038737297058105,
      "learning_rate": 0.00023927793044631858,
      "loss": 0.118,
      "step": 2137
    },
    {
      "epoch": 0.20260601753139065,
      "grad_norm": 13.121278762817383,
      "learning_rate": 0.00023924950251113424,
      "loss": 0.3267,
      "step": 2138
    },
    {
      "epoch": 0.2027007818052594,
      "grad_norm": 0.8319206833839417,
      "learning_rate": 0.00023922107457594993,
      "loss": 0.0263,
      "step": 2139
    },
    {
      "epoch": 0.20279554607912817,
      "grad_norm": 8.701327323913574,
      "learning_rate": 0.00023919264664076562,
      "loss": 0.2285,
      "step": 2140
    },
    {
      "epoch": 0.20289031035299693,
      "grad_norm": 0.9219265580177307,
      "learning_rate": 0.00023916421870558133,
      "loss": 0.0613,
      "step": 2141
    },
    {
      "epoch": 0.20298507462686566,
      "grad_norm": 10.753629684448242,
      "learning_rate": 0.00023913579077039702,
      "loss": 0.1213,
      "step": 2142
    },
    {
      "epoch": 0.20307983890073442,
      "grad_norm": 8.027219772338867,
      "learning_rate": 0.0002391073628352127,
      "loss": 0.4946,
      "step": 2143
    },
    {
      "epoch": 0.20317460317460317,
      "grad_norm": 7.550285816192627,
      "learning_rate": 0.0002390789349000284,
      "loss": 0.3443,
      "step": 2144
    },
    {
      "epoch": 0.20326936744847193,
      "grad_norm": 12.001787185668945,
      "learning_rate": 0.0002390505069648441,
      "loss": 0.2233,
      "step": 2145
    },
    {
      "epoch": 0.2033641317223407,
      "grad_norm": 2.127624034881592,
      "learning_rate": 0.0002390220790296598,
      "loss": 0.0501,
      "step": 2146
    },
    {
      "epoch": 0.20345889599620942,
      "grad_norm": 2.437319755554199,
      "learning_rate": 0.0002389936510944755,
      "loss": 0.1118,
      "step": 2147
    },
    {
      "epoch": 0.20355366027007818,
      "grad_norm": 2.7191312313079834,
      "learning_rate": 0.00023896522315929118,
      "loss": 0.2511,
      "step": 2148
    },
    {
      "epoch": 0.20364842454394694,
      "grad_norm": 6.1083760261535645,
      "learning_rate": 0.0002389367952241069,
      "loss": 0.189,
      "step": 2149
    },
    {
      "epoch": 0.2037431888178157,
      "grad_norm": 2.658102035522461,
      "learning_rate": 0.00023890836728892258,
      "loss": 0.1686,
      "step": 2150
    },
    {
      "epoch": 0.20383795309168443,
      "grad_norm": 9.070775985717773,
      "learning_rate": 0.00023887993935373824,
      "loss": 0.2013,
      "step": 2151
    },
    {
      "epoch": 0.20393271736555318,
      "grad_norm": 4.603090286254883,
      "learning_rate": 0.00023885151141855393,
      "loss": 0.2669,
      "step": 2152
    },
    {
      "epoch": 0.20402748163942194,
      "grad_norm": 1.9337427616119385,
      "learning_rate": 0.00023882308348336962,
      "loss": 0.1639,
      "step": 2153
    },
    {
      "epoch": 0.2041222459132907,
      "grad_norm": 5.95983362197876,
      "learning_rate": 0.00023879465554818533,
      "loss": 0.1761,
      "step": 2154
    },
    {
      "epoch": 0.20421701018715943,
      "grad_norm": 6.64766263961792,
      "learning_rate": 0.00023876622761300102,
      "loss": 0.1068,
      "step": 2155
    },
    {
      "epoch": 0.2043117744610282,
      "grad_norm": 1.6327025890350342,
      "learning_rate": 0.0002387377996778167,
      "loss": 0.132,
      "step": 2156
    },
    {
      "epoch": 0.20440653873489695,
      "grad_norm": 2.529238700866699,
      "learning_rate": 0.0002387093717426324,
      "loss": 0.0446,
      "step": 2157
    },
    {
      "epoch": 0.2045013030087657,
      "grad_norm": 9.462610244750977,
      "learning_rate": 0.0002386809438074481,
      "loss": 0.2701,
      "step": 2158
    },
    {
      "epoch": 0.20459606728263444,
      "grad_norm": 6.748682975769043,
      "learning_rate": 0.0002386525158722638,
      "loss": 0.3115,
      "step": 2159
    },
    {
      "epoch": 0.2046908315565032,
      "grad_norm": 3.170820951461792,
      "learning_rate": 0.0002386240879370795,
      "loss": 0.1052,
      "step": 2160
    },
    {
      "epoch": 0.20478559583037195,
      "grad_norm": 7.1251349449157715,
      "learning_rate": 0.00023859566000189518,
      "loss": 0.1139,
      "step": 2161
    },
    {
      "epoch": 0.2048803601042407,
      "grad_norm": 3.8125317096710205,
      "learning_rate": 0.00023856723206671087,
      "loss": 0.344,
      "step": 2162
    },
    {
      "epoch": 0.20497512437810944,
      "grad_norm": 14.183117866516113,
      "learning_rate": 0.00023853880413152658,
      "loss": 0.3163,
      "step": 2163
    },
    {
      "epoch": 0.2050698886519782,
      "grad_norm": 2.119518280029297,
      "learning_rate": 0.00023851037619634227,
      "loss": 0.2271,
      "step": 2164
    },
    {
      "epoch": 0.20516465292584696,
      "grad_norm": 2.464826822280884,
      "learning_rate": 0.00023848194826115793,
      "loss": 0.1812,
      "step": 2165
    },
    {
      "epoch": 0.20525941719971572,
      "grad_norm": 2.5544216632843018,
      "learning_rate": 0.00023845352032597362,
      "loss": 0.2406,
      "step": 2166
    },
    {
      "epoch": 0.20535418147358445,
      "grad_norm": 2.2168996334075928,
      "learning_rate": 0.0002384250923907893,
      "loss": 0.1343,
      "step": 2167
    },
    {
      "epoch": 0.2054489457474532,
      "grad_norm": 2.3938252925872803,
      "learning_rate": 0.00023839666445560502,
      "loss": 0.1877,
      "step": 2168
    },
    {
      "epoch": 0.20554371002132196,
      "grad_norm": 2.0111794471740723,
      "learning_rate": 0.0002383682365204207,
      "loss": 0.0891,
      "step": 2169
    },
    {
      "epoch": 0.20563847429519072,
      "grad_norm": 1.9349008798599243,
      "learning_rate": 0.0002383398085852364,
      "loss": 0.0596,
      "step": 2170
    },
    {
      "epoch": 0.20573323856905945,
      "grad_norm": 1.6083906888961792,
      "learning_rate": 0.0002383113806500521,
      "loss": 0.1556,
      "step": 2171
    },
    {
      "epoch": 0.2058280028429282,
      "grad_norm": 4.344687461853027,
      "learning_rate": 0.0002382829527148678,
      "loss": 0.2438,
      "step": 2172
    },
    {
      "epoch": 0.20592276711679697,
      "grad_norm": 3.870327949523926,
      "learning_rate": 0.0002382545247796835,
      "loss": 0.0916,
      "step": 2173
    },
    {
      "epoch": 0.20601753139066573,
      "grad_norm": 2.3456616401672363,
      "learning_rate": 0.00023822609684449918,
      "loss": 0.2867,
      "step": 2174
    },
    {
      "epoch": 0.20611229566453448,
      "grad_norm": 4.510607719421387,
      "learning_rate": 0.00023819766890931487,
      "loss": 0.1549,
      "step": 2175
    },
    {
      "epoch": 0.20620705993840321,
      "grad_norm": 6.354869365692139,
      "learning_rate": 0.00023816924097413056,
      "loss": 0.2354,
      "step": 2176
    },
    {
      "epoch": 0.20630182421227197,
      "grad_norm": 2.189223051071167,
      "learning_rate": 0.00023814081303894627,
      "loss": 0.1831,
      "step": 2177
    },
    {
      "epoch": 0.20639658848614073,
      "grad_norm": 2.683333158493042,
      "learning_rate": 0.00023811238510376194,
      "loss": 0.1645,
      "step": 2178
    },
    {
      "epoch": 0.2064913527600095,
      "grad_norm": 2.38161301612854,
      "learning_rate": 0.00023808395716857762,
      "loss": 0.0947,
      "step": 2179
    },
    {
      "epoch": 0.20658611703387822,
      "grad_norm": 5.535432815551758,
      "learning_rate": 0.0002380555292333933,
      "loss": 0.1753,
      "step": 2180
    },
    {
      "epoch": 0.20668088130774698,
      "grad_norm": 2.978635787963867,
      "learning_rate": 0.000238027101298209,
      "loss": 0.3481,
      "step": 2181
    },
    {
      "epoch": 0.20677564558161574,
      "grad_norm": 2.304065704345703,
      "learning_rate": 0.00023799867336302472,
      "loss": 0.1063,
      "step": 2182
    },
    {
      "epoch": 0.2068704098554845,
      "grad_norm": 2.4138576984405518,
      "learning_rate": 0.0002379702454278404,
      "loss": 0.1161,
      "step": 2183
    },
    {
      "epoch": 0.20696517412935322,
      "grad_norm": 2.982743263244629,
      "learning_rate": 0.0002379418174926561,
      "loss": 0.1772,
      "step": 2184
    },
    {
      "epoch": 0.20705993840322198,
      "grad_norm": 5.1859130859375,
      "learning_rate": 0.00023791338955747178,
      "loss": 0.179,
      "step": 2185
    },
    {
      "epoch": 0.20715470267709074,
      "grad_norm": 3.5392913818359375,
      "learning_rate": 0.0002378849616222875,
      "loss": 0.2314,
      "step": 2186
    },
    {
      "epoch": 0.2072494669509595,
      "grad_norm": 2.4051411151885986,
      "learning_rate": 0.00023785653368710318,
      "loss": 0.1051,
      "step": 2187
    },
    {
      "epoch": 0.20734423122482823,
      "grad_norm": 3.1585404872894287,
      "learning_rate": 0.00023782810575191887,
      "loss": 0.2129,
      "step": 2188
    },
    {
      "epoch": 0.207438995498697,
      "grad_norm": 3.837536573410034,
      "learning_rate": 0.00023779967781673456,
      "loss": 0.1287,
      "step": 2189
    },
    {
      "epoch": 0.20753375977256575,
      "grad_norm": 2.159475564956665,
      "learning_rate": 0.00023777124988155025,
      "loss": 0.1153,
      "step": 2190
    },
    {
      "epoch": 0.2076285240464345,
      "grad_norm": 1.5698680877685547,
      "learning_rate": 0.00023774282194636597,
      "loss": 0.1604,
      "step": 2191
    },
    {
      "epoch": 0.20772328832030323,
      "grad_norm": 1.149812936782837,
      "learning_rate": 0.00023771439401118163,
      "loss": 0.0665,
      "step": 2192
    },
    {
      "epoch": 0.207818052594172,
      "grad_norm": 9.466904640197754,
      "learning_rate": 0.00023768596607599732,
      "loss": 0.4389,
      "step": 2193
    },
    {
      "epoch": 0.20791281686804075,
      "grad_norm": 3.308178186416626,
      "learning_rate": 0.000237657538140813,
      "loss": 0.1158,
      "step": 2194
    },
    {
      "epoch": 0.2080075811419095,
      "grad_norm": 2.3743748664855957,
      "learning_rate": 0.0002376291102056287,
      "loss": 0.0779,
      "step": 2195
    },
    {
      "epoch": 0.20810234541577824,
      "grad_norm": 5.350663185119629,
      "learning_rate": 0.0002376006822704444,
      "loss": 0.11,
      "step": 2196
    },
    {
      "epoch": 0.208197109689647,
      "grad_norm": 1.5865119695663452,
      "learning_rate": 0.0002375722543352601,
      "loss": 0.074,
      "step": 2197
    },
    {
      "epoch": 0.20829187396351576,
      "grad_norm": 2.577038049697876,
      "learning_rate": 0.00023754382640007578,
      "loss": 0.136,
      "step": 2198
    },
    {
      "epoch": 0.2083866382373845,
      "grad_norm": 1.659114122390747,
      "learning_rate": 0.00023751539846489147,
      "loss": 0.0195,
      "step": 2199
    },
    {
      "epoch": 0.20848140251125324,
      "grad_norm": 5.961132526397705,
      "learning_rate": 0.0002374869705297072,
      "loss": 0.0679,
      "step": 2200
    },
    {
      "epoch": 0.208576166785122,
      "grad_norm": 3.9983253479003906,
      "learning_rate": 0.00023745854259452288,
      "loss": 0.2356,
      "step": 2201
    },
    {
      "epoch": 0.20867093105899076,
      "grad_norm": 2.552137851715088,
      "learning_rate": 0.00023743011465933856,
      "loss": 0.1969,
      "step": 2202
    },
    {
      "epoch": 0.20876569533285952,
      "grad_norm": 1.4190956354141235,
      "learning_rate": 0.00023740168672415425,
      "loss": 0.0636,
      "step": 2203
    },
    {
      "epoch": 0.20886045960672828,
      "grad_norm": 5.628076076507568,
      "learning_rate": 0.00023737325878896997,
      "loss": 0.0981,
      "step": 2204
    },
    {
      "epoch": 0.208955223880597,
      "grad_norm": 0.48441147804260254,
      "learning_rate": 0.00023734483085378563,
      "loss": 0.0194,
      "step": 2205
    },
    {
      "epoch": 0.20904998815446577,
      "grad_norm": 17.743616104125977,
      "learning_rate": 0.00023731640291860132,
      "loss": 0.3284,
      "step": 2206
    },
    {
      "epoch": 0.20914475242833452,
      "grad_norm": 2.2068421840667725,
      "learning_rate": 0.000237287974983417,
      "loss": 0.0747,
      "step": 2207
    },
    {
      "epoch": 0.20923951670220328,
      "grad_norm": 2.160890817642212,
      "learning_rate": 0.0002372595470482327,
      "loss": 0.0534,
      "step": 2208
    },
    {
      "epoch": 0.209334280976072,
      "grad_norm": 2.325791358947754,
      "learning_rate": 0.00023723111911304838,
      "loss": 0.1489,
      "step": 2209
    },
    {
      "epoch": 0.20942904524994077,
      "grad_norm": 7.215681076049805,
      "learning_rate": 0.0002372026911778641,
      "loss": 0.2313,
      "step": 2210
    },
    {
      "epoch": 0.20952380952380953,
      "grad_norm": 4.247716903686523,
      "learning_rate": 0.0002371742632426798,
      "loss": 0.0846,
      "step": 2211
    },
    {
      "epoch": 0.2096185737976783,
      "grad_norm": 3.531780242919922,
      "learning_rate": 0.00023714583530749548,
      "loss": 0.1446,
      "step": 2212
    },
    {
      "epoch": 0.20971333807154702,
      "grad_norm": 1.9903452396392822,
      "learning_rate": 0.00023711740737231116,
      "loss": 0.126,
      "step": 2213
    },
    {
      "epoch": 0.20980810234541578,
      "grad_norm": 1.2704979181289673,
      "learning_rate": 0.00023708897943712688,
      "loss": 0.0889,
      "step": 2214
    },
    {
      "epoch": 0.20990286661928453,
      "grad_norm": 2.8672783374786377,
      "learning_rate": 0.00023706055150194257,
      "loss": 0.1228,
      "step": 2215
    },
    {
      "epoch": 0.2099976308931533,
      "grad_norm": 1.673429250717163,
      "learning_rate": 0.00023703212356675826,
      "loss": 0.1238,
      "step": 2216
    },
    {
      "epoch": 0.21009239516702202,
      "grad_norm": 5.851113319396973,
      "learning_rate": 0.00023700369563157394,
      "loss": 0.0417,
      "step": 2217
    },
    {
      "epoch": 0.21018715944089078,
      "grad_norm": 3.0753791332244873,
      "learning_rate": 0.00023697526769638966,
      "loss": 0.1537,
      "step": 2218
    },
    {
      "epoch": 0.21028192371475954,
      "grad_norm": 2.568632125854492,
      "learning_rate": 0.00023694683976120532,
      "loss": 0.1928,
      "step": 2219
    },
    {
      "epoch": 0.2103766879886283,
      "grad_norm": 3.177069664001465,
      "learning_rate": 0.000236918411826021,
      "loss": 0.2273,
      "step": 2220
    },
    {
      "epoch": 0.21047145226249703,
      "grad_norm": 3.5647594928741455,
      "learning_rate": 0.0002368899838908367,
      "loss": 0.1593,
      "step": 2221
    },
    {
      "epoch": 0.21056621653636579,
      "grad_norm": 3.918414354324341,
      "learning_rate": 0.00023686155595565239,
      "loss": 0.1489,
      "step": 2222
    },
    {
      "epoch": 0.21066098081023454,
      "grad_norm": 14.041259765625,
      "learning_rate": 0.0002368331280204681,
      "loss": 0.2905,
      "step": 2223
    },
    {
      "epoch": 0.2107557450841033,
      "grad_norm": 2.040400981903076,
      "learning_rate": 0.0002368047000852838,
      "loss": 0.2579,
      "step": 2224
    },
    {
      "epoch": 0.21085050935797203,
      "grad_norm": 6.54199743270874,
      "learning_rate": 0.00023677627215009948,
      "loss": 0.1118,
      "step": 2225
    },
    {
      "epoch": 0.2109452736318408,
      "grad_norm": 9.053753852844238,
      "learning_rate": 0.00023674784421491517,
      "loss": 0.4661,
      "step": 2226
    },
    {
      "epoch": 0.21104003790570955,
      "grad_norm": 8.505234718322754,
      "learning_rate": 0.00023671941627973085,
      "loss": 0.5648,
      "step": 2227
    },
    {
      "epoch": 0.2111348021795783,
      "grad_norm": 4.901126861572266,
      "learning_rate": 0.00023669098834454657,
      "loss": 0.3072,
      "step": 2228
    },
    {
      "epoch": 0.21122956645344704,
      "grad_norm": 7.110881328582764,
      "learning_rate": 0.00023666256040936226,
      "loss": 0.2194,
      "step": 2229
    },
    {
      "epoch": 0.2113243307273158,
      "grad_norm": 3.5145037174224854,
      "learning_rate": 0.00023663413247417795,
      "loss": 0.2915,
      "step": 2230
    },
    {
      "epoch": 0.21141909500118455,
      "grad_norm": 4.492230415344238,
      "learning_rate": 0.00023660570453899364,
      "loss": 0.253,
      "step": 2231
    },
    {
      "epoch": 0.2115138592750533,
      "grad_norm": 0.905685305595398,
      "learning_rate": 0.0002365772766038093,
      "loss": 0.0801,
      "step": 2232
    },
    {
      "epoch": 0.21160862354892207,
      "grad_norm": 5.9113335609436035,
      "learning_rate": 0.000236548848668625,
      "loss": 0.2196,
      "step": 2233
    },
    {
      "epoch": 0.2117033878227908,
      "grad_norm": 2.1850695610046387,
      "learning_rate": 0.0002365204207334407,
      "loss": 0.3179,
      "step": 2234
    },
    {
      "epoch": 0.21179815209665956,
      "grad_norm": 1.1069082021713257,
      "learning_rate": 0.0002364919927982564,
      "loss": 0.0453,
      "step": 2235
    },
    {
      "epoch": 0.21189291637052832,
      "grad_norm": 0.6922561526298523,
      "learning_rate": 0.00023646356486307208,
      "loss": 0.0438,
      "step": 2236
    },
    {
      "epoch": 0.21198768064439708,
      "grad_norm": 5.446650505065918,
      "learning_rate": 0.0002364351369278878,
      "loss": 0.1101,
      "step": 2237
    },
    {
      "epoch": 0.2120824449182658,
      "grad_norm": 3.336009979248047,
      "learning_rate": 0.00023640670899270348,
      "loss": 0.1636,
      "step": 2238
    },
    {
      "epoch": 0.21217720919213456,
      "grad_norm": 4.15728759765625,
      "learning_rate": 0.00023637828105751917,
      "loss": 0.1516,
      "step": 2239
    },
    {
      "epoch": 0.21227197346600332,
      "grad_norm": 3.091614007949829,
      "learning_rate": 0.00023634985312233486,
      "loss": 0.145,
      "step": 2240
    },
    {
      "epoch": 0.21236673773987208,
      "grad_norm": 1.6335241794586182,
      "learning_rate": 0.00023632142518715055,
      "loss": 0.1748,
      "step": 2241
    },
    {
      "epoch": 0.2124615020137408,
      "grad_norm": 2.510633707046509,
      "learning_rate": 0.00023629299725196626,
      "loss": 0.0894,
      "step": 2242
    },
    {
      "epoch": 0.21255626628760957,
      "grad_norm": 2.659083127975464,
      "learning_rate": 0.00023626456931678195,
      "loss": 0.1466,
      "step": 2243
    },
    {
      "epoch": 0.21265103056147833,
      "grad_norm": 2.102715492248535,
      "learning_rate": 0.00023623614138159764,
      "loss": 0.1703,
      "step": 2244
    },
    {
      "epoch": 0.21274579483534709,
      "grad_norm": 0.6513950228691101,
      "learning_rate": 0.00023620771344641333,
      "loss": 0.0512,
      "step": 2245
    },
    {
      "epoch": 0.21284055910921582,
      "grad_norm": 1.3097513914108276,
      "learning_rate": 0.000236179285511229,
      "loss": 0.0376,
      "step": 2246
    },
    {
      "epoch": 0.21293532338308457,
      "grad_norm": 2.116706371307373,
      "learning_rate": 0.0002361508575760447,
      "loss": 0.0425,
      "step": 2247
    },
    {
      "epoch": 0.21303008765695333,
      "grad_norm": 6.42410945892334,
      "learning_rate": 0.0002361224296408604,
      "loss": 0.1527,
      "step": 2248
    },
    {
      "epoch": 0.2131248519308221,
      "grad_norm": 2.8176534175872803,
      "learning_rate": 0.00023609400170567608,
      "loss": 0.1335,
      "step": 2249
    },
    {
      "epoch": 0.21321961620469082,
      "grad_norm": 11.364990234375,
      "learning_rate": 0.00023606557377049177,
      "loss": 0.2295,
      "step": 2250
    },
    {
      "epoch": 0.21331438047855958,
      "grad_norm": 1.6916027069091797,
      "learning_rate": 0.00023603714583530748,
      "loss": 0.1532,
      "step": 2251
    },
    {
      "epoch": 0.21340914475242834,
      "grad_norm": 3.670957326889038,
      "learning_rate": 0.00023600871790012317,
      "loss": 0.2788,
      "step": 2252
    },
    {
      "epoch": 0.2135039090262971,
      "grad_norm": 9.152260780334473,
      "learning_rate": 0.00023598028996493886,
      "loss": 0.1978,
      "step": 2253
    },
    {
      "epoch": 0.21359867330016583,
      "grad_norm": 1.5518559217453003,
      "learning_rate": 0.00023595186202975455,
      "loss": 0.1386,
      "step": 2254
    },
    {
      "epoch": 0.21369343757403458,
      "grad_norm": 3.846029758453369,
      "learning_rate": 0.00023592343409457026,
      "loss": 0.082,
      "step": 2255
    },
    {
      "epoch": 0.21378820184790334,
      "grad_norm": 7.341555595397949,
      "learning_rate": 0.00023589500615938595,
      "loss": 0.2919,
      "step": 2256
    },
    {
      "epoch": 0.2138829661217721,
      "grad_norm": 3.6342947483062744,
      "learning_rate": 0.00023586657822420164,
      "loss": 0.1333,
      "step": 2257
    },
    {
      "epoch": 0.21397773039564083,
      "grad_norm": 4.964310646057129,
      "learning_rate": 0.00023583815028901733,
      "loss": 0.081,
      "step": 2258
    },
    {
      "epoch": 0.2140724946695096,
      "grad_norm": 2.1592140197753906,
      "learning_rate": 0.000235809722353833,
      "loss": 0.1472,
      "step": 2259
    },
    {
      "epoch": 0.21416725894337835,
      "grad_norm": 2.530770778656006,
      "learning_rate": 0.00023578129441864868,
      "loss": 0.1629,
      "step": 2260
    },
    {
      "epoch": 0.2142620232172471,
      "grad_norm": 1.3364909887313843,
      "learning_rate": 0.0002357528664834644,
      "loss": 0.0888,
      "step": 2261
    },
    {
      "epoch": 0.21435678749111584,
      "grad_norm": 1.2405668497085571,
      "learning_rate": 0.00023572443854828008,
      "loss": 0.0308,
      "step": 2262
    },
    {
      "epoch": 0.2144515517649846,
      "grad_norm": 16.142471313476562,
      "learning_rate": 0.00023569601061309577,
      "loss": 0.2935,
      "step": 2263
    },
    {
      "epoch": 0.21454631603885335,
      "grad_norm": 7.063518524169922,
      "learning_rate": 0.00023566758267791146,
      "loss": 0.1175,
      "step": 2264
    },
    {
      "epoch": 0.2146410803127221,
      "grad_norm": 6.151642322540283,
      "learning_rate": 0.00023563915474272718,
      "loss": 0.3046,
      "step": 2265
    },
    {
      "epoch": 0.21473584458659087,
      "grad_norm": 2.189377546310425,
      "learning_rate": 0.00023561072680754286,
      "loss": 0.1734,
      "step": 2266
    },
    {
      "epoch": 0.2148306088604596,
      "grad_norm": 4.619247913360596,
      "learning_rate": 0.00023558229887235855,
      "loss": 0.1027,
      "step": 2267
    },
    {
      "epoch": 0.21492537313432836,
      "grad_norm": 2.8693740367889404,
      "learning_rate": 0.00023555387093717424,
      "loss": 0.2551,
      "step": 2268
    },
    {
      "epoch": 0.21502013740819712,
      "grad_norm": 6.567137241363525,
      "learning_rate": 0.00023552544300198996,
      "loss": 0.4604,
      "step": 2269
    },
    {
      "epoch": 0.21511490168206587,
      "grad_norm": 3.3029448986053467,
      "learning_rate": 0.00023549701506680564,
      "loss": 0.1346,
      "step": 2270
    },
    {
      "epoch": 0.2152096659559346,
      "grad_norm": 1.8968299627304077,
      "learning_rate": 0.00023546858713162133,
      "loss": 0.179,
      "step": 2271
    },
    {
      "epoch": 0.21530443022980336,
      "grad_norm": 0.7072057127952576,
      "learning_rate": 0.000235440159196437,
      "loss": 0.0519,
      "step": 2272
    },
    {
      "epoch": 0.21539919450367212,
      "grad_norm": 5.976841926574707,
      "learning_rate": 0.00023541173126125268,
      "loss": 0.2832,
      "step": 2273
    },
    {
      "epoch": 0.21549395877754088,
      "grad_norm": 13.595430374145508,
      "learning_rate": 0.0002353833033260684,
      "loss": 0.2097,
      "step": 2274
    },
    {
      "epoch": 0.2155887230514096,
      "grad_norm": 1.2165647745132446,
      "learning_rate": 0.00023535487539088409,
      "loss": 0.0721,
      "step": 2275
    },
    {
      "epoch": 0.21568348732527837,
      "grad_norm": 1.3279271125793457,
      "learning_rate": 0.00023532644745569977,
      "loss": 0.1378,
      "step": 2276
    },
    {
      "epoch": 0.21577825159914713,
      "grad_norm": 7.153794288635254,
      "learning_rate": 0.00023529801952051546,
      "loss": 0.2902,
      "step": 2277
    },
    {
      "epoch": 0.21587301587301588,
      "grad_norm": 2.1500422954559326,
      "learning_rate": 0.00023526959158533115,
      "loss": 0.1556,
      "step": 2278
    },
    {
      "epoch": 0.2159677801468846,
      "grad_norm": 7.274971008300781,
      "learning_rate": 0.00023524116365014687,
      "loss": 0.1218,
      "step": 2279
    },
    {
      "epoch": 0.21606254442075337,
      "grad_norm": 6.866924285888672,
      "learning_rate": 0.00023521273571496255,
      "loss": 0.1729,
      "step": 2280
    },
    {
      "epoch": 0.21615730869462213,
      "grad_norm": 0.7490570545196533,
      "learning_rate": 0.00023518430777977824,
      "loss": 0.0266,
      "step": 2281
    },
    {
      "epoch": 0.2162520729684909,
      "grad_norm": 1.0247502326965332,
      "learning_rate": 0.00023515587984459393,
      "loss": 0.0873,
      "step": 2282
    },
    {
      "epoch": 0.21634683724235962,
      "grad_norm": 1.749420166015625,
      "learning_rate": 0.00023512745190940965,
      "loss": 0.1976,
      "step": 2283
    },
    {
      "epoch": 0.21644160151622838,
      "grad_norm": 4.752812385559082,
      "learning_rate": 0.00023509902397422534,
      "loss": 0.1813,
      "step": 2284
    },
    {
      "epoch": 0.21653636579009714,
      "grad_norm": 3.858980655670166,
      "learning_rate": 0.00023507059603904102,
      "loss": 0.2447,
      "step": 2285
    },
    {
      "epoch": 0.2166311300639659,
      "grad_norm": 2.1876604557037354,
      "learning_rate": 0.00023504216810385668,
      "loss": 0.1613,
      "step": 2286
    },
    {
      "epoch": 0.21672589433783462,
      "grad_norm": 2.5323169231414795,
      "learning_rate": 0.00023501374016867237,
      "loss": 0.0785,
      "step": 2287
    },
    {
      "epoch": 0.21682065861170338,
      "grad_norm": 2.015007495880127,
      "learning_rate": 0.0002349853122334881,
      "loss": 0.1876,
      "step": 2288
    },
    {
      "epoch": 0.21691542288557214,
      "grad_norm": 4.461079120635986,
      "learning_rate": 0.00023495688429830378,
      "loss": 0.2164,
      "step": 2289
    },
    {
      "epoch": 0.2170101871594409,
      "grad_norm": 5.322462558746338,
      "learning_rate": 0.00023492845636311947,
      "loss": 0.1817,
      "step": 2290
    },
    {
      "epoch": 0.21710495143330963,
      "grad_norm": 3.7448673248291016,
      "learning_rate": 0.00023490002842793515,
      "loss": 0.0879,
      "step": 2291
    },
    {
      "epoch": 0.2171997157071784,
      "grad_norm": 1.997585415840149,
      "learning_rate": 0.00023487160049275084,
      "loss": 0.0942,
      "step": 2292
    },
    {
      "epoch": 0.21729447998104715,
      "grad_norm": 2.5085794925689697,
      "learning_rate": 0.00023484317255756656,
      "loss": 0.1389,
      "step": 2293
    },
    {
      "epoch": 0.2173892442549159,
      "grad_norm": 1.477216362953186,
      "learning_rate": 0.00023481474462238225,
      "loss": 0.1319,
      "step": 2294
    },
    {
      "epoch": 0.21748400852878466,
      "grad_norm": 3.7566959857940674,
      "learning_rate": 0.00023478631668719793,
      "loss": 0.1191,
      "step": 2295
    },
    {
      "epoch": 0.2175787728026534,
      "grad_norm": 2.086115837097168,
      "learning_rate": 0.00023475788875201362,
      "loss": 0.1021,
      "step": 2296
    },
    {
      "epoch": 0.21767353707652215,
      "grad_norm": 5.10888147354126,
      "learning_rate": 0.00023472946081682934,
      "loss": 0.2179,
      "step": 2297
    },
    {
      "epoch": 0.2177683013503909,
      "grad_norm": 8.130593299865723,
      "learning_rate": 0.00023470103288164503,
      "loss": 0.3009,
      "step": 2298
    },
    {
      "epoch": 0.21786306562425967,
      "grad_norm": 3.2223596572875977,
      "learning_rate": 0.0002346726049464607,
      "loss": 0.0997,
      "step": 2299
    },
    {
      "epoch": 0.2179578298981284,
      "grad_norm": 3.722353219985962,
      "learning_rate": 0.00023464417701127638,
      "loss": 0.1624,
      "step": 2300
    },
    {
      "epoch": 0.21805259417199715,
      "grad_norm": 7.736639976501465,
      "learning_rate": 0.00023461574907609206,
      "loss": 0.2553,
      "step": 2301
    },
    {
      "epoch": 0.2181473584458659,
      "grad_norm": 1.5060700178146362,
      "learning_rate": 0.00023458732114090778,
      "loss": 0.1676,
      "step": 2302
    },
    {
      "epoch": 0.21824212271973467,
      "grad_norm": 3.6890382766723633,
      "learning_rate": 0.00023455889320572347,
      "loss": 0.091,
      "step": 2303
    },
    {
      "epoch": 0.2183368869936034,
      "grad_norm": 7.147850513458252,
      "learning_rate": 0.00023453046527053916,
      "loss": 0.3495,
      "step": 2304
    },
    {
      "epoch": 0.21843165126747216,
      "grad_norm": 8.929150581359863,
      "learning_rate": 0.00023450203733535485,
      "loss": 0.2928,
      "step": 2305
    },
    {
      "epoch": 0.21852641554134092,
      "grad_norm": 1.1582229137420654,
      "learning_rate": 0.00023447360940017056,
      "loss": 0.0894,
      "step": 2306
    },
    {
      "epoch": 0.21862117981520968,
      "grad_norm": 1.8816418647766113,
      "learning_rate": 0.00023444518146498625,
      "loss": 0.1436,
      "step": 2307
    },
    {
      "epoch": 0.2187159440890784,
      "grad_norm": 1.6734352111816406,
      "learning_rate": 0.00023441675352980194,
      "loss": 0.1204,
      "step": 2308
    },
    {
      "epoch": 0.21881070836294716,
      "grad_norm": 6.193562984466553,
      "learning_rate": 0.00023438832559461763,
      "loss": 0.1757,
      "step": 2309
    },
    {
      "epoch": 0.21890547263681592,
      "grad_norm": 3.9489827156066895,
      "learning_rate": 0.00023435989765943331,
      "loss": 0.1819,
      "step": 2310
    },
    {
      "epoch": 0.21900023691068468,
      "grad_norm": 11.049661636352539,
      "learning_rate": 0.00023433146972424903,
      "loss": 0.2421,
      "step": 2311
    },
    {
      "epoch": 0.2190950011845534,
      "grad_norm": 2.4013512134552,
      "learning_rate": 0.00023430304178906472,
      "loss": 0.2519,
      "step": 2312
    },
    {
      "epoch": 0.21918976545842217,
      "grad_norm": 1.4255074262619019,
      "learning_rate": 0.00023427461385388038,
      "loss": 0.1051,
      "step": 2313
    },
    {
      "epoch": 0.21928452973229093,
      "grad_norm": 3.8384976387023926,
      "learning_rate": 0.00023424618591869607,
      "loss": 0.123,
      "step": 2314
    },
    {
      "epoch": 0.2193792940061597,
      "grad_norm": 6.364903450012207,
      "learning_rate": 0.00023421775798351176,
      "loss": 0.168,
      "step": 2315
    },
    {
      "epoch": 0.21947405828002842,
      "grad_norm": 2.62722110748291,
      "learning_rate": 0.00023418933004832747,
      "loss": 0.118,
      "step": 2316
    },
    {
      "epoch": 0.21956882255389717,
      "grad_norm": 3.931252956390381,
      "learning_rate": 0.00023416090211314316,
      "loss": 0.1008,
      "step": 2317
    },
    {
      "epoch": 0.21966358682776593,
      "grad_norm": 9.086315155029297,
      "learning_rate": 0.00023413247417795885,
      "loss": 0.4188,
      "step": 2318
    },
    {
      "epoch": 0.2197583511016347,
      "grad_norm": 0.47641515731811523,
      "learning_rate": 0.00023410404624277454,
      "loss": 0.0306,
      "step": 2319
    },
    {
      "epoch": 0.21985311537550342,
      "grad_norm": 8.387845039367676,
      "learning_rate": 0.00023407561830759025,
      "loss": 0.4241,
      "step": 2320
    },
    {
      "epoch": 0.21994787964937218,
      "grad_norm": 4.333334445953369,
      "learning_rate": 0.00023404719037240594,
      "loss": 0.1782,
      "step": 2321
    },
    {
      "epoch": 0.22004264392324094,
      "grad_norm": 2.999760866165161,
      "learning_rate": 0.00023401876243722163,
      "loss": 0.1679,
      "step": 2322
    },
    {
      "epoch": 0.2201374081971097,
      "grad_norm": 7.952483177185059,
      "learning_rate": 0.00023399033450203732,
      "loss": 0.3431,
      "step": 2323
    },
    {
      "epoch": 0.22023217247097845,
      "grad_norm": 2.6916608810424805,
      "learning_rate": 0.00023396190656685303,
      "loss": 0.1172,
      "step": 2324
    },
    {
      "epoch": 0.22032693674484718,
      "grad_norm": 2.479733943939209,
      "learning_rate": 0.00023393347863166872,
      "loss": 0.1561,
      "step": 2325
    },
    {
      "epoch": 0.22042170101871594,
      "grad_norm": 2.4271128177642822,
      "learning_rate": 0.00023390505069648438,
      "loss": 0.1582,
      "step": 2326
    },
    {
      "epoch": 0.2205164652925847,
      "grad_norm": 0.718731164932251,
      "learning_rate": 0.00023387662276130007,
      "loss": 0.049,
      "step": 2327
    },
    {
      "epoch": 0.22061122956645346,
      "grad_norm": 0.768561065196991,
      "learning_rate": 0.00023384819482611576,
      "loss": 0.0498,
      "step": 2328
    },
    {
      "epoch": 0.2207059938403222,
      "grad_norm": 1.0491613149642944,
      "learning_rate": 0.00023381976689093145,
      "loss": 0.0339,
      "step": 2329
    },
    {
      "epoch": 0.22080075811419095,
      "grad_norm": 2.2335169315338135,
      "learning_rate": 0.00023379133895574716,
      "loss": 0.1284,
      "step": 2330
    },
    {
      "epoch": 0.2208955223880597,
      "grad_norm": 1.932210087776184,
      "learning_rate": 0.00023376291102056285,
      "loss": 0.1377,
      "step": 2331
    },
    {
      "epoch": 0.22099028666192846,
      "grad_norm": 1.5542362928390503,
      "learning_rate": 0.00023373448308537854,
      "loss": 0.0274,
      "step": 2332
    },
    {
      "epoch": 0.2210850509357972,
      "grad_norm": 2.27462100982666,
      "learning_rate": 0.00023370605515019423,
      "loss": 0.121,
      "step": 2333
    },
    {
      "epoch": 0.22117981520966595,
      "grad_norm": 3.29945969581604,
      "learning_rate": 0.00023367762721500994,
      "loss": 0.1147,
      "step": 2334
    },
    {
      "epoch": 0.2212745794835347,
      "grad_norm": 1.820317029953003,
      "learning_rate": 0.00023364919927982563,
      "loss": 0.0706,
      "step": 2335
    },
    {
      "epoch": 0.22136934375740347,
      "grad_norm": 1.5868667364120483,
      "learning_rate": 0.00023362077134464132,
      "loss": 0.0559,
      "step": 2336
    },
    {
      "epoch": 0.2214641080312722,
      "grad_norm": 5.13955545425415,
      "learning_rate": 0.000233592343409457,
      "loss": 0.2445,
      "step": 2337
    },
    {
      "epoch": 0.22155887230514096,
      "grad_norm": 2.076984405517578,
      "learning_rate": 0.00023356391547427272,
      "loss": 0.0918,
      "step": 2338
    },
    {
      "epoch": 0.22165363657900972,
      "grad_norm": 7.1615190505981445,
      "learning_rate": 0.0002335354875390884,
      "loss": 0.1062,
      "step": 2339
    },
    {
      "epoch": 0.22174840085287847,
      "grad_norm": 1.3352199792861938,
      "learning_rate": 0.00023350705960390407,
      "loss": 0.1002,
      "step": 2340
    },
    {
      "epoch": 0.2218431651267472,
      "grad_norm": 8.53560733795166,
      "learning_rate": 0.00023347863166871976,
      "loss": 0.2865,
      "step": 2341
    },
    {
      "epoch": 0.22193792940061596,
      "grad_norm": 2.221219301223755,
      "learning_rate": 0.00023345020373353545,
      "loss": 0.0683,
      "step": 2342
    },
    {
      "epoch": 0.22203269367448472,
      "grad_norm": 3.1495938301086426,
      "learning_rate": 0.00023342177579835114,
      "loss": 0.2001,
      "step": 2343
    },
    {
      "epoch": 0.22212745794835348,
      "grad_norm": 0.29768991470336914,
      "learning_rate": 0.00023339334786316685,
      "loss": 0.0279,
      "step": 2344
    },
    {
      "epoch": 0.2222222222222222,
      "grad_norm": 1.9783954620361328,
      "learning_rate": 0.00023336491992798254,
      "loss": 0.1458,
      "step": 2345
    },
    {
      "epoch": 0.22231698649609097,
      "grad_norm": 2.4135897159576416,
      "learning_rate": 0.00023333649199279823,
      "loss": 0.0908,
      "step": 2346
    },
    {
      "epoch": 0.22241175076995973,
      "grad_norm": 8.55605411529541,
      "learning_rate": 0.00023330806405761392,
      "loss": 0.3343,
      "step": 2347
    },
    {
      "epoch": 0.22250651504382848,
      "grad_norm": 5.498420715332031,
      "learning_rate": 0.00023327963612242963,
      "loss": 0.4071,
      "step": 2348
    },
    {
      "epoch": 0.22260127931769721,
      "grad_norm": 7.549142837524414,
      "learning_rate": 0.00023325120818724532,
      "loss": 0.1931,
      "step": 2349
    },
    {
      "epoch": 0.22269604359156597,
      "grad_norm": 2.1656956672668457,
      "learning_rate": 0.000233222780252061,
      "loss": 0.0918,
      "step": 2350
    },
    {
      "epoch": 0.22279080786543473,
      "grad_norm": 3.086603879928589,
      "learning_rate": 0.0002331943523168767,
      "loss": 0.2856,
      "step": 2351
    },
    {
      "epoch": 0.2228855721393035,
      "grad_norm": 4.100269317626953,
      "learning_rate": 0.00023316592438169241,
      "loss": 0.2952,
      "step": 2352
    },
    {
      "epoch": 0.22298033641317225,
      "grad_norm": 2.4454896450042725,
      "learning_rate": 0.00023313749644650808,
      "loss": 0.099,
      "step": 2353
    },
    {
      "epoch": 0.22307510068704098,
      "grad_norm": 1.6467429399490356,
      "learning_rate": 0.00023310906851132376,
      "loss": 0.0842,
      "step": 2354
    },
    {
      "epoch": 0.22316986496090974,
      "grad_norm": 1.5998468399047852,
      "learning_rate": 0.00023308064057613945,
      "loss": 0.0816,
      "step": 2355
    },
    {
      "epoch": 0.2232646292347785,
      "grad_norm": 4.038638114929199,
      "learning_rate": 0.00023305221264095514,
      "loss": 0.1541,
      "step": 2356
    },
    {
      "epoch": 0.22335939350864725,
      "grad_norm": 5.814206600189209,
      "learning_rate": 0.00023302378470577086,
      "loss": 0.2068,
      "step": 2357
    },
    {
      "epoch": 0.22345415778251598,
      "grad_norm": 5.161970615386963,
      "learning_rate": 0.00023299535677058654,
      "loss": 0.1305,
      "step": 2358
    },
    {
      "epoch": 0.22354892205638474,
      "grad_norm": 6.480769157409668,
      "learning_rate": 0.00023296692883540223,
      "loss": 0.1633,
      "step": 2359
    },
    {
      "epoch": 0.2236436863302535,
      "grad_norm": 3.853437662124634,
      "learning_rate": 0.00023293850090021792,
      "loss": 0.1258,
      "step": 2360
    },
    {
      "epoch": 0.22373845060412226,
      "grad_norm": 2.1798479557037354,
      "learning_rate": 0.0002329100729650336,
      "loss": 0.1657,
      "step": 2361
    },
    {
      "epoch": 0.223833214877991,
      "grad_norm": 2.98659348487854,
      "learning_rate": 0.00023288164502984933,
      "loss": 0.2982,
      "step": 2362
    },
    {
      "epoch": 0.22392797915185975,
      "grad_norm": 3.812638521194458,
      "learning_rate": 0.00023285321709466501,
      "loss": 0.0381,
      "step": 2363
    },
    {
      "epoch": 0.2240227434257285,
      "grad_norm": 5.392037868499756,
      "learning_rate": 0.0002328247891594807,
      "loss": 0.2561,
      "step": 2364
    },
    {
      "epoch": 0.22411750769959726,
      "grad_norm": 4.804625034332275,
      "learning_rate": 0.0002327963612242964,
      "loss": 0.1336,
      "step": 2365
    },
    {
      "epoch": 0.224212271973466,
      "grad_norm": 3.7026541233062744,
      "learning_rate": 0.0002327679332891121,
      "loss": 0.1238,
      "step": 2366
    },
    {
      "epoch": 0.22430703624733475,
      "grad_norm": 4.921585559844971,
      "learning_rate": 0.00023273950535392777,
      "loss": 0.5159,
      "step": 2367
    },
    {
      "epoch": 0.2244018005212035,
      "grad_norm": 2.198404550552368,
      "learning_rate": 0.00023271107741874346,
      "loss": 0.1464,
      "step": 2368
    },
    {
      "epoch": 0.22449656479507227,
      "grad_norm": 9.928166389465332,
      "learning_rate": 0.00023268264948355914,
      "loss": 0.1413,
      "step": 2369
    },
    {
      "epoch": 0.224591329068941,
      "grad_norm": 1.1715115308761597,
      "learning_rate": 0.00023265422154837483,
      "loss": 0.0797,
      "step": 2370
    },
    {
      "epoch": 0.22468609334280976,
      "grad_norm": 4.9438371658325195,
      "learning_rate": 0.00023262579361319055,
      "loss": 0.1838,
      "step": 2371
    },
    {
      "epoch": 0.22478085761667851,
      "grad_norm": 7.72096061706543,
      "learning_rate": 0.00023259736567800624,
      "loss": 0.095,
      "step": 2372
    },
    {
      "epoch": 0.22487562189054727,
      "grad_norm": 1.6750715970993042,
      "learning_rate": 0.00023256893774282192,
      "loss": 0.1686,
      "step": 2373
    },
    {
      "epoch": 0.224970386164416,
      "grad_norm": 4.121943473815918,
      "learning_rate": 0.0002325405098076376,
      "loss": 0.1558,
      "step": 2374
    },
    {
      "epoch": 0.22506515043828476,
      "grad_norm": 2.9920096397399902,
      "learning_rate": 0.00023251208187245333,
      "loss": 0.1524,
      "step": 2375
    },
    {
      "epoch": 0.22515991471215352,
      "grad_norm": 1.8381261825561523,
      "learning_rate": 0.00023248365393726902,
      "loss": 0.1095,
      "step": 2376
    },
    {
      "epoch": 0.22525467898602228,
      "grad_norm": 1.55513596534729,
      "learning_rate": 0.0002324552260020847,
      "loss": 0.0519,
      "step": 2377
    },
    {
      "epoch": 0.225349443259891,
      "grad_norm": 1.210702896118164,
      "learning_rate": 0.0002324267980669004,
      "loss": 0.1379,
      "step": 2378
    },
    {
      "epoch": 0.22544420753375977,
      "grad_norm": 2.0452866554260254,
      "learning_rate": 0.00023239837013171608,
      "loss": 0.124,
      "step": 2379
    },
    {
      "epoch": 0.22553897180762852,
      "grad_norm": 5.961760997772217,
      "learning_rate": 0.00023236994219653174,
      "loss": 0.1627,
      "step": 2380
    },
    {
      "epoch": 0.22563373608149728,
      "grad_norm": 3.6934680938720703,
      "learning_rate": 0.00023234151426134746,
      "loss": 0.1763,
      "step": 2381
    },
    {
      "epoch": 0.22572850035536604,
      "grad_norm": 2.980687379837036,
      "learning_rate": 0.00023231308632616315,
      "loss": 0.0226,
      "step": 2382
    },
    {
      "epoch": 0.22582326462923477,
      "grad_norm": 4.233932018280029,
      "learning_rate": 0.00023228465839097884,
      "loss": 0.1101,
      "step": 2383
    },
    {
      "epoch": 0.22591802890310353,
      "grad_norm": 10.032316207885742,
      "learning_rate": 0.00023225623045579452,
      "loss": 0.1645,
      "step": 2384
    },
    {
      "epoch": 0.2260127931769723,
      "grad_norm": 12.894977569580078,
      "learning_rate": 0.00023222780252061024,
      "loss": 0.1869,
      "step": 2385
    },
    {
      "epoch": 0.22610755745084105,
      "grad_norm": 6.74311637878418,
      "learning_rate": 0.00023219937458542593,
      "loss": 0.2637,
      "step": 2386
    },
    {
      "epoch": 0.22620232172470978,
      "grad_norm": 5.7770161628723145,
      "learning_rate": 0.00023217094665024162,
      "loss": 0.0884,
      "step": 2387
    },
    {
      "epoch": 0.22629708599857853,
      "grad_norm": 2.0787243843078613,
      "learning_rate": 0.0002321425187150573,
      "loss": 0.1564,
      "step": 2388
    },
    {
      "epoch": 0.2263918502724473,
      "grad_norm": 2.0366148948669434,
      "learning_rate": 0.00023211409077987302,
      "loss": 0.1234,
      "step": 2389
    },
    {
      "epoch": 0.22648661454631605,
      "grad_norm": 4.230295181274414,
      "learning_rate": 0.0002320856628446887,
      "loss": 0.1176,
      "step": 2390
    },
    {
      "epoch": 0.22658137882018478,
      "grad_norm": 1.681517243385315,
      "learning_rate": 0.0002320572349095044,
      "loss": 0.1588,
      "step": 2391
    },
    {
      "epoch": 0.22667614309405354,
      "grad_norm": 3.6234543323516846,
      "learning_rate": 0.00023202880697432008,
      "loss": 0.1543,
      "step": 2392
    },
    {
      "epoch": 0.2267709073679223,
      "grad_norm": 1.7734249830245972,
      "learning_rate": 0.00023200037903913577,
      "loss": 0.2471,
      "step": 2393
    },
    {
      "epoch": 0.22686567164179106,
      "grad_norm": 1.384371042251587,
      "learning_rate": 0.00023197195110395143,
      "loss": 0.0824,
      "step": 2394
    },
    {
      "epoch": 0.2269604359156598,
      "grad_norm": 2.3298330307006836,
      "learning_rate": 0.00023194352316876715,
      "loss": 0.197,
      "step": 2395
    },
    {
      "epoch": 0.22705520018952854,
      "grad_norm": 2.72511887550354,
      "learning_rate": 0.00023191509523358284,
      "loss": 0.0442,
      "step": 2396
    },
    {
      "epoch": 0.2271499644633973,
      "grad_norm": 3.6813623905181885,
      "learning_rate": 0.00023188666729839853,
      "loss": 0.1628,
      "step": 2397
    },
    {
      "epoch": 0.22724472873726606,
      "grad_norm": 3.182386636734009,
      "learning_rate": 0.00023185823936321421,
      "loss": 0.1859,
      "step": 2398
    },
    {
      "epoch": 0.2273394930111348,
      "grad_norm": 6.942875862121582,
      "learning_rate": 0.00023182981142802993,
      "loss": 0.2399,
      "step": 2399
    },
    {
      "epoch": 0.22743425728500355,
      "grad_norm": 3.002751588821411,
      "learning_rate": 0.00023180138349284562,
      "loss": 0.2199,
      "step": 2400
    },
    {
      "epoch": 0.2275290215588723,
      "grad_norm": 7.150964736938477,
      "learning_rate": 0.0002317729555576613,
      "loss": 0.2321,
      "step": 2401
    },
    {
      "epoch": 0.22762378583274107,
      "grad_norm": 1.125391960144043,
      "learning_rate": 0.000231744527622477,
      "loss": 0.0543,
      "step": 2402
    },
    {
      "epoch": 0.2277185501066098,
      "grad_norm": 2.2352101802825928,
      "learning_rate": 0.0002317160996872927,
      "loss": 0.1715,
      "step": 2403
    },
    {
      "epoch": 0.22781331438047855,
      "grad_norm": 2.1780223846435547,
      "learning_rate": 0.0002316876717521084,
      "loss": 0.1112,
      "step": 2404
    },
    {
      "epoch": 0.2279080786543473,
      "grad_norm": 1.915960431098938,
      "learning_rate": 0.0002316592438169241,
      "loss": 0.075,
      "step": 2405
    },
    {
      "epoch": 0.22800284292821607,
      "grad_norm": 3.040227174758911,
      "learning_rate": 0.00023163081588173978,
      "loss": 0.2144,
      "step": 2406
    },
    {
      "epoch": 0.2280976072020848,
      "grad_norm": 6.960188865661621,
      "learning_rate": 0.00023160238794655544,
      "loss": 0.2376,
      "step": 2407
    },
    {
      "epoch": 0.22819237147595356,
      "grad_norm": 5.974197864532471,
      "learning_rate": 0.00023157396001137115,
      "loss": 0.0911,
      "step": 2408
    },
    {
      "epoch": 0.22828713574982232,
      "grad_norm": 15.30473518371582,
      "learning_rate": 0.00023154553207618684,
      "loss": 0.2524,
      "step": 2409
    },
    {
      "epoch": 0.22838190002369108,
      "grad_norm": 6.43180513381958,
      "learning_rate": 0.00023151710414100253,
      "loss": 0.297,
      "step": 2410
    },
    {
      "epoch": 0.2284766642975598,
      "grad_norm": 6.085509777069092,
      "learning_rate": 0.00023148867620581822,
      "loss": 0.2841,
      "step": 2411
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 12.203332901000977,
      "learning_rate": 0.0002314602482706339,
      "loss": 0.3607,
      "step": 2412
    },
    {
      "epoch": 0.22866619284529732,
      "grad_norm": 3.104968547821045,
      "learning_rate": 0.00023143182033544962,
      "loss": 0.0742,
      "step": 2413
    },
    {
      "epoch": 0.22876095711916608,
      "grad_norm": 2.6696677207946777,
      "learning_rate": 0.0002314033924002653,
      "loss": 0.1447,
      "step": 2414
    },
    {
      "epoch": 0.22885572139303484,
      "grad_norm": 3.30178165435791,
      "learning_rate": 0.000231374964465081,
      "loss": 0.203,
      "step": 2415
    },
    {
      "epoch": 0.22895048566690357,
      "grad_norm": 11.440223693847656,
      "learning_rate": 0.0002313465365298967,
      "loss": 0.3687,
      "step": 2416
    },
    {
      "epoch": 0.22904524994077233,
      "grad_norm": 2.0543479919433594,
      "learning_rate": 0.0002313181085947124,
      "loss": 0.1161,
      "step": 2417
    },
    {
      "epoch": 0.22914001421464109,
      "grad_norm": 2.435515880584717,
      "learning_rate": 0.0002312896806595281,
      "loss": 0.131,
      "step": 2418
    },
    {
      "epoch": 0.22923477848850984,
      "grad_norm": 1.6157466173171997,
      "learning_rate": 0.00023126125272434378,
      "loss": 0.1142,
      "step": 2419
    },
    {
      "epoch": 0.22932954276237857,
      "grad_norm": 4.163680553436279,
      "learning_rate": 0.00023123282478915947,
      "loss": 0.1067,
      "step": 2420
    },
    {
      "epoch": 0.22942430703624733,
      "grad_norm": 1.6393578052520752,
      "learning_rate": 0.00023120439685397513,
      "loss": 0.1468,
      "step": 2421
    },
    {
      "epoch": 0.2295190713101161,
      "grad_norm": 2.5253920555114746,
      "learning_rate": 0.00023117596891879084,
      "loss": 0.2077,
      "step": 2422
    },
    {
      "epoch": 0.22961383558398485,
      "grad_norm": 1.6734932661056519,
      "learning_rate": 0.00023114754098360653,
      "loss": 0.0442,
      "step": 2423
    },
    {
      "epoch": 0.22970859985785358,
      "grad_norm": 2.2622711658477783,
      "learning_rate": 0.00023111911304842222,
      "loss": 0.0733,
      "step": 2424
    },
    {
      "epoch": 0.22980336413172234,
      "grad_norm": 1.4104803800582886,
      "learning_rate": 0.0002310906851132379,
      "loss": 0.0587,
      "step": 2425
    },
    {
      "epoch": 0.2298981284055911,
      "grad_norm": 1.9248766899108887,
      "learning_rate": 0.00023106225717805362,
      "loss": 0.0934,
      "step": 2426
    },
    {
      "epoch": 0.22999289267945985,
      "grad_norm": 1.6896567344665527,
      "learning_rate": 0.0002310338292428693,
      "loss": 0.0916,
      "step": 2427
    },
    {
      "epoch": 0.23008765695332858,
      "grad_norm": 2.4044549465179443,
      "learning_rate": 0.000231005401307685,
      "loss": 0.1151,
      "step": 2428
    },
    {
      "epoch": 0.23018242122719734,
      "grad_norm": 4.328292369842529,
      "learning_rate": 0.0002309769733725007,
      "loss": 0.0591,
      "step": 2429
    },
    {
      "epoch": 0.2302771855010661,
      "grad_norm": 4.946730613708496,
      "learning_rate": 0.00023094854543731638,
      "loss": 0.27,
      "step": 2430
    },
    {
      "epoch": 0.23037194977493486,
      "grad_norm": 1.172900915145874,
      "learning_rate": 0.0002309201175021321,
      "loss": 0.0263,
      "step": 2431
    },
    {
      "epoch": 0.2304667140488036,
      "grad_norm": 3.8219990730285645,
      "learning_rate": 0.00023089168956694778,
      "loss": 0.0771,
      "step": 2432
    },
    {
      "epoch": 0.23056147832267235,
      "grad_norm": 1.0137895345687866,
      "learning_rate": 0.00023086326163176347,
      "loss": 0.043,
      "step": 2433
    },
    {
      "epoch": 0.2306562425965411,
      "grad_norm": 6.061826705932617,
      "learning_rate": 0.00023083483369657913,
      "loss": 0.1449,
      "step": 2434
    },
    {
      "epoch": 0.23075100687040986,
      "grad_norm": 1.2662802934646606,
      "learning_rate": 0.00023080640576139482,
      "loss": 0.0481,
      "step": 2435
    },
    {
      "epoch": 0.2308457711442786,
      "grad_norm": 2.3168115615844727,
      "learning_rate": 0.00023077797782621054,
      "loss": 0.0552,
      "step": 2436
    },
    {
      "epoch": 0.23094053541814735,
      "grad_norm": 2.541400909423828,
      "learning_rate": 0.00023074954989102622,
      "loss": 0.0821,
      "step": 2437
    },
    {
      "epoch": 0.2310352996920161,
      "grad_norm": 10.170821189880371,
      "learning_rate": 0.0002307211219558419,
      "loss": 0.2864,
      "step": 2438
    },
    {
      "epoch": 0.23113006396588487,
      "grad_norm": 3.139158248901367,
      "learning_rate": 0.0002306926940206576,
      "loss": 0.1067,
      "step": 2439
    },
    {
      "epoch": 0.2312248282397536,
      "grad_norm": 1.2547353506088257,
      "learning_rate": 0.00023066426608547332,
      "loss": 0.0401,
      "step": 2440
    },
    {
      "epoch": 0.23131959251362236,
      "grad_norm": 2.515352725982666,
      "learning_rate": 0.000230635838150289,
      "loss": 0.0849,
      "step": 2441
    },
    {
      "epoch": 0.23141435678749112,
      "grad_norm": 2.456120014190674,
      "learning_rate": 0.0002306074102151047,
      "loss": 0.0416,
      "step": 2442
    },
    {
      "epoch": 0.23150912106135987,
      "grad_norm": 1.2678614854812622,
      "learning_rate": 0.00023057898227992038,
      "loss": 0.0135,
      "step": 2443
    },
    {
      "epoch": 0.23160388533522863,
      "grad_norm": 5.617635250091553,
      "learning_rate": 0.00023055055434473607,
      "loss": 0.2949,
      "step": 2444
    },
    {
      "epoch": 0.23169864960909736,
      "grad_norm": 4.079911231994629,
      "learning_rate": 0.00023052212640955178,
      "loss": 0.0278,
      "step": 2445
    },
    {
      "epoch": 0.23179341388296612,
      "grad_norm": 11.171154975891113,
      "learning_rate": 0.00023049369847436747,
      "loss": 0.0885,
      "step": 2446
    },
    {
      "epoch": 0.23188817815683488,
      "grad_norm": 1.5664403438568115,
      "learning_rate": 0.00023046527053918316,
      "loss": 0.0313,
      "step": 2447
    },
    {
      "epoch": 0.23198294243070364,
      "grad_norm": 1.710559368133545,
      "learning_rate": 0.00023043684260399882,
      "loss": 0.0583,
      "step": 2448
    },
    {
      "epoch": 0.23207770670457237,
      "grad_norm": 5.85088586807251,
      "learning_rate": 0.0002304084146688145,
      "loss": 0.2562,
      "step": 2449
    },
    {
      "epoch": 0.23217247097844113,
      "grad_norm": 13.048048973083496,
      "learning_rate": 0.00023037998673363023,
      "loss": 0.2293,
      "step": 2450
    },
    {
      "epoch": 0.23226723525230988,
      "grad_norm": 10.405137062072754,
      "learning_rate": 0.00023035155879844591,
      "loss": 0.1209,
      "step": 2451
    },
    {
      "epoch": 0.23236199952617864,
      "grad_norm": 5.8400702476501465,
      "learning_rate": 0.0002303231308632616,
      "loss": 0.1549,
      "step": 2452
    },
    {
      "epoch": 0.23245676380004737,
      "grad_norm": 3.930177927017212,
      "learning_rate": 0.0002302947029280773,
      "loss": 0.1476,
      "step": 2453
    },
    {
      "epoch": 0.23255152807391613,
      "grad_norm": 2.2865233421325684,
      "learning_rate": 0.000230266274992893,
      "loss": 0.1127,
      "step": 2454
    },
    {
      "epoch": 0.2326462923477849,
      "grad_norm": 2.732290506362915,
      "learning_rate": 0.0002302378470577087,
      "loss": 0.1222,
      "step": 2455
    },
    {
      "epoch": 0.23274105662165365,
      "grad_norm": 14.237829208374023,
      "learning_rate": 0.00023020941912252438,
      "loss": 0.2371,
      "step": 2456
    },
    {
      "epoch": 0.23283582089552238,
      "grad_norm": 4.144468307495117,
      "learning_rate": 0.00023018099118734007,
      "loss": 0.2714,
      "step": 2457
    },
    {
      "epoch": 0.23293058516939114,
      "grad_norm": 3.9230544567108154,
      "learning_rate": 0.0002301525632521558,
      "loss": 0.1462,
      "step": 2458
    },
    {
      "epoch": 0.2330253494432599,
      "grad_norm": 1.1522194147109985,
      "learning_rate": 0.00023012413531697148,
      "loss": 0.0416,
      "step": 2459
    },
    {
      "epoch": 0.23312011371712865,
      "grad_norm": 5.664870738983154,
      "learning_rate": 0.00023009570738178716,
      "loss": 0.0812,
      "step": 2460
    },
    {
      "epoch": 0.23321487799099738,
      "grad_norm": 2.199653387069702,
      "learning_rate": 0.00023006727944660283,
      "loss": 0.1615,
      "step": 2461
    },
    {
      "epoch": 0.23330964226486614,
      "grad_norm": 3.299891233444214,
      "learning_rate": 0.00023003885151141851,
      "loss": 0.2306,
      "step": 2462
    },
    {
      "epoch": 0.2334044065387349,
      "grad_norm": 3.6195566654205322,
      "learning_rate": 0.0002300104235762342,
      "loss": 0.3268,
      "step": 2463
    },
    {
      "epoch": 0.23349917081260366,
      "grad_norm": 8.650832176208496,
      "learning_rate": 0.00022998199564104992,
      "loss": 0.1505,
      "step": 2464
    },
    {
      "epoch": 0.2335939350864724,
      "grad_norm": 5.842200756072998,
      "learning_rate": 0.0002299535677058656,
      "loss": 0.077,
      "step": 2465
    },
    {
      "epoch": 0.23368869936034115,
      "grad_norm": 5.954631805419922,
      "learning_rate": 0.0002299251397706813,
      "loss": 0.0878,
      "step": 2466
    },
    {
      "epoch": 0.2337834636342099,
      "grad_norm": 3.979473352432251,
      "learning_rate": 0.00022989671183549698,
      "loss": 0.1361,
      "step": 2467
    },
    {
      "epoch": 0.23387822790807866,
      "grad_norm": 1.6242599487304688,
      "learning_rate": 0.0002298682839003127,
      "loss": 0.1241,
      "step": 2468
    },
    {
      "epoch": 0.2339729921819474,
      "grad_norm": 2.7021284103393555,
      "learning_rate": 0.0002298398559651284,
      "loss": 0.1165,
      "step": 2469
    },
    {
      "epoch": 0.23406775645581615,
      "grad_norm": 2.3449010848999023,
      "learning_rate": 0.00022981142802994407,
      "loss": 0.1705,
      "step": 2470
    },
    {
      "epoch": 0.2341625207296849,
      "grad_norm": 8.2062406539917,
      "learning_rate": 0.00022978300009475976,
      "loss": 0.1419,
      "step": 2471
    },
    {
      "epoch": 0.23425728500355367,
      "grad_norm": 7.411388874053955,
      "learning_rate": 0.00022975457215957548,
      "loss": 0.1033,
      "step": 2472
    },
    {
      "epoch": 0.23435204927742243,
      "grad_norm": 3.178953170776367,
      "learning_rate": 0.00022972614422439117,
      "loss": 0.2013,
      "step": 2473
    },
    {
      "epoch": 0.23444681355129116,
      "grad_norm": 3.6833529472351074,
      "learning_rate": 0.00022969771628920686,
      "loss": 0.1112,
      "step": 2474
    },
    {
      "epoch": 0.2345415778251599,
      "grad_norm": 5.1651129722595215,
      "learning_rate": 0.00022966928835402252,
      "loss": 0.0697,
      "step": 2475
    },
    {
      "epoch": 0.23463634209902867,
      "grad_norm": 7.196262836456299,
      "learning_rate": 0.0002296408604188382,
      "loss": 0.2526,
      "step": 2476
    },
    {
      "epoch": 0.23473110637289743,
      "grad_norm": 3.318997383117676,
      "learning_rate": 0.00022961243248365392,
      "loss": 0.248,
      "step": 2477
    },
    {
      "epoch": 0.23482587064676616,
      "grad_norm": 2.601226806640625,
      "learning_rate": 0.0002295840045484696,
      "loss": 0.0888,
      "step": 2478
    },
    {
      "epoch": 0.23492063492063492,
      "grad_norm": 2.3949668407440186,
      "learning_rate": 0.0002295555766132853,
      "loss": 0.0915,
      "step": 2479
    },
    {
      "epoch": 0.23501539919450368,
      "grad_norm": 5.8531036376953125,
      "learning_rate": 0.00022952714867810099,
      "loss": 0.0776,
      "step": 2480
    },
    {
      "epoch": 0.23511016346837244,
      "grad_norm": 1.488684892654419,
      "learning_rate": 0.00022949872074291667,
      "loss": 0.0749,
      "step": 2481
    },
    {
      "epoch": 0.23520492774224117,
      "grad_norm": 4.843082904815674,
      "learning_rate": 0.0002294702928077324,
      "loss": 0.1032,
      "step": 2482
    },
    {
      "epoch": 0.23529969201610992,
      "grad_norm": 4.25240421295166,
      "learning_rate": 0.00022944186487254808,
      "loss": 0.202,
      "step": 2483
    },
    {
      "epoch": 0.23539445628997868,
      "grad_norm": 6.414634704589844,
      "learning_rate": 0.00022941343693736377,
      "loss": 0.2673,
      "step": 2484
    },
    {
      "epoch": 0.23548922056384744,
      "grad_norm": 11.04985237121582,
      "learning_rate": 0.00022938500900217945,
      "loss": 0.1292,
      "step": 2485
    },
    {
      "epoch": 0.23558398483771617,
      "grad_norm": 11.266389846801758,
      "learning_rate": 0.00022935658106699517,
      "loss": 0.3955,
      "step": 2486
    },
    {
      "epoch": 0.23567874911158493,
      "grad_norm": 3.3451309204101562,
      "learning_rate": 0.00022932815313181086,
      "loss": 0.1413,
      "step": 2487
    },
    {
      "epoch": 0.2357735133854537,
      "grad_norm": 4.48507022857666,
      "learning_rate": 0.00022929972519662652,
      "loss": 0.3235,
      "step": 2488
    },
    {
      "epoch": 0.23586827765932245,
      "grad_norm": 4.068912982940674,
      "learning_rate": 0.0002292712972614422,
      "loss": 0.1885,
      "step": 2489
    },
    {
      "epoch": 0.23596304193319118,
      "grad_norm": 4.142428398132324,
      "learning_rate": 0.0002292428693262579,
      "loss": 0.2077,
      "step": 2490
    },
    {
      "epoch": 0.23605780620705993,
      "grad_norm": 1.26325523853302,
      "learning_rate": 0.0002292144413910736,
      "loss": 0.0387,
      "step": 2491
    },
    {
      "epoch": 0.2361525704809287,
      "grad_norm": 2.9506261348724365,
      "learning_rate": 0.0002291860134558893,
      "loss": 0.1122,
      "step": 2492
    },
    {
      "epoch": 0.23624733475479745,
      "grad_norm": 2.261654853820801,
      "learning_rate": 0.000229157585520705,
      "loss": 0.1095,
      "step": 2493
    },
    {
      "epoch": 0.23634209902866618,
      "grad_norm": 3.088073492050171,
      "learning_rate": 0.00022912915758552068,
      "loss": 0.1266,
      "step": 2494
    },
    {
      "epoch": 0.23643686330253494,
      "grad_norm": 2.855710983276367,
      "learning_rate": 0.00022910072965033637,
      "loss": 0.1724,
      "step": 2495
    },
    {
      "epoch": 0.2365316275764037,
      "grad_norm": 1.7014871835708618,
      "learning_rate": 0.00022907230171515208,
      "loss": 0.112,
      "step": 2496
    },
    {
      "epoch": 0.23662639185027246,
      "grad_norm": 2.7248024940490723,
      "learning_rate": 0.00022904387377996777,
      "loss": 0.1273,
      "step": 2497
    },
    {
      "epoch": 0.23672115612414119,
      "grad_norm": 3.5896198749542236,
      "learning_rate": 0.00022901544584478346,
      "loss": 0.1705,
      "step": 2498
    },
    {
      "epoch": 0.23681592039800994,
      "grad_norm": 2.466517448425293,
      "learning_rate": 0.00022898701790959915,
      "loss": 0.1478,
      "step": 2499
    },
    {
      "epoch": 0.2369106846718787,
      "grad_norm": 4.052593231201172,
      "learning_rate": 0.00022895858997441486,
      "loss": 0.0963,
      "step": 2500
    },
    {
      "epoch": 0.23700544894574746,
      "grad_norm": 9.602873802185059,
      "learning_rate": 0.00022893016203923055,
      "loss": 0.1777,
      "step": 2501
    },
    {
      "epoch": 0.23710021321961622,
      "grad_norm": 4.445596218109131,
      "learning_rate": 0.0002289017341040462,
      "loss": 0.1789,
      "step": 2502
    },
    {
      "epoch": 0.23719497749348495,
      "grad_norm": 5.8609938621521,
      "learning_rate": 0.0002288733061688619,
      "loss": 0.1972,
      "step": 2503
    },
    {
      "epoch": 0.2372897417673537,
      "grad_norm": 4.4077534675598145,
      "learning_rate": 0.0002288448782336776,
      "loss": 0.2515,
      "step": 2504
    },
    {
      "epoch": 0.23738450604122247,
      "grad_norm": 7.689358234405518,
      "learning_rate": 0.0002288164502984933,
      "loss": 0.2001,
      "step": 2505
    },
    {
      "epoch": 0.23747927031509122,
      "grad_norm": 7.5347676277160645,
      "learning_rate": 0.000228788022363309,
      "loss": 0.2073,
      "step": 2506
    },
    {
      "epoch": 0.23757403458895995,
      "grad_norm": 7.490480899810791,
      "learning_rate": 0.00022875959442812468,
      "loss": 0.2126,
      "step": 2507
    },
    {
      "epoch": 0.2376687988628287,
      "grad_norm": 2.1206419467926025,
      "learning_rate": 0.00022873116649294037,
      "loss": 0.0646,
      "step": 2508
    },
    {
      "epoch": 0.23776356313669747,
      "grad_norm": 2.26045823097229,
      "learning_rate": 0.00022870273855775608,
      "loss": 0.1016,
      "step": 2509
    },
    {
      "epoch": 0.23785832741056623,
      "grad_norm": 5.865337371826172,
      "learning_rate": 0.00022867431062257177,
      "loss": 0.1417,
      "step": 2510
    },
    {
      "epoch": 0.23795309168443496,
      "grad_norm": 3.280205011367798,
      "learning_rate": 0.00022864588268738746,
      "loss": 0.1249,
      "step": 2511
    },
    {
      "epoch": 0.23804785595830372,
      "grad_norm": 3.2595338821411133,
      "learning_rate": 0.00022861745475220315,
      "loss": 0.1942,
      "step": 2512
    },
    {
      "epoch": 0.23814262023217247,
      "grad_norm": 9.416908264160156,
      "learning_rate": 0.00022858902681701884,
      "loss": 0.2362,
      "step": 2513
    },
    {
      "epoch": 0.23823738450604123,
      "grad_norm": 2.809998035430908,
      "learning_rate": 0.00022856059888183455,
      "loss": 0.1622,
      "step": 2514
    },
    {
      "epoch": 0.23833214877990996,
      "grad_norm": 2.9318807125091553,
      "learning_rate": 0.00022853217094665021,
      "loss": 0.2633,
      "step": 2515
    },
    {
      "epoch": 0.23842691305377872,
      "grad_norm": 1.5309529304504395,
      "learning_rate": 0.0002285037430114659,
      "loss": 0.1041,
      "step": 2516
    },
    {
      "epoch": 0.23852167732764748,
      "grad_norm": 2.8478686809539795,
      "learning_rate": 0.0002284753150762816,
      "loss": 0.0998,
      "step": 2517
    },
    {
      "epoch": 0.23861644160151624,
      "grad_norm": 5.179548263549805,
      "learning_rate": 0.00022844688714109728,
      "loss": 0.1874,
      "step": 2518
    },
    {
      "epoch": 0.23871120587538497,
      "grad_norm": 7.806310176849365,
      "learning_rate": 0.000228418459205913,
      "loss": 0.2089,
      "step": 2519
    },
    {
      "epoch": 0.23880597014925373,
      "grad_norm": 4.18730354309082,
      "learning_rate": 0.00022839003127072868,
      "loss": 0.2448,
      "step": 2520
    },
    {
      "epoch": 0.23890073442312248,
      "grad_norm": 1.833497166633606,
      "learning_rate": 0.00022836160333554437,
      "loss": 0.0893,
      "step": 2521
    },
    {
      "epoch": 0.23899549869699124,
      "grad_norm": 2.0246713161468506,
      "learning_rate": 0.00022833317540036006,
      "loss": 0.1351,
      "step": 2522
    },
    {
      "epoch": 0.23909026297085997,
      "grad_norm": 1.308708906173706,
      "learning_rate": 0.00022830474746517577,
      "loss": 0.1411,
      "step": 2523
    },
    {
      "epoch": 0.23918502724472873,
      "grad_norm": 0.7268757224082947,
      "learning_rate": 0.00022827631952999146,
      "loss": 0.0454,
      "step": 2524
    },
    {
      "epoch": 0.2392797915185975,
      "grad_norm": 2.2564268112182617,
      "learning_rate": 0.00022824789159480715,
      "loss": 0.1617,
      "step": 2525
    },
    {
      "epoch": 0.23937455579246625,
      "grad_norm": 10.567386627197266,
      "learning_rate": 0.00022821946365962284,
      "loss": 0.1412,
      "step": 2526
    },
    {
      "epoch": 0.23946932006633498,
      "grad_norm": 2.5008296966552734,
      "learning_rate": 0.00022819103572443856,
      "loss": 0.1052,
      "step": 2527
    },
    {
      "epoch": 0.23956408434020374,
      "grad_norm": 3.7422749996185303,
      "learning_rate": 0.00022816260778925424,
      "loss": 0.147,
      "step": 2528
    },
    {
      "epoch": 0.2396588486140725,
      "grad_norm": 4.825085163116455,
      "learning_rate": 0.0002281341798540699,
      "loss": 0.2047,
      "step": 2529
    },
    {
      "epoch": 0.23975361288794125,
      "grad_norm": 2.2898943424224854,
      "learning_rate": 0.0002281057519188856,
      "loss": 0.113,
      "step": 2530
    },
    {
      "epoch": 0.23984837716181,
      "grad_norm": 1.4179799556732178,
      "learning_rate": 0.00022807732398370128,
      "loss": 0.0943,
      "step": 2531
    },
    {
      "epoch": 0.23994314143567874,
      "grad_norm": 0.8331389427185059,
      "learning_rate": 0.00022804889604851697,
      "loss": 0.0582,
      "step": 2532
    },
    {
      "epoch": 0.2400379057095475,
      "grad_norm": 2.7266933917999268,
      "learning_rate": 0.00022802046811333269,
      "loss": 0.1509,
      "step": 2533
    },
    {
      "epoch": 0.24013266998341626,
      "grad_norm": 1.2195649147033691,
      "learning_rate": 0.00022799204017814837,
      "loss": 0.0392,
      "step": 2534
    },
    {
      "epoch": 0.24022743425728502,
      "grad_norm": 1.6908372640609741,
      "learning_rate": 0.00022796361224296406,
      "loss": 0.0895,
      "step": 2535
    },
    {
      "epoch": 0.24032219853115375,
      "grad_norm": 4.13857889175415,
      "learning_rate": 0.00022793518430777975,
      "loss": 0.0436,
      "step": 2536
    },
    {
      "epoch": 0.2404169628050225,
      "grad_norm": 1.5444337129592896,
      "learning_rate": 0.00022790675637259547,
      "loss": 0.0338,
      "step": 2537
    },
    {
      "epoch": 0.24051172707889126,
      "grad_norm": 4.098666191101074,
      "learning_rate": 0.00022787832843741115,
      "loss": 0.1085,
      "step": 2538
    },
    {
      "epoch": 0.24060649135276002,
      "grad_norm": 3.110582113265991,
      "learning_rate": 0.00022784990050222684,
      "loss": 0.2433,
      "step": 2539
    },
    {
      "epoch": 0.24070125562662875,
      "grad_norm": 1.1561063528060913,
      "learning_rate": 0.00022782147256704253,
      "loss": 0.0225,
      "step": 2540
    },
    {
      "epoch": 0.2407960199004975,
      "grad_norm": 4.169135570526123,
      "learning_rate": 0.00022779304463185825,
      "loss": 0.1702,
      "step": 2541
    },
    {
      "epoch": 0.24089078417436627,
      "grad_norm": 1.996812343597412,
      "learning_rate": 0.0002277646166966739,
      "loss": 0.0272,
      "step": 2542
    },
    {
      "epoch": 0.24098554844823503,
      "grad_norm": 2.7713077068328857,
      "learning_rate": 0.0002277361887614896,
      "loss": 0.0964,
      "step": 2543
    },
    {
      "epoch": 0.24108031272210376,
      "grad_norm": 1.5653167963027954,
      "learning_rate": 0.00022770776082630528,
      "loss": 0.037,
      "step": 2544
    },
    {
      "epoch": 0.24117507699597251,
      "grad_norm": 4.983802795410156,
      "learning_rate": 0.00022767933289112097,
      "loss": 0.1414,
      "step": 2545
    },
    {
      "epoch": 0.24126984126984127,
      "grad_norm": 0.2348651885986328,
      "learning_rate": 0.0002276509049559367,
      "loss": 0.0071,
      "step": 2546
    },
    {
      "epoch": 0.24136460554371003,
      "grad_norm": 3.0153653621673584,
      "learning_rate": 0.00022762247702075238,
      "loss": 0.1982,
      "step": 2547
    },
    {
      "epoch": 0.24145936981757876,
      "grad_norm": 9.810685157775879,
      "learning_rate": 0.00022759404908556807,
      "loss": 0.1441,
      "step": 2548
    },
    {
      "epoch": 0.24155413409144752,
      "grad_norm": 2.3683996200561523,
      "learning_rate": 0.00022756562115038375,
      "loss": 0.1265,
      "step": 2549
    },
    {
      "epoch": 0.24164889836531628,
      "grad_norm": 3.507880926132202,
      "learning_rate": 0.00022753719321519944,
      "loss": 0.1213,
      "step": 2550
    },
    {
      "epoch": 0.24174366263918504,
      "grad_norm": 3.40857195854187,
      "learning_rate": 0.00022750876528001516,
      "loss": 0.1266,
      "step": 2551
    },
    {
      "epoch": 0.24183842691305377,
      "grad_norm": 2.411916971206665,
      "learning_rate": 0.00022748033734483085,
      "loss": 0.2233,
      "step": 2552
    },
    {
      "epoch": 0.24193319118692252,
      "grad_norm": 4.845195293426514,
      "learning_rate": 0.00022745190940964653,
      "loss": 0.2375,
      "step": 2553
    },
    {
      "epoch": 0.24202795546079128,
      "grad_norm": 3.217116355895996,
      "learning_rate": 0.00022742348147446222,
      "loss": 0.0731,
      "step": 2554
    },
    {
      "epoch": 0.24212271973466004,
      "grad_norm": 2.8145291805267334,
      "learning_rate": 0.00022739505353927794,
      "loss": 0.1494,
      "step": 2555
    },
    {
      "epoch": 0.24221748400852877,
      "grad_norm": 3.1861159801483154,
      "learning_rate": 0.0002273666256040936,
      "loss": 0.2045,
      "step": 2556
    },
    {
      "epoch": 0.24231224828239753,
      "grad_norm": 2.630669355392456,
      "learning_rate": 0.0002273381976689093,
      "loss": 0.1008,
      "step": 2557
    },
    {
      "epoch": 0.2424070125562663,
      "grad_norm": 3.3396475315093994,
      "learning_rate": 0.00022730976973372498,
      "loss": 0.0978,
      "step": 2558
    },
    {
      "epoch": 0.24250177683013505,
      "grad_norm": 3.147312879562378,
      "learning_rate": 0.00022728134179854066,
      "loss": 0.1476,
      "step": 2559
    },
    {
      "epoch": 0.2425965411040038,
      "grad_norm": 1.887809157371521,
      "learning_rate": 0.00022725291386335638,
      "loss": 0.0794,
      "step": 2560
    },
    {
      "epoch": 0.24269130537787253,
      "grad_norm": 2.3983449935913086,
      "learning_rate": 0.00022722448592817207,
      "loss": 0.0134,
      "step": 2561
    },
    {
      "epoch": 0.2427860696517413,
      "grad_norm": 5.528940677642822,
      "learning_rate": 0.00022719605799298776,
      "loss": 0.09,
      "step": 2562
    },
    {
      "epoch": 0.24288083392561005,
      "grad_norm": 11.672698974609375,
      "learning_rate": 0.00022716763005780344,
      "loss": 0.3131,
      "step": 2563
    },
    {
      "epoch": 0.2429755981994788,
      "grad_norm": 2.4346506595611572,
      "learning_rate": 0.00022713920212261913,
      "loss": 0.1088,
      "step": 2564
    },
    {
      "epoch": 0.24307036247334754,
      "grad_norm": 1.9268560409545898,
      "learning_rate": 0.00022711077418743485,
      "loss": 0.0543,
      "step": 2565
    },
    {
      "epoch": 0.2431651267472163,
      "grad_norm": 2.9393317699432373,
      "learning_rate": 0.00022708234625225054,
      "loss": 0.1464,
      "step": 2566
    },
    {
      "epoch": 0.24325989102108506,
      "grad_norm": 9.626551628112793,
      "learning_rate": 0.00022705391831706623,
      "loss": 0.2539,
      "step": 2567
    },
    {
      "epoch": 0.24335465529495381,
      "grad_norm": 7.752351760864258,
      "learning_rate": 0.00022702549038188191,
      "loss": 0.1548,
      "step": 2568
    },
    {
      "epoch": 0.24344941956882254,
      "grad_norm": 4.370753288269043,
      "learning_rate": 0.00022699706244669757,
      "loss": 0.183,
      "step": 2569
    },
    {
      "epoch": 0.2435441838426913,
      "grad_norm": 2.0635666847229004,
      "learning_rate": 0.0002269686345115133,
      "loss": 0.1154,
      "step": 2570
    },
    {
      "epoch": 0.24363894811656006,
      "grad_norm": 9.70018196105957,
      "learning_rate": 0.00022694020657632898,
      "loss": 0.1775,
      "step": 2571
    },
    {
      "epoch": 0.24373371239042882,
      "grad_norm": 7.181138515472412,
      "learning_rate": 0.00022691177864114467,
      "loss": 0.1389,
      "step": 2572
    },
    {
      "epoch": 0.24382847666429755,
      "grad_norm": 8.271160125732422,
      "learning_rate": 0.00022688335070596036,
      "loss": 0.379,
      "step": 2573
    },
    {
      "epoch": 0.2439232409381663,
      "grad_norm": 2.958684206008911,
      "learning_rate": 0.00022685492277077607,
      "loss": 0.1868,
      "step": 2574
    },
    {
      "epoch": 0.24401800521203507,
      "grad_norm": 0.9885596632957458,
      "learning_rate": 0.00022682649483559176,
      "loss": 0.0214,
      "step": 2575
    },
    {
      "epoch": 0.24411276948590382,
      "grad_norm": 10.180370330810547,
      "learning_rate": 0.00022679806690040745,
      "loss": 0.1882,
      "step": 2576
    },
    {
      "epoch": 0.24420753375977255,
      "grad_norm": 0.5992103219032288,
      "learning_rate": 0.00022676963896522314,
      "loss": 0.0362,
      "step": 2577
    },
    {
      "epoch": 0.2443022980336413,
      "grad_norm": 5.4094624519348145,
      "learning_rate": 0.00022674121103003885,
      "loss": 0.1062,
      "step": 2578
    },
    {
      "epoch": 0.24439706230751007,
      "grad_norm": 1.6454161405563354,
      "learning_rate": 0.00022671278309485454,
      "loss": 0.0794,
      "step": 2579
    },
    {
      "epoch": 0.24449182658137883,
      "grad_norm": 1.8018981218338013,
      "learning_rate": 0.00022668435515967023,
      "loss": 0.0723,
      "step": 2580
    },
    {
      "epoch": 0.24458659085524756,
      "grad_norm": 12.722579956054688,
      "learning_rate": 0.00022665592722448592,
      "loss": 0.0906,
      "step": 2581
    },
    {
      "epoch": 0.24468135512911632,
      "grad_norm": 4.707290172576904,
      "learning_rate": 0.00022662749928930158,
      "loss": 0.0783,
      "step": 2582
    },
    {
      "epoch": 0.24477611940298508,
      "grad_norm": 3.577199697494507,
      "learning_rate": 0.00022659907135411727,
      "loss": 0.2262,
      "step": 2583
    },
    {
      "epoch": 0.24487088367685383,
      "grad_norm": 8.07284927368164,
      "learning_rate": 0.00022657064341893298,
      "loss": 0.0809,
      "step": 2584
    },
    {
      "epoch": 0.24496564795072256,
      "grad_norm": 3.353591203689575,
      "learning_rate": 0.00022654221548374867,
      "loss": 0.0997,
      "step": 2585
    },
    {
      "epoch": 0.24506041222459132,
      "grad_norm": 2.4806015491485596,
      "learning_rate": 0.00022651378754856436,
      "loss": 0.1828,
      "step": 2586
    },
    {
      "epoch": 0.24515517649846008,
      "grad_norm": 0.850719690322876,
      "learning_rate": 0.00022648535961338005,
      "loss": 0.024,
      "step": 2587
    },
    {
      "epoch": 0.24524994077232884,
      "grad_norm": 6.954102993011475,
      "learning_rate": 0.00022645693167819576,
      "loss": 0.1345,
      "step": 2588
    },
    {
      "epoch": 0.24534470504619757,
      "grad_norm": 11.895689010620117,
      "learning_rate": 0.00022642850374301145,
      "loss": 0.2105,
      "step": 2589
    },
    {
      "epoch": 0.24543946932006633,
      "grad_norm": 9.300158500671387,
      "learning_rate": 0.00022640007580782714,
      "loss": 0.4616,
      "step": 2590
    },
    {
      "epoch": 0.2455342335939351,
      "grad_norm": 5.412646770477295,
      "learning_rate": 0.00022637164787264283,
      "loss": 0.0643,
      "step": 2591
    },
    {
      "epoch": 0.24562899786780384,
      "grad_norm": 4.028825283050537,
      "learning_rate": 0.00022634321993745854,
      "loss": 0.2146,
      "step": 2592
    },
    {
      "epoch": 0.2457237621416726,
      "grad_norm": 1.7939350605010986,
      "learning_rate": 0.00022631479200227423,
      "loss": 0.047,
      "step": 2593
    },
    {
      "epoch": 0.24581852641554133,
      "grad_norm": 5.425191879272461,
      "learning_rate": 0.00022628636406708992,
      "loss": 0.1686,
      "step": 2594
    },
    {
      "epoch": 0.2459132906894101,
      "grad_norm": 2.2446627616882324,
      "learning_rate": 0.0002262579361319056,
      "loss": 0.1058,
      "step": 2595
    },
    {
      "epoch": 0.24600805496327885,
      "grad_norm": 6.209436416625977,
      "learning_rate": 0.00022622950819672127,
      "loss": 0.0904,
      "step": 2596
    },
    {
      "epoch": 0.2461028192371476,
      "grad_norm": 5.001934051513672,
      "learning_rate": 0.00022620108026153698,
      "loss": 0.1948,
      "step": 2597
    },
    {
      "epoch": 0.24619758351101634,
      "grad_norm": 5.674884796142578,
      "learning_rate": 0.00022617265232635267,
      "loss": 0.319,
      "step": 2598
    },
    {
      "epoch": 0.2462923477848851,
      "grad_norm": 2.6582024097442627,
      "learning_rate": 0.00022614422439116836,
      "loss": 0.1534,
      "step": 2599
    },
    {
      "epoch": 0.24638711205875385,
      "grad_norm": 1.3358064889907837,
      "learning_rate": 0.00022611579645598405,
      "loss": 0.0859,
      "step": 2600
    },
    {
      "epoch": 0.2464818763326226,
      "grad_norm": 8.249454498291016,
      "learning_rate": 0.00022608736852079974,
      "loss": 0.3431,
      "step": 2601
    },
    {
      "epoch": 0.24657664060649134,
      "grad_norm": 2.278872013092041,
      "learning_rate": 0.00022605894058561545,
      "loss": 0.038,
      "step": 2602
    },
    {
      "epoch": 0.2466714048803601,
      "grad_norm": 4.594461917877197,
      "learning_rate": 0.00022603051265043114,
      "loss": 0.0945,
      "step": 2603
    },
    {
      "epoch": 0.24676616915422886,
      "grad_norm": 15.432077407836914,
      "learning_rate": 0.00022600208471524683,
      "loss": 0.374,
      "step": 2604
    },
    {
      "epoch": 0.24686093342809762,
      "grad_norm": 6.373379707336426,
      "learning_rate": 0.00022597365678006252,
      "loss": 0.0851,
      "step": 2605
    },
    {
      "epoch": 0.24695569770196635,
      "grad_norm": 3.5749082565307617,
      "learning_rate": 0.00022594522884487823,
      "loss": 0.2747,
      "step": 2606
    },
    {
      "epoch": 0.2470504619758351,
      "grad_norm": 2.4754395484924316,
      "learning_rate": 0.00022591680090969392,
      "loss": 0.1131,
      "step": 2607
    },
    {
      "epoch": 0.24714522624970386,
      "grad_norm": 3.2373046875,
      "learning_rate": 0.0002258883729745096,
      "loss": 0.0608,
      "step": 2608
    },
    {
      "epoch": 0.24723999052357262,
      "grad_norm": 3.5895674228668213,
      "learning_rate": 0.00022585994503932527,
      "loss": 0.0346,
      "step": 2609
    },
    {
      "epoch": 0.24733475479744135,
      "grad_norm": 5.9197211265563965,
      "learning_rate": 0.00022583151710414096,
      "loss": 0.0493,
      "step": 2610
    },
    {
      "epoch": 0.2474295190713101,
      "grad_norm": 2.0678911209106445,
      "learning_rate": 0.00022580308916895668,
      "loss": 0.1503,
      "step": 2611
    },
    {
      "epoch": 0.24752428334517887,
      "grad_norm": 5.150984764099121,
      "learning_rate": 0.00022577466123377236,
      "loss": 0.2298,
      "step": 2612
    },
    {
      "epoch": 0.24761904761904763,
      "grad_norm": 3.9182567596435547,
      "learning_rate": 0.00022574623329858805,
      "loss": 0.1399,
      "step": 2613
    },
    {
      "epoch": 0.24771381189291636,
      "grad_norm": 2.110748767852783,
      "learning_rate": 0.00022571780536340374,
      "loss": 0.1415,
      "step": 2614
    },
    {
      "epoch": 0.24780857616678512,
      "grad_norm": 10.954715728759766,
      "learning_rate": 0.00022568937742821943,
      "loss": 0.2823,
      "step": 2615
    },
    {
      "epoch": 0.24790334044065387,
      "grad_norm": 2.1641032695770264,
      "learning_rate": 0.00022566094949303514,
      "loss": 0.0658,
      "step": 2616
    },
    {
      "epoch": 0.24799810471452263,
      "grad_norm": 7.382042407989502,
      "learning_rate": 0.00022563252155785083,
      "loss": 0.1709,
      "step": 2617
    },
    {
      "epoch": 0.24809286898839136,
      "grad_norm": 8.646682739257812,
      "learning_rate": 0.00022560409362266652,
      "loss": 0.1527,
      "step": 2618
    },
    {
      "epoch": 0.24818763326226012,
      "grad_norm": 1.3382413387298584,
      "learning_rate": 0.0002255756656874822,
      "loss": 0.0589,
      "step": 2619
    },
    {
      "epoch": 0.24828239753612888,
      "grad_norm": 5.312417030334473,
      "learning_rate": 0.00022554723775229793,
      "loss": 0.1737,
      "step": 2620
    },
    {
      "epoch": 0.24837716180999764,
      "grad_norm": 2.184849739074707,
      "learning_rate": 0.0002255188098171136,
      "loss": 0.1235,
      "step": 2621
    },
    {
      "epoch": 0.2484719260838664,
      "grad_norm": 2.162140130996704,
      "learning_rate": 0.0002254903818819293,
      "loss": 0.2249,
      "step": 2622
    },
    {
      "epoch": 0.24856669035773513,
      "grad_norm": 4.32795524597168,
      "learning_rate": 0.00022546195394674496,
      "loss": 0.155,
      "step": 2623
    },
    {
      "epoch": 0.24866145463160388,
      "grad_norm": 2.440636396408081,
      "learning_rate": 0.00022543352601156065,
      "loss": 0.1038,
      "step": 2624
    },
    {
      "epoch": 0.24875621890547264,
      "grad_norm": 2.13944149017334,
      "learning_rate": 0.00022540509807637637,
      "loss": 0.0695,
      "step": 2625
    },
    {
      "epoch": 0.2488509831793414,
      "grad_norm": 3.7457826137542725,
      "learning_rate": 0.00022537667014119206,
      "loss": 0.1401,
      "step": 2626
    },
    {
      "epoch": 0.24894574745321013,
      "grad_norm": 2.900374412536621,
      "learning_rate": 0.00022534824220600774,
      "loss": 0.0904,
      "step": 2627
    },
    {
      "epoch": 0.2490405117270789,
      "grad_norm": 2.4804134368896484,
      "learning_rate": 0.00022531981427082343,
      "loss": 0.1432,
      "step": 2628
    },
    {
      "epoch": 0.24913527600094765,
      "grad_norm": 2.556849241256714,
      "learning_rate": 0.00022529138633563915,
      "loss": 0.0505,
      "step": 2629
    },
    {
      "epoch": 0.2492300402748164,
      "grad_norm": 2.1939921379089355,
      "learning_rate": 0.00022526295840045484,
      "loss": 0.1919,
      "step": 2630
    },
    {
      "epoch": 0.24932480454868514,
      "grad_norm": 4.007663726806641,
      "learning_rate": 0.00022523453046527052,
      "loss": 0.0858,
      "step": 2631
    },
    {
      "epoch": 0.2494195688225539,
      "grad_norm": 1.5191569328308105,
      "learning_rate": 0.0002252061025300862,
      "loss": 0.0765,
      "step": 2632
    },
    {
      "epoch": 0.24951433309642265,
      "grad_norm": 11.659287452697754,
      "learning_rate": 0.0002251776745949019,
      "loss": 0.1393,
      "step": 2633
    },
    {
      "epoch": 0.2496090973702914,
      "grad_norm": 6.391395568847656,
      "learning_rate": 0.00022514924665971762,
      "loss": 0.1573,
      "step": 2634
    },
    {
      "epoch": 0.24970386164416014,
      "grad_norm": 4.101916790008545,
      "learning_rate": 0.0002251208187245333,
      "loss": 0.2379,
      "step": 2635
    },
    {
      "epoch": 0.2497986259180289,
      "grad_norm": 5.878182888031006,
      "learning_rate": 0.00022509239078934897,
      "loss": 0.1426,
      "step": 2636
    },
    {
      "epoch": 0.24989339019189766,
      "grad_norm": 2.665343999862671,
      "learning_rate": 0.00022506396285416465,
      "loss": 0.144,
      "step": 2637
    },
    {
      "epoch": 0.24998815446576642,
      "grad_norm": 8.20058822631836,
      "learning_rate": 0.00022503553491898034,
      "loss": 0.1079,
      "step": 2638
    },
    {
      "epoch": 0.2500829187396352,
      "grad_norm": 3.407219409942627,
      "learning_rate": 0.00022500710698379606,
      "loss": 0.0759,
      "step": 2639
    },
    {
      "epoch": 0.2501776830135039,
      "grad_norm": 3.119882822036743,
      "learning_rate": 0.00022497867904861175,
      "loss": 0.186,
      "step": 2640
    },
    {
      "epoch": 0.25027244728737263,
      "grad_norm": 3.341096878051758,
      "learning_rate": 0.00022495025111342743,
      "loss": 0.1284,
      "step": 2641
    },
    {
      "epoch": 0.2503672115612414,
      "grad_norm": 1.849449634552002,
      "learning_rate": 0.00022492182317824312,
      "loss": 0.086,
      "step": 2642
    },
    {
      "epoch": 0.25046197583511015,
      "grad_norm": 18.198768615722656,
      "learning_rate": 0.00022489339524305884,
      "loss": 0.3315,
      "step": 2643
    },
    {
      "epoch": 0.25055674010897894,
      "grad_norm": 4.322601795196533,
      "learning_rate": 0.00022486496730787453,
      "loss": 0.3082,
      "step": 2644
    },
    {
      "epoch": 0.25065150438284767,
      "grad_norm": 6.784631729125977,
      "learning_rate": 0.00022483653937269022,
      "loss": 0.2318,
      "step": 2645
    },
    {
      "epoch": 0.2507462686567164,
      "grad_norm": 1.143232822418213,
      "learning_rate": 0.0002248081114375059,
      "loss": 0.055,
      "step": 2646
    },
    {
      "epoch": 0.2508410329305852,
      "grad_norm": 3.9610631465911865,
      "learning_rate": 0.00022477968350232162,
      "loss": 0.1038,
      "step": 2647
    },
    {
      "epoch": 0.2509357972044539,
      "grad_norm": 1.7161866426467896,
      "learning_rate": 0.0002247512555671373,
      "loss": 0.1565,
      "step": 2648
    },
    {
      "epoch": 0.25103056147832264,
      "grad_norm": 3.2173709869384766,
      "learning_rate": 0.000224722827631953,
      "loss": 0.2156,
      "step": 2649
    },
    {
      "epoch": 0.25112532575219143,
      "grad_norm": 0.5922062397003174,
      "learning_rate": 0.00022469439969676866,
      "loss": 0.0249,
      "step": 2650
    },
    {
      "epoch": 0.25122009002606016,
      "grad_norm": 2.5721943378448486,
      "learning_rate": 0.00022466597176158435,
      "loss": 0.08,
      "step": 2651
    },
    {
      "epoch": 0.25131485429992895,
      "grad_norm": 8.042393684387207,
      "learning_rate": 0.00022463754382640003,
      "loss": 0.3128,
      "step": 2652
    },
    {
      "epoch": 0.2514096185737977,
      "grad_norm": 7.063627243041992,
      "learning_rate": 0.00022460911589121575,
      "loss": 0.1765,
      "step": 2653
    },
    {
      "epoch": 0.2515043828476664,
      "grad_norm": 3.231293201446533,
      "learning_rate": 0.00022458068795603144,
      "loss": 0.2599,
      "step": 2654
    },
    {
      "epoch": 0.2515991471215352,
      "grad_norm": 3.6781649589538574,
      "learning_rate": 0.00022455226002084713,
      "loss": 0.2517,
      "step": 2655
    },
    {
      "epoch": 0.2516939113954039,
      "grad_norm": 3.279636859893799,
      "learning_rate": 0.00022452383208566281,
      "loss": 0.0767,
      "step": 2656
    },
    {
      "epoch": 0.2517886756692727,
      "grad_norm": 6.16990852355957,
      "learning_rate": 0.00022449540415047853,
      "loss": 0.1027,
      "step": 2657
    },
    {
      "epoch": 0.25188343994314144,
      "grad_norm": 6.35900354385376,
      "learning_rate": 0.00022446697621529422,
      "loss": 0.195,
      "step": 2658
    },
    {
      "epoch": 0.25197820421701017,
      "grad_norm": 2.590223789215088,
      "learning_rate": 0.0002244385482801099,
      "loss": 0.0825,
      "step": 2659
    },
    {
      "epoch": 0.25207296849087896,
      "grad_norm": 2.1054563522338867,
      "learning_rate": 0.0002244101203449256,
      "loss": 0.1058,
      "step": 2660
    },
    {
      "epoch": 0.2521677327647477,
      "grad_norm": 5.427118301391602,
      "learning_rate": 0.0002243816924097413,
      "loss": 0.1141,
      "step": 2661
    },
    {
      "epoch": 0.2522624970386164,
      "grad_norm": 1.7727129459381104,
      "learning_rate": 0.000224353264474557,
      "loss": 0.0769,
      "step": 2662
    },
    {
      "epoch": 0.2523572613124852,
      "grad_norm": 3.124441623687744,
      "learning_rate": 0.00022432483653937266,
      "loss": 0.2474,
      "step": 2663
    },
    {
      "epoch": 0.25245202558635393,
      "grad_norm": 8.723774909973145,
      "learning_rate": 0.00022429640860418835,
      "loss": 0.2206,
      "step": 2664
    },
    {
      "epoch": 0.2525467898602227,
      "grad_norm": 2.1348257064819336,
      "learning_rate": 0.00022426798066900404,
      "loss": 0.1055,
      "step": 2665
    },
    {
      "epoch": 0.25264155413409145,
      "grad_norm": 12.495734214782715,
      "learning_rate": 0.00022423955273381973,
      "loss": 0.3487,
      "step": 2666
    },
    {
      "epoch": 0.2527363184079602,
      "grad_norm": 1.543320655822754,
      "learning_rate": 0.00022421112479863544,
      "loss": 0.0632,
      "step": 2667
    },
    {
      "epoch": 0.25283108268182897,
      "grad_norm": 13.161385536193848,
      "learning_rate": 0.00022418269686345113,
      "loss": 0.2877,
      "step": 2668
    },
    {
      "epoch": 0.2529258469556977,
      "grad_norm": 1.0827000141143799,
      "learning_rate": 0.00022415426892826682,
      "loss": 0.0289,
      "step": 2669
    },
    {
      "epoch": 0.25302061122956643,
      "grad_norm": 0.5047460198402405,
      "learning_rate": 0.0002241258409930825,
      "loss": 0.0139,
      "step": 2670
    },
    {
      "epoch": 0.2531153755034352,
      "grad_norm": 1.6173062324523926,
      "learning_rate": 0.00022409741305789822,
      "loss": 0.0571,
      "step": 2671
    },
    {
      "epoch": 0.25321013977730394,
      "grad_norm": 9.046289443969727,
      "learning_rate": 0.0002240689851227139,
      "loss": 0.2009,
      "step": 2672
    },
    {
      "epoch": 0.25330490405117273,
      "grad_norm": 7.653389930725098,
      "learning_rate": 0.0002240405571875296,
      "loss": 0.3884,
      "step": 2673
    },
    {
      "epoch": 0.25339966832504146,
      "grad_norm": 3.5020320415496826,
      "learning_rate": 0.00022401212925234529,
      "loss": 0.1095,
      "step": 2674
    },
    {
      "epoch": 0.2534944325989102,
      "grad_norm": 7.725658893585205,
      "learning_rate": 0.000223983701317161,
      "loss": 0.3907,
      "step": 2675
    },
    {
      "epoch": 0.253589196872779,
      "grad_norm": 7.476500034332275,
      "learning_rate": 0.0002239552733819767,
      "loss": 0.0683,
      "step": 2676
    },
    {
      "epoch": 0.2536839611466477,
      "grad_norm": 2.148245096206665,
      "learning_rate": 0.00022392684544679235,
      "loss": 0.0596,
      "step": 2677
    },
    {
      "epoch": 0.25377872542051644,
      "grad_norm": 6.539540767669678,
      "learning_rate": 0.00022389841751160804,
      "loss": 0.2234,
      "step": 2678
    },
    {
      "epoch": 0.2538734896943852,
      "grad_norm": 3.0717711448669434,
      "learning_rate": 0.00022386998957642373,
      "loss": 0.2604,
      "step": 2679
    },
    {
      "epoch": 0.25396825396825395,
      "grad_norm": 4.347339153289795,
      "learning_rate": 0.00022384156164123944,
      "loss": 0.1856,
      "step": 2680
    },
    {
      "epoch": 0.25406301824212274,
      "grad_norm": 10.267807960510254,
      "learning_rate": 0.00022381313370605513,
      "loss": 0.3622,
      "step": 2681
    },
    {
      "epoch": 0.25415778251599147,
      "grad_norm": 4.166136741638184,
      "learning_rate": 0.00022378470577087082,
      "loss": 0.2345,
      "step": 2682
    },
    {
      "epoch": 0.2542525467898602,
      "grad_norm": 3.2032175064086914,
      "learning_rate": 0.0002237562778356865,
      "loss": 0.0768,
      "step": 2683
    },
    {
      "epoch": 0.254347311063729,
      "grad_norm": 2.737809658050537,
      "learning_rate": 0.0002237278499005022,
      "loss": 0.149,
      "step": 2684
    },
    {
      "epoch": 0.2544420753375977,
      "grad_norm": 1.2052944898605347,
      "learning_rate": 0.0002236994219653179,
      "loss": 0.078,
      "step": 2685
    },
    {
      "epoch": 0.2545368396114665,
      "grad_norm": 1.3799983263015747,
      "learning_rate": 0.0002236709940301336,
      "loss": 0.0534,
      "step": 2686
    },
    {
      "epoch": 0.25463160388533523,
      "grad_norm": 0.8057593703269958,
      "learning_rate": 0.0002236425660949493,
      "loss": 0.0355,
      "step": 2687
    },
    {
      "epoch": 0.25472636815920396,
      "grad_norm": 6.195634365081787,
      "learning_rate": 0.00022361413815976498,
      "loss": 0.2059,
      "step": 2688
    },
    {
      "epoch": 0.25482113243307275,
      "grad_norm": 3.8812568187713623,
      "learning_rate": 0.0002235857102245807,
      "loss": 0.237,
      "step": 2689
    },
    {
      "epoch": 0.2549158967069415,
      "grad_norm": 2.487412214279175,
      "learning_rate": 0.00022355728228939635,
      "loss": 0.0483,
      "step": 2690
    },
    {
      "epoch": 0.2550106609808102,
      "grad_norm": 9.927041053771973,
      "learning_rate": 0.00022352885435421204,
      "loss": 0.1865,
      "step": 2691
    },
    {
      "epoch": 0.255105425254679,
      "grad_norm": 3.7019455432891846,
      "learning_rate": 0.00022350042641902773,
      "loss": 0.0805,
      "step": 2692
    },
    {
      "epoch": 0.2552001895285477,
      "grad_norm": 4.439667701721191,
      "learning_rate": 0.00022347199848384342,
      "loss": 0.0961,
      "step": 2693
    },
    {
      "epoch": 0.2552949538024165,
      "grad_norm": 0.8706492781639099,
      "learning_rate": 0.00022344357054865913,
      "loss": 0.0613,
      "step": 2694
    },
    {
      "epoch": 0.25538971807628524,
      "grad_norm": 2.050565481185913,
      "learning_rate": 0.00022341514261347482,
      "loss": 0.1078,
      "step": 2695
    },
    {
      "epoch": 0.255484482350154,
      "grad_norm": 2.858595132827759,
      "learning_rate": 0.0002233867146782905,
      "loss": 0.2932,
      "step": 2696
    },
    {
      "epoch": 0.25557924662402276,
      "grad_norm": 1.4309308528900146,
      "learning_rate": 0.0002233582867431062,
      "loss": 0.0847,
      "step": 2697
    },
    {
      "epoch": 0.2556740108978915,
      "grad_norm": 1.6964592933654785,
      "learning_rate": 0.00022332985880792192,
      "loss": 0.1287,
      "step": 2698
    },
    {
      "epoch": 0.2557687751717602,
      "grad_norm": 1.4131735563278198,
      "learning_rate": 0.0002233014308727376,
      "loss": 0.0786,
      "step": 2699
    },
    {
      "epoch": 0.255863539445629,
      "grad_norm": 0.8940513730049133,
      "learning_rate": 0.0002232730029375533,
      "loss": 0.0373,
      "step": 2700
    }
  ],
  "logging_steps": 1,
  "max_steps": 10553,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 8528229094044672.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
