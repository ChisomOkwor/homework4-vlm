{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0021551491902026916,
  "eval_steps": 500,
  "global_step": 200,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.0775745951013459e-05,
      "grad_norm": 11.734519004821777,
      "learning_rate": 0.0005,
      "loss": 0.7383,
      "step": 1
    },
    {
      "epoch": 2.1551491902026918e-05,
      "grad_norm": 34.50665283203125,
      "learning_rate": 0.0004994617868675996,
      "loss": 0.7266,
      "step": 2
    },
    {
      "epoch": 3.2327237853040375e-05,
      "grad_norm": 18.584224700927734,
      "learning_rate": 0.0004989235737351992,
      "loss": 0.5781,
      "step": 3
    },
    {
      "epoch": 4.3102983804053835e-05,
      "grad_norm": 10.856942176818848,
      "learning_rate": 0.0004983853606027987,
      "loss": 0.6094,
      "step": 4
    },
    {
      "epoch": 5.3878729755067296e-05,
      "grad_norm": 15.20124626159668,
      "learning_rate": 0.0004978471474703982,
      "loss": 0.7188,
      "step": 5
    },
    {
      "epoch": 6.465447570608075e-05,
      "grad_norm": 38.783966064453125,
      "learning_rate": 0.0004973089343379978,
      "loss": 1.0938,
      "step": 6
    },
    {
      "epoch": 7.543022165709421e-05,
      "grad_norm": 12.740062713623047,
      "learning_rate": 0.0004967707212055974,
      "loss": 0.2539,
      "step": 7
    },
    {
      "epoch": 8.620596760810767e-05,
      "grad_norm": 24.5117130279541,
      "learning_rate": 0.000496232508073197,
      "loss": 0.8203,
      "step": 8
    },
    {
      "epoch": 9.698171355912113e-05,
      "grad_norm": 19.737266540527344,
      "learning_rate": 0.0004956942949407966,
      "loss": 0.9375,
      "step": 9
    },
    {
      "epoch": 0.00010775745951013459,
      "grad_norm": 6.9918670654296875,
      "learning_rate": 0.0004951560818083961,
      "loss": 0.707,
      "step": 10
    },
    {
      "epoch": 0.00011853320546114805,
      "grad_norm": 11.607245445251465,
      "learning_rate": 0.0004946178686759958,
      "loss": 0.5898,
      "step": 11
    },
    {
      "epoch": 0.0001293089514121615,
      "grad_norm": 31.876806259155273,
      "learning_rate": 0.0004940796555435953,
      "loss": 1.3594,
      "step": 12
    },
    {
      "epoch": 0.00014008469736317497,
      "grad_norm": 12.43700122833252,
      "learning_rate": 0.0004935414424111948,
      "loss": 0.8047,
      "step": 13
    },
    {
      "epoch": 0.00015086044331418842,
      "grad_norm": 7.036546230316162,
      "learning_rate": 0.0004930032292787944,
      "loss": 0.625,
      "step": 14
    },
    {
      "epoch": 0.0001616361892652019,
      "grad_norm": 16.671363830566406,
      "learning_rate": 0.000492465016146394,
      "loss": 0.6953,
      "step": 15
    },
    {
      "epoch": 0.00017241193521621534,
      "grad_norm": 11.222743034362793,
      "learning_rate": 0.0004919268030139936,
      "loss": 0.7266,
      "step": 16
    },
    {
      "epoch": 0.0001831876811672288,
      "grad_norm": 20.321971893310547,
      "learning_rate": 0.0004913885898815932,
      "loss": 0.6719,
      "step": 17
    },
    {
      "epoch": 0.00019396342711824226,
      "grad_norm": 43.40980529785156,
      "learning_rate": 0.0004908503767491927,
      "loss": 1.5312,
      "step": 18
    },
    {
      "epoch": 0.0002047391730692557,
      "grad_norm": 8.459393501281738,
      "learning_rate": 0.0004903121636167922,
      "loss": 0.7422,
      "step": 19
    },
    {
      "epoch": 0.00021551491902026918,
      "grad_norm": 7.693176746368408,
      "learning_rate": 0.0004897739504843918,
      "loss": 0.5898,
      "step": 20
    },
    {
      "epoch": 0.00022629066497128263,
      "grad_norm": 3.173727512359619,
      "learning_rate": 0.0004892357373519914,
      "loss": 0.7109,
      "step": 21
    },
    {
      "epoch": 0.0002370664109222961,
      "grad_norm": 6.842949390411377,
      "learning_rate": 0.000488697524219591,
      "loss": 0.7109,
      "step": 22
    },
    {
      "epoch": 0.00024784215687330955,
      "grad_norm": 1.0444997549057007,
      "learning_rate": 0.0004881593110871905,
      "loss": 0.6992,
      "step": 23
    },
    {
      "epoch": 0.000258617902824323,
      "grad_norm": 8.681434631347656,
      "learning_rate": 0.0004876210979547901,
      "loss": 0.7266,
      "step": 24
    },
    {
      "epoch": 0.00026939364877533645,
      "grad_norm": 20.266357421875,
      "learning_rate": 0.0004870828848223897,
      "loss": 0.8008,
      "step": 25
    },
    {
      "epoch": 0.00028016939472634995,
      "grad_norm": 25.284927368164062,
      "learning_rate": 0.0004865446716899892,
      "loss": 0.6562,
      "step": 26
    },
    {
      "epoch": 0.0002909451406773634,
      "grad_norm": 8.273880958557129,
      "learning_rate": 0.0004860064585575888,
      "loss": 0.7109,
      "step": 27
    },
    {
      "epoch": 0.00030172088662837684,
      "grad_norm": 6.550272464752197,
      "learning_rate": 0.0004854682454251884,
      "loss": 0.6484,
      "step": 28
    },
    {
      "epoch": 0.0003124966325793903,
      "grad_norm": 12.523265838623047,
      "learning_rate": 0.00048493003229278797,
      "loss": 0.6133,
      "step": 29
    },
    {
      "epoch": 0.0003232723785304038,
      "grad_norm": 8.922310829162598,
      "learning_rate": 0.00048439181916038756,
      "loss": 0.6797,
      "step": 30
    },
    {
      "epoch": 0.00033404812448141724,
      "grad_norm": 5.908444404602051,
      "learning_rate": 0.0004838536060279871,
      "loss": 0.6641,
      "step": 31
    },
    {
      "epoch": 0.0003448238704324307,
      "grad_norm": 8.336326599121094,
      "learning_rate": 0.0004833153928955867,
      "loss": 0.7422,
      "step": 32
    },
    {
      "epoch": 0.00035559961638344413,
      "grad_norm": 8.50965404510498,
      "learning_rate": 0.00048277717976318626,
      "loss": 0.7891,
      "step": 33
    },
    {
      "epoch": 0.0003663753623344576,
      "grad_norm": 5.557366371154785,
      "learning_rate": 0.0004822389666307858,
      "loss": 0.6719,
      "step": 34
    },
    {
      "epoch": 0.0003771511082854711,
      "grad_norm": 0.37071704864501953,
      "learning_rate": 0.0004817007534983854,
      "loss": 0.6953,
      "step": 35
    },
    {
      "epoch": 0.0003879268542364845,
      "grad_norm": 5.816033840179443,
      "learning_rate": 0.00048116254036598496,
      "loss": 0.7656,
      "step": 36
    },
    {
      "epoch": 0.000398702600187498,
      "grad_norm": 2.553257703781128,
      "learning_rate": 0.0004806243272335845,
      "loss": 0.7031,
      "step": 37
    },
    {
      "epoch": 0.0004094783461385114,
      "grad_norm": 4.526583671569824,
      "learning_rate": 0.0004800861141011841,
      "loss": 0.6953,
      "step": 38
    },
    {
      "epoch": 0.0004202540920895249,
      "grad_norm": 9.842535018920898,
      "learning_rate": 0.00047954790096878366,
      "loss": 0.7734,
      "step": 39
    },
    {
      "epoch": 0.00043102983804053837,
      "grad_norm": 0.4583864212036133,
      "learning_rate": 0.0004790096878363832,
      "loss": 0.7031,
      "step": 40
    },
    {
      "epoch": 0.0004418055839915518,
      "grad_norm": 1.3144721984863281,
      "learning_rate": 0.0004784714747039828,
      "loss": 0.6758,
      "step": 41
    },
    {
      "epoch": 0.00045258132994256526,
      "grad_norm": 0.11504288762807846,
      "learning_rate": 0.00047793326157158237,
      "loss": 0.6953,
      "step": 42
    },
    {
      "epoch": 0.0004633570758935787,
      "grad_norm": 5.859048366546631,
      "learning_rate": 0.0004773950484391819,
      "loss": 0.8359,
      "step": 43
    },
    {
      "epoch": 0.0004741328218445922,
      "grad_norm": 0.8226392865180969,
      "learning_rate": 0.0004768568353067815,
      "loss": 0.6953,
      "step": 44
    },
    {
      "epoch": 0.00048490856779560566,
      "grad_norm": 4.871356010437012,
      "learning_rate": 0.00047631862217438107,
      "loss": 0.7422,
      "step": 45
    },
    {
      "epoch": 0.0004956843137466191,
      "grad_norm": 0.7852871417999268,
      "learning_rate": 0.0004757804090419806,
      "loss": 0.6797,
      "step": 46
    },
    {
      "epoch": 0.0005064600596976326,
      "grad_norm": 1.2970898151397705,
      "learning_rate": 0.0004752421959095802,
      "loss": 0.7227,
      "step": 47
    },
    {
      "epoch": 0.000517235805648646,
      "grad_norm": 1.3379088640213013,
      "learning_rate": 0.0004747039827771798,
      "loss": 0.7109,
      "step": 48
    },
    {
      "epoch": 0.0005280115515996595,
      "grad_norm": 0.7468075752258301,
      "learning_rate": 0.00047416576964477936,
      "loss": 0.7031,
      "step": 49
    },
    {
      "epoch": 0.0005387872975506729,
      "grad_norm": 0.44925615191459656,
      "learning_rate": 0.00047362755651237894,
      "loss": 0.7031,
      "step": 50
    },
    {
      "epoch": 0.0005495630435016864,
      "grad_norm": 1.7216135263442993,
      "learning_rate": 0.0004730893433799785,
      "loss": 0.7969,
      "step": 51
    },
    {
      "epoch": 0.0005603387894526999,
      "grad_norm": 0.4494679570198059,
      "learning_rate": 0.00047255113024757806,
      "loss": 0.7031,
      "step": 52
    },
    {
      "epoch": 0.0005711145354037133,
      "grad_norm": 0.16350017488002777,
      "learning_rate": 0.00047201291711517764,
      "loss": 0.6875,
      "step": 53
    },
    {
      "epoch": 0.0005818902813547268,
      "grad_norm": 0.22198203206062317,
      "learning_rate": 0.0004714747039827772,
      "loss": 0.6953,
      "step": 54
    },
    {
      "epoch": 0.0005926660273057403,
      "grad_norm": 0.28978458046913147,
      "learning_rate": 0.00047093649085037676,
      "loss": 0.7031,
      "step": 55
    },
    {
      "epoch": 0.0006034417732567537,
      "grad_norm": 0.11231286823749542,
      "learning_rate": 0.00047039827771797635,
      "loss": 0.6953,
      "step": 56
    },
    {
      "epoch": 0.0006142175192077672,
      "grad_norm": 0.062344279140233994,
      "learning_rate": 0.0004698600645855759,
      "loss": 0.6953,
      "step": 57
    },
    {
      "epoch": 0.0006249932651587806,
      "grad_norm": 0.2770012319087982,
      "learning_rate": 0.00046932185145317546,
      "loss": 0.6953,
      "step": 58
    },
    {
      "epoch": 0.0006357690111097941,
      "grad_norm": 0.15838752686977386,
      "learning_rate": 0.00046878363832077505,
      "loss": 0.6953,
      "step": 59
    },
    {
      "epoch": 0.0006465447570608076,
      "grad_norm": 0.041051506996154785,
      "learning_rate": 0.0004682454251883746,
      "loss": 0.6953,
      "step": 60
    },
    {
      "epoch": 0.000657320503011821,
      "grad_norm": 0.030132250860333443,
      "learning_rate": 0.00046770721205597417,
      "loss": 0.6953,
      "step": 61
    },
    {
      "epoch": 0.0006680962489628345,
      "grad_norm": 0.14417271316051483,
      "learning_rate": 0.00046716899892357375,
      "loss": 0.6992,
      "step": 62
    },
    {
      "epoch": 0.0006788719949138479,
      "grad_norm": 0.06584625691175461,
      "learning_rate": 0.0004666307857911733,
      "loss": 0.6953,
      "step": 63
    },
    {
      "epoch": 0.0006896477408648614,
      "grad_norm": 0.14320559799671173,
      "learning_rate": 0.00046609257265877287,
      "loss": 0.6953,
      "step": 64
    },
    {
      "epoch": 0.0007004234868158749,
      "grad_norm": 0.03932982310652733,
      "learning_rate": 0.00046555435952637245,
      "loss": 0.6953,
      "step": 65
    },
    {
      "epoch": 0.0007111992327668883,
      "grad_norm": 0.024386074393987656,
      "learning_rate": 0.000465016146393972,
      "loss": 0.6953,
      "step": 66
    },
    {
      "epoch": 0.0007219749787179018,
      "grad_norm": 0.06202773377299309,
      "learning_rate": 0.0004644779332615716,
      "loss": 0.6953,
      "step": 67
    },
    {
      "epoch": 0.0007327507246689152,
      "grad_norm": 0.026047902181744576,
      "learning_rate": 0.00046393972012917116,
      "loss": 0.6953,
      "step": 68
    },
    {
      "epoch": 0.0007435264706199287,
      "grad_norm": 0.021200066432356834,
      "learning_rate": 0.00046340150699677074,
      "loss": 0.6953,
      "step": 69
    },
    {
      "epoch": 0.0007543022165709422,
      "grad_norm": 0.1150161400437355,
      "learning_rate": 0.00046286329386437033,
      "loss": 0.6992,
      "step": 70
    },
    {
      "epoch": 0.0007650779625219556,
      "grad_norm": 0.11491447687149048,
      "learning_rate": 0.00046232508073196986,
      "loss": 0.6992,
      "step": 71
    },
    {
      "epoch": 0.000775853708472969,
      "grad_norm": 0.049712248146533966,
      "learning_rate": 0.00046178686759956944,
      "loss": 0.6953,
      "step": 72
    },
    {
      "epoch": 0.0007866294544239826,
      "grad_norm": 0.03645993396639824,
      "learning_rate": 0.00046124865446716903,
      "loss": 0.6953,
      "step": 73
    },
    {
      "epoch": 0.000797405200374996,
      "grad_norm": 0.001515377894975245,
      "learning_rate": 0.00046071044133476856,
      "loss": 0.6953,
      "step": 74
    },
    {
      "epoch": 0.0008081809463260094,
      "grad_norm": 0.06143084168434143,
      "learning_rate": 0.00046017222820236815,
      "loss": 0.6953,
      "step": 75
    },
    {
      "epoch": 0.0008189566922770228,
      "grad_norm": 0.020924333482980728,
      "learning_rate": 0.00045963401506996773,
      "loss": 0.6953,
      "step": 76
    },
    {
      "epoch": 0.0008297324382280363,
      "grad_norm": 0.0008198375580832362,
      "learning_rate": 0.00045909580193756726,
      "loss": 0.6914,
      "step": 77
    },
    {
      "epoch": 0.0008405081841790498,
      "grad_norm": 0.0071079605259001255,
      "learning_rate": 0.00045855758880516685,
      "loss": 0.6953,
      "step": 78
    },
    {
      "epoch": 0.0008512839301300632,
      "grad_norm": 0.014495369978249073,
      "learning_rate": 0.00045801937567276643,
      "loss": 0.6953,
      "step": 79
    },
    {
      "epoch": 0.0008620596760810767,
      "grad_norm": 0.01886623352766037,
      "learning_rate": 0.00045748116254036597,
      "loss": 0.6914,
      "step": 80
    },
    {
      "epoch": 0.0008728354220320901,
      "grad_norm": 0.048742953687906265,
      "learning_rate": 0.00045694294940796555,
      "loss": 0.6953,
      "step": 81
    },
    {
      "epoch": 0.0008836111679831036,
      "grad_norm": 0.014850815758109093,
      "learning_rate": 0.00045640473627556514,
      "loss": 0.6953,
      "step": 82
    },
    {
      "epoch": 0.0008943869139341171,
      "grad_norm": 0.0009989471873268485,
      "learning_rate": 0.00045586652314316467,
      "loss": 0.6914,
      "step": 83
    },
    {
      "epoch": 0.0009051626598851305,
      "grad_norm": 0.018819093704223633,
      "learning_rate": 0.00045532831001076425,
      "loss": 0.6953,
      "step": 84
    },
    {
      "epoch": 0.000915938405836144,
      "grad_norm": 0.012720396742224693,
      "learning_rate": 0.0004547900968783639,
      "loss": 0.6953,
      "step": 85
    },
    {
      "epoch": 0.0009267141517871574,
      "grad_norm": 0.028048809617757797,
      "learning_rate": 0.0004542518837459634,
      "loss": 0.6953,
      "step": 86
    },
    {
      "epoch": 0.0009374898977381709,
      "grad_norm": 0.06389182060956955,
      "learning_rate": 0.000453713670613563,
      "loss": 0.6992,
      "step": 87
    },
    {
      "epoch": 0.0009482656436891844,
      "grad_norm": 0.032145436853170395,
      "learning_rate": 0.00045317545748116254,
      "loss": 0.6914,
      "step": 88
    },
    {
      "epoch": 0.0009590413896401978,
      "grad_norm": 0.012306541204452515,
      "learning_rate": 0.0004526372443487621,
      "loss": 0.6953,
      "step": 89
    },
    {
      "epoch": 0.0009698171355912113,
      "grad_norm": 0.03073877841234207,
      "learning_rate": 0.0004520990312163617,
      "loss": 0.6875,
      "step": 90
    },
    {
      "epoch": 0.0009805928815422248,
      "grad_norm": 0.0021830834448337555,
      "learning_rate": 0.00045156081808396124,
      "loss": 0.6914,
      "step": 91
    },
    {
      "epoch": 0.0009913686274932382,
      "grad_norm": 0.023216793313622475,
      "learning_rate": 0.00045102260495156083,
      "loss": 0.6953,
      "step": 92
    },
    {
      "epoch": 0.0010021443734442516,
      "grad_norm": 0.020133869722485542,
      "learning_rate": 0.0004504843918191604,
      "loss": 0.6953,
      "step": 93
    },
    {
      "epoch": 0.0010129201193952652,
      "grad_norm": 0.030482027679681778,
      "learning_rate": 0.00044994617868675995,
      "loss": 0.6875,
      "step": 94
    },
    {
      "epoch": 0.0010236958653462786,
      "grad_norm": 0.12721019983291626,
      "learning_rate": 0.00044940796555435953,
      "loss": 0.6953,
      "step": 95
    },
    {
      "epoch": 0.001034471611297292,
      "grad_norm": 0.029001332819461823,
      "learning_rate": 0.0004488697524219591,
      "loss": 0.6953,
      "step": 96
    },
    {
      "epoch": 0.0010452473572483056,
      "grad_norm": 0.03435803949832916,
      "learning_rate": 0.00044833153928955865,
      "loss": 0.6953,
      "step": 97
    },
    {
      "epoch": 0.001056023103199319,
      "grad_norm": 0.0018838801188394427,
      "learning_rate": 0.00044779332615715823,
      "loss": 0.6953,
      "step": 98
    },
    {
      "epoch": 0.0010667988491503324,
      "grad_norm": 0.018102528527379036,
      "learning_rate": 0.0004472551130247578,
      "loss": 0.6953,
      "step": 99
    },
    {
      "epoch": 0.0010775745951013458,
      "grad_norm": 0.011329111643135548,
      "learning_rate": 0.00044671689989235735,
      "loss": 0.6953,
      "step": 100
    },
    {
      "epoch": 0.0010883503410523594,
      "grad_norm": 0.051521237939596176,
      "learning_rate": 0.00044617868675995694,
      "loss": 0.6953,
      "step": 101
    },
    {
      "epoch": 0.0010991260870033728,
      "grad_norm": 0.0052434043027460575,
      "learning_rate": 0.0004456404736275565,
      "loss": 0.6953,
      "step": 102
    },
    {
      "epoch": 0.0011099018329543862,
      "grad_norm": 0.05539214611053467,
      "learning_rate": 0.00044510226049515605,
      "loss": 0.6953,
      "step": 103
    },
    {
      "epoch": 0.0011206775789053998,
      "grad_norm": 0.013675270602107048,
      "learning_rate": 0.0004445640473627557,
      "loss": 0.6953,
      "step": 104
    },
    {
      "epoch": 0.0011314533248564132,
      "grad_norm": 0.02283107116818428,
      "learning_rate": 0.0004440258342303553,
      "loss": 0.6953,
      "step": 105
    },
    {
      "epoch": 0.0011422290708074266,
      "grad_norm": 0.03127456083893776,
      "learning_rate": 0.0004434876210979548,
      "loss": 0.6953,
      "step": 106
    },
    {
      "epoch": 0.0011530048167584402,
      "grad_norm": 0.029745033010840416,
      "learning_rate": 0.0004429494079655544,
      "loss": 0.6953,
      "step": 107
    },
    {
      "epoch": 0.0011637805627094536,
      "grad_norm": 0.012453691102564335,
      "learning_rate": 0.0004424111948331539,
      "loss": 0.6953,
      "step": 108
    },
    {
      "epoch": 0.001174556308660467,
      "grad_norm": 0.01366970781236887,
      "learning_rate": 0.0004418729817007535,
      "loss": 0.6953,
      "step": 109
    },
    {
      "epoch": 0.0011853320546114806,
      "grad_norm": 0.02146892435848713,
      "learning_rate": 0.0004413347685683531,
      "loss": 0.6953,
      "step": 110
    },
    {
      "epoch": 0.001196107800562494,
      "grad_norm": 0.013856190256774426,
      "learning_rate": 0.00044079655543595263,
      "loss": 0.6914,
      "step": 111
    },
    {
      "epoch": 0.0012068835465135074,
      "grad_norm": 0.006614057347178459,
      "learning_rate": 0.0004402583423035522,
      "loss": 0.6953,
      "step": 112
    },
    {
      "epoch": 0.0012176592924645208,
      "grad_norm": 0.026580432429909706,
      "learning_rate": 0.0004397201291711518,
      "loss": 0.6953,
      "step": 113
    },
    {
      "epoch": 0.0012284350384155344,
      "grad_norm": 0.029960481449961662,
      "learning_rate": 0.00043918191603875133,
      "loss": 0.6953,
      "step": 114
    },
    {
      "epoch": 0.0012392107843665478,
      "grad_norm": 0.01611967757344246,
      "learning_rate": 0.0004386437029063509,
      "loss": 0.6953,
      "step": 115
    },
    {
      "epoch": 0.0012499865303175612,
      "grad_norm": 0.016840921714901924,
      "learning_rate": 0.0004381054897739505,
      "loss": 0.6953,
      "step": 116
    },
    {
      "epoch": 0.0012607622762685748,
      "grad_norm": 0.01683507300913334,
      "learning_rate": 0.00043756727664155003,
      "loss": 0.6953,
      "step": 117
    },
    {
      "epoch": 0.0012715380222195882,
      "grad_norm": 0.014805804938077927,
      "learning_rate": 0.0004370290635091496,
      "loss": 0.6953,
      "step": 118
    },
    {
      "epoch": 0.0012823137681706015,
      "grad_norm": 0.01535983756184578,
      "learning_rate": 0.0004364908503767492,
      "loss": 0.6953,
      "step": 119
    },
    {
      "epoch": 0.0012930895141216152,
      "grad_norm": 0.04279153421521187,
      "learning_rate": 0.00043595263724434874,
      "loss": 0.6914,
      "step": 120
    },
    {
      "epoch": 0.0013038652600726286,
      "grad_norm": 0.029214264824986458,
      "learning_rate": 0.0004354144241119483,
      "loss": 0.6953,
      "step": 121
    },
    {
      "epoch": 0.001314641006023642,
      "grad_norm": 0.0032109448220580816,
      "learning_rate": 0.00043487621097954796,
      "loss": 0.6953,
      "step": 122
    },
    {
      "epoch": 0.0013254167519746553,
      "grad_norm": 0.014943735674023628,
      "learning_rate": 0.0004343379978471475,
      "loss": 0.6953,
      "step": 123
    },
    {
      "epoch": 0.001336192497925669,
      "grad_norm": 0.030306726694107056,
      "learning_rate": 0.0004337997847147471,
      "loss": 0.6953,
      "step": 124
    },
    {
      "epoch": 0.0013469682438766823,
      "grad_norm": 0.010050998069345951,
      "learning_rate": 0.0004332615715823466,
      "loss": 0.6953,
      "step": 125
    },
    {
      "epoch": 0.0013577439898276957,
      "grad_norm": 0.005367073230445385,
      "learning_rate": 0.0004327233584499462,
      "loss": 0.6953,
      "step": 126
    },
    {
      "epoch": 0.0013685197357787093,
      "grad_norm": 0.028713984414935112,
      "learning_rate": 0.0004321851453175458,
      "loss": 0.6914,
      "step": 127
    },
    {
      "epoch": 0.0013792954817297227,
      "grad_norm": 0.031523555517196655,
      "learning_rate": 0.0004316469321851453,
      "loss": 0.6953,
      "step": 128
    },
    {
      "epoch": 0.0013900712276807361,
      "grad_norm": 0.0025705895386636257,
      "learning_rate": 0.0004311087190527449,
      "loss": 0.6914,
      "step": 129
    },
    {
      "epoch": 0.0014008469736317497,
      "grad_norm": 0.007987374439835548,
      "learning_rate": 0.0004305705059203445,
      "loss": 0.6953,
      "step": 130
    },
    {
      "epoch": 0.0014116227195827631,
      "grad_norm": 0.014997277408838272,
      "learning_rate": 0.000430032292787944,
      "loss": 0.6914,
      "step": 131
    },
    {
      "epoch": 0.0014223984655337765,
      "grad_norm": 0.02974371425807476,
      "learning_rate": 0.0004294940796555436,
      "loss": 0.6953,
      "step": 132
    },
    {
      "epoch": 0.0014331742114847901,
      "grad_norm": 0.009671029634773731,
      "learning_rate": 0.0004289558665231432,
      "loss": 0.6953,
      "step": 133
    },
    {
      "epoch": 0.0014439499574358035,
      "grad_norm": 0.030629880726337433,
      "learning_rate": 0.0004284176533907427,
      "loss": 0.6914,
      "step": 134
    },
    {
      "epoch": 0.001454725703386817,
      "grad_norm": 0.03126155585050583,
      "learning_rate": 0.0004278794402583423,
      "loss": 0.6875,
      "step": 135
    },
    {
      "epoch": 0.0014655014493378303,
      "grad_norm": 0.0007261506980285048,
      "learning_rate": 0.0004273412271259419,
      "loss": 0.6914,
      "step": 136
    },
    {
      "epoch": 0.001476277195288844,
      "grad_norm": 0.004006209783256054,
      "learning_rate": 0.0004268030139935414,
      "loss": 0.6953,
      "step": 137
    },
    {
      "epoch": 0.0014870529412398573,
      "grad_norm": 0.002652860013768077,
      "learning_rate": 0.000426264800861141,
      "loss": 0.6875,
      "step": 138
    },
    {
      "epoch": 0.0014978286871908707,
      "grad_norm": 0.003940967842936516,
      "learning_rate": 0.0004257265877287406,
      "loss": 0.6914,
      "step": 139
    },
    {
      "epoch": 0.0015086044331418843,
      "grad_norm": 0.008003349415957928,
      "learning_rate": 0.0004251883745963401,
      "loss": 0.6875,
      "step": 140
    },
    {
      "epoch": 0.0015193801790928977,
      "grad_norm": 0.019936492666602135,
      "learning_rate": 0.00042465016146393976,
      "loss": 0.6914,
      "step": 141
    },
    {
      "epoch": 0.001530155925043911,
      "grad_norm": 0.0201292484998703,
      "learning_rate": 0.00042411194833153935,
      "loss": 0.6914,
      "step": 142
    },
    {
      "epoch": 0.0015409316709949247,
      "grad_norm": 0.010424052365124226,
      "learning_rate": 0.0004235737351991389,
      "loss": 0.6953,
      "step": 143
    },
    {
      "epoch": 0.001551707416945938,
      "grad_norm": 0.000667188607621938,
      "learning_rate": 0.00042303552206673846,
      "loss": 0.6914,
      "step": 144
    },
    {
      "epoch": 0.0015624831628969515,
      "grad_norm": 0.00712944520637393,
      "learning_rate": 0.000422497308934338,
      "loss": 0.6875,
      "step": 145
    },
    {
      "epoch": 0.001573258908847965,
      "grad_norm": 0.02044042944908142,
      "learning_rate": 0.0004219590958019376,
      "loss": 0.6914,
      "step": 146
    },
    {
      "epoch": 0.0015840346547989785,
      "grad_norm": 0.021057091653347015,
      "learning_rate": 0.00042142088266953717,
      "loss": 0.6953,
      "step": 147
    },
    {
      "epoch": 0.001594810400749992,
      "grad_norm": 0.004307215102016926,
      "learning_rate": 0.0004208826695371367,
      "loss": 0.6914,
      "step": 148
    },
    {
      "epoch": 0.0016055861467010053,
      "grad_norm": 0.003326662117615342,
      "learning_rate": 0.0004203444564047363,
      "loss": 0.6914,
      "step": 149
    },
    {
      "epoch": 0.001616361892652019,
      "grad_norm": 0.005101069808006287,
      "learning_rate": 0.00041980624327233587,
      "loss": 0.6875,
      "step": 150
    },
    {
      "epoch": 0.0016271376386030323,
      "grad_norm": 0.006717669311910868,
      "learning_rate": 0.0004192680301399354,
      "loss": 0.6875,
      "step": 151
    },
    {
      "epoch": 0.0016379133845540457,
      "grad_norm": 0.0198175348341465,
      "learning_rate": 0.000418729817007535,
      "loss": 0.6953,
      "step": 152
    },
    {
      "epoch": 0.0016486891305050593,
      "grad_norm": 0.01992504484951496,
      "learning_rate": 0.00041819160387513457,
      "loss": 0.6875,
      "step": 153
    },
    {
      "epoch": 0.0016594648764560727,
      "grad_norm": 0.003115877043455839,
      "learning_rate": 0.0004176533907427341,
      "loss": 0.6875,
      "step": 154
    },
    {
      "epoch": 0.001670240622407086,
      "grad_norm": 0.004768575541675091,
      "learning_rate": 0.0004171151776103337,
      "loss": 0.6914,
      "step": 155
    },
    {
      "epoch": 0.0016810163683580997,
      "grad_norm": 0.018648702651262283,
      "learning_rate": 0.0004165769644779333,
      "loss": 0.6875,
      "step": 156
    },
    {
      "epoch": 0.001691792114309113,
      "grad_norm": 0.004156444221735001,
      "learning_rate": 0.0004160387513455328,
      "loss": 0.6914,
      "step": 157
    },
    {
      "epoch": 0.0017025678602601265,
      "grad_norm": 0.002553887665271759,
      "learning_rate": 0.0004155005382131324,
      "loss": 0.6914,
      "step": 158
    },
    {
      "epoch": 0.0017133436062111399,
      "grad_norm": 0.01846577785909176,
      "learning_rate": 0.00041496232508073203,
      "loss": 0.6875,
      "step": 159
    },
    {
      "epoch": 0.0017241193521621535,
      "grad_norm": 0.018055586144328117,
      "learning_rate": 0.00041442411194833156,
      "loss": 0.6953,
      "step": 160
    },
    {
      "epoch": 0.0017348950981131669,
      "grad_norm": 0.008523440919816494,
      "learning_rate": 0.00041388589881593115,
      "loss": 0.6914,
      "step": 161
    },
    {
      "epoch": 0.0017456708440641803,
      "grad_norm": 0.007313666399568319,
      "learning_rate": 0.00041334768568353073,
      "loss": 0.6875,
      "step": 162
    },
    {
      "epoch": 0.0017564465900151939,
      "grad_norm": 0.00619418453425169,
      "learning_rate": 0.00041280947255113026,
      "loss": 0.6953,
      "step": 163
    },
    {
      "epoch": 0.0017672223359662073,
      "grad_norm": 0.004469897598028183,
      "learning_rate": 0.00041227125941872985,
      "loss": 0.6914,
      "step": 164
    },
    {
      "epoch": 0.0017779980819172207,
      "grad_norm": 2.0064903905048226e-10,
      "learning_rate": 0.0004117330462863294,
      "loss": 0.6914,
      "step": 165
    },
    {
      "epoch": 0.0017887738278682343,
      "grad_norm": 0.003926629666239023,
      "learning_rate": 0.00041119483315392897,
      "loss": 0.6914,
      "step": 166
    },
    {
      "epoch": 0.0017995495738192477,
      "grad_norm": 0.015950435772538185,
      "learning_rate": 0.00041065662002152855,
      "loss": 0.6875,
      "step": 167
    },
    {
      "epoch": 0.001810325319770261,
      "grad_norm": 0.00025127691333182156,
      "learning_rate": 0.0004101184068891281,
      "loss": 0.6914,
      "step": 168
    },
    {
      "epoch": 0.0018211010657212747,
      "grad_norm": 0.0028654695488512516,
      "learning_rate": 0.00040958019375672767,
      "loss": 0.6875,
      "step": 169
    },
    {
      "epoch": 0.001831876811672288,
      "grad_norm": 0.014520768076181412,
      "learning_rate": 0.00040904198062432725,
      "loss": 0.6875,
      "step": 170
    },
    {
      "epoch": 0.0018426525576233014,
      "grad_norm": 0.0008505797013640404,
      "learning_rate": 0.0004085037674919268,
      "loss": 0.6914,
      "step": 171
    },
    {
      "epoch": 0.0018534283035743148,
      "grad_norm": 0.028126860037446022,
      "learning_rate": 0.00040796555435952637,
      "loss": 0.6914,
      "step": 172
    },
    {
      "epoch": 0.0018642040495253284,
      "grad_norm": 0.041749462485313416,
      "learning_rate": 0.00040742734122712596,
      "loss": 0.6914,
      "step": 173
    },
    {
      "epoch": 0.0018749797954763418,
      "grad_norm": 0.00034972027060575783,
      "learning_rate": 0.0004068891280947255,
      "loss": 0.6914,
      "step": 174
    },
    {
      "epoch": 0.0018857555414273552,
      "grad_norm": 0.007847758010029793,
      "learning_rate": 0.00040635091496232507,
      "loss": 0.6914,
      "step": 175
    },
    {
      "epoch": 0.0018965312873783688,
      "grad_norm": 0.015299703925848007,
      "learning_rate": 0.00040581270182992466,
      "loss": 0.6953,
      "step": 176
    },
    {
      "epoch": 0.0019073070333293822,
      "grad_norm": 0.0056711542420089245,
      "learning_rate": 0.0004052744886975242,
      "loss": 0.6953,
      "step": 177
    },
    {
      "epoch": 0.0019180827792803956,
      "grad_norm": 0.0033114817924797535,
      "learning_rate": 0.00040473627556512383,
      "loss": 0.6914,
      "step": 178
    },
    {
      "epoch": 0.0019288585252314092,
      "grad_norm": 0.017580734565854073,
      "learning_rate": 0.0004041980624327234,
      "loss": 0.6953,
      "step": 179
    },
    {
      "epoch": 0.0019396342711824226,
      "grad_norm": 0.015868302434682846,
      "learning_rate": 0.00040365984930032295,
      "loss": 0.6914,
      "step": 180
    },
    {
      "epoch": 0.001950410017133436,
      "grad_norm": 0.0005387014825828373,
      "learning_rate": 0.00040312163616792253,
      "loss": 0.6914,
      "step": 181
    },
    {
      "epoch": 0.0019611857630844496,
      "grad_norm": 0.008335382677614689,
      "learning_rate": 0.00040258342303552206,
      "loss": 0.6953,
      "step": 182
    },
    {
      "epoch": 0.001971961509035463,
      "grad_norm": 0.016068344935774803,
      "learning_rate": 0.00040204520990312165,
      "loss": 0.6953,
      "step": 183
    },
    {
      "epoch": 0.0019827372549864764,
      "grad_norm": 0.004869932308793068,
      "learning_rate": 0.00040150699677072123,
      "loss": 0.6875,
      "step": 184
    },
    {
      "epoch": 0.00199351300093749,
      "grad_norm": 0.004749581217765808,
      "learning_rate": 0.00040096878363832077,
      "loss": 0.6875,
      "step": 185
    },
    {
      "epoch": 0.002004288746888503,
      "grad_norm": 0.01617790199816227,
      "learning_rate": 0.00040043057050592035,
      "loss": 0.6914,
      "step": 186
    },
    {
      "epoch": 0.002015064492839517,
      "grad_norm": 0.004567855503410101,
      "learning_rate": 0.00039989235737351994,
      "loss": 0.6914,
      "step": 187
    },
    {
      "epoch": 0.0020258402387905304,
      "grad_norm": 0.003815695643424988,
      "learning_rate": 0.00039935414424111947,
      "loss": 0.6914,
      "step": 188
    },
    {
      "epoch": 0.002036615984741544,
      "grad_norm": 0.01571294106543064,
      "learning_rate": 0.00039881593110871905,
      "loss": 0.6953,
      "step": 189
    },
    {
      "epoch": 0.002047391730692557,
      "grad_norm": 0.005921507254242897,
      "learning_rate": 0.00039827771797631864,
      "loss": 0.6914,
      "step": 190
    },
    {
      "epoch": 0.0020581674766435706,
      "grad_norm": 0.004730383399873972,
      "learning_rate": 0.00039773950484391817,
      "loss": 0.6914,
      "step": 191
    },
    {
      "epoch": 0.002068943222594584,
      "grad_norm": 0.0029887829441577196,
      "learning_rate": 0.00039720129171151776,
      "loss": 0.6914,
      "step": 192
    },
    {
      "epoch": 0.0020797189685455974,
      "grad_norm": 0.015911784023046494,
      "learning_rate": 0.00039666307857911734,
      "loss": 0.6914,
      "step": 193
    },
    {
      "epoch": 0.002090494714496611,
      "grad_norm": 0.0032152095809578896,
      "learning_rate": 0.00039612486544671687,
      "loss": 0.6875,
      "step": 194
    },
    {
      "epoch": 0.0021012704604476246,
      "grad_norm": 0.01677742414176464,
      "learning_rate": 0.00039558665231431646,
      "loss": 0.6875,
      "step": 195
    },
    {
      "epoch": 0.002112046206398638,
      "grad_norm": 0.007944020442664623,
      "learning_rate": 0.00039504843918191604,
      "loss": 0.6953,
      "step": 196
    },
    {
      "epoch": 0.0021228219523496514,
      "grad_norm": 0.009435789659619331,
      "learning_rate": 0.00039451022604951563,
      "loss": 0.6953,
      "step": 197
    },
    {
      "epoch": 0.002133597698300665,
      "grad_norm": 0.015353367663919926,
      "learning_rate": 0.0003939720129171152,
      "loss": 0.6953,
      "step": 198
    },
    {
      "epoch": 0.002144373444251678,
      "grad_norm": 0.0340539813041687,
      "learning_rate": 0.0003934337997847148,
      "loss": 0.6953,
      "step": 199
    },
    {
      "epoch": 0.0021551491902026916,
      "grad_norm": 0.015367909334599972,
      "learning_rate": 0.00039289558665231433,
      "loss": 0.6914,
      "step": 200
    }
  ],
  "logging_steps": 1,
  "max_steps": 929,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
