{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.0010775745951013458,
  "eval_steps": 500,
  "global_step": 100,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 1.0775745951013459e-05,
      "grad_norm": 11.734519004821777,
      "learning_rate": 0.0005,
      "loss": 0.7383,
      "step": 1
    },
    {
      "epoch": 2.1551491902026918e-05,
      "grad_norm": 34.50665283203125,
      "learning_rate": 0.0004994617868675996,
      "loss": 0.7266,
      "step": 2
    },
    {
      "epoch": 3.2327237853040375e-05,
      "grad_norm": 18.584224700927734,
      "learning_rate": 0.0004989235737351992,
      "loss": 0.5781,
      "step": 3
    },
    {
      "epoch": 4.3102983804053835e-05,
      "grad_norm": 10.856942176818848,
      "learning_rate": 0.0004983853606027987,
      "loss": 0.6094,
      "step": 4
    },
    {
      "epoch": 5.3878729755067296e-05,
      "grad_norm": 15.20124626159668,
      "learning_rate": 0.0004978471474703982,
      "loss": 0.7188,
      "step": 5
    },
    {
      "epoch": 6.465447570608075e-05,
      "grad_norm": 38.783966064453125,
      "learning_rate": 0.0004973089343379978,
      "loss": 1.0938,
      "step": 6
    },
    {
      "epoch": 7.543022165709421e-05,
      "grad_norm": 12.740062713623047,
      "learning_rate": 0.0004967707212055974,
      "loss": 0.2539,
      "step": 7
    },
    {
      "epoch": 8.620596760810767e-05,
      "grad_norm": 24.5117130279541,
      "learning_rate": 0.000496232508073197,
      "loss": 0.8203,
      "step": 8
    },
    {
      "epoch": 9.698171355912113e-05,
      "grad_norm": 19.737266540527344,
      "learning_rate": 0.0004956942949407966,
      "loss": 0.9375,
      "step": 9
    },
    {
      "epoch": 0.00010775745951013459,
      "grad_norm": 6.9918670654296875,
      "learning_rate": 0.0004951560818083961,
      "loss": 0.707,
      "step": 10
    },
    {
      "epoch": 0.00011853320546114805,
      "grad_norm": 11.607245445251465,
      "learning_rate": 0.0004946178686759958,
      "loss": 0.5898,
      "step": 11
    },
    {
      "epoch": 0.0001293089514121615,
      "grad_norm": 31.876806259155273,
      "learning_rate": 0.0004940796555435953,
      "loss": 1.3594,
      "step": 12
    },
    {
      "epoch": 0.00014008469736317497,
      "grad_norm": 12.43700122833252,
      "learning_rate": 0.0004935414424111948,
      "loss": 0.8047,
      "step": 13
    },
    {
      "epoch": 0.00015086044331418842,
      "grad_norm": 7.036546230316162,
      "learning_rate": 0.0004930032292787944,
      "loss": 0.625,
      "step": 14
    },
    {
      "epoch": 0.0001616361892652019,
      "grad_norm": 16.671363830566406,
      "learning_rate": 0.000492465016146394,
      "loss": 0.6953,
      "step": 15
    },
    {
      "epoch": 0.00017241193521621534,
      "grad_norm": 11.222743034362793,
      "learning_rate": 0.0004919268030139936,
      "loss": 0.7266,
      "step": 16
    },
    {
      "epoch": 0.0001831876811672288,
      "grad_norm": 20.321971893310547,
      "learning_rate": 0.0004913885898815932,
      "loss": 0.6719,
      "step": 17
    },
    {
      "epoch": 0.00019396342711824226,
      "grad_norm": 43.40980529785156,
      "learning_rate": 0.0004908503767491927,
      "loss": 1.5312,
      "step": 18
    },
    {
      "epoch": 0.0002047391730692557,
      "grad_norm": 8.459393501281738,
      "learning_rate": 0.0004903121636167922,
      "loss": 0.7422,
      "step": 19
    },
    {
      "epoch": 0.00021551491902026918,
      "grad_norm": 7.693176746368408,
      "learning_rate": 0.0004897739504843918,
      "loss": 0.5898,
      "step": 20
    },
    {
      "epoch": 0.00022629066497128263,
      "grad_norm": 3.173727512359619,
      "learning_rate": 0.0004892357373519914,
      "loss": 0.7109,
      "step": 21
    },
    {
      "epoch": 0.0002370664109222961,
      "grad_norm": 6.842949390411377,
      "learning_rate": 0.000488697524219591,
      "loss": 0.7109,
      "step": 22
    },
    {
      "epoch": 0.00024784215687330955,
      "grad_norm": 1.0444997549057007,
      "learning_rate": 0.0004881593110871905,
      "loss": 0.6992,
      "step": 23
    },
    {
      "epoch": 0.000258617902824323,
      "grad_norm": 8.681434631347656,
      "learning_rate": 0.0004876210979547901,
      "loss": 0.7266,
      "step": 24
    },
    {
      "epoch": 0.00026939364877533645,
      "grad_norm": 20.266357421875,
      "learning_rate": 0.0004870828848223897,
      "loss": 0.8008,
      "step": 25
    },
    {
      "epoch": 0.00028016939472634995,
      "grad_norm": 25.284927368164062,
      "learning_rate": 0.0004865446716899892,
      "loss": 0.6562,
      "step": 26
    },
    {
      "epoch": 0.0002909451406773634,
      "grad_norm": 8.273880958557129,
      "learning_rate": 0.0004860064585575888,
      "loss": 0.7109,
      "step": 27
    },
    {
      "epoch": 0.00030172088662837684,
      "grad_norm": 6.550272464752197,
      "learning_rate": 0.0004854682454251884,
      "loss": 0.6484,
      "step": 28
    },
    {
      "epoch": 0.0003124966325793903,
      "grad_norm": 12.523265838623047,
      "learning_rate": 0.00048493003229278797,
      "loss": 0.6133,
      "step": 29
    },
    {
      "epoch": 0.0003232723785304038,
      "grad_norm": 8.922310829162598,
      "learning_rate": 0.00048439181916038756,
      "loss": 0.6797,
      "step": 30
    },
    {
      "epoch": 0.00033404812448141724,
      "grad_norm": 5.908444404602051,
      "learning_rate": 0.0004838536060279871,
      "loss": 0.6641,
      "step": 31
    },
    {
      "epoch": 0.0003448238704324307,
      "grad_norm": 8.336326599121094,
      "learning_rate": 0.0004833153928955867,
      "loss": 0.7422,
      "step": 32
    },
    {
      "epoch": 0.00035559961638344413,
      "grad_norm": 8.50965404510498,
      "learning_rate": 0.00048277717976318626,
      "loss": 0.7891,
      "step": 33
    },
    {
      "epoch": 0.0003663753623344576,
      "grad_norm": 5.557366371154785,
      "learning_rate": 0.0004822389666307858,
      "loss": 0.6719,
      "step": 34
    },
    {
      "epoch": 0.0003771511082854711,
      "grad_norm": 0.37071704864501953,
      "learning_rate": 0.0004817007534983854,
      "loss": 0.6953,
      "step": 35
    },
    {
      "epoch": 0.0003879268542364845,
      "grad_norm": 5.816033840179443,
      "learning_rate": 0.00048116254036598496,
      "loss": 0.7656,
      "step": 36
    },
    {
      "epoch": 0.000398702600187498,
      "grad_norm": 2.553257703781128,
      "learning_rate": 0.0004806243272335845,
      "loss": 0.7031,
      "step": 37
    },
    {
      "epoch": 0.0004094783461385114,
      "grad_norm": 4.526583671569824,
      "learning_rate": 0.0004800861141011841,
      "loss": 0.6953,
      "step": 38
    },
    {
      "epoch": 0.0004202540920895249,
      "grad_norm": 9.842535018920898,
      "learning_rate": 0.00047954790096878366,
      "loss": 0.7734,
      "step": 39
    },
    {
      "epoch": 0.00043102983804053837,
      "grad_norm": 0.4583864212036133,
      "learning_rate": 0.0004790096878363832,
      "loss": 0.7031,
      "step": 40
    },
    {
      "epoch": 0.0004418055839915518,
      "grad_norm": 1.3144721984863281,
      "learning_rate": 0.0004784714747039828,
      "loss": 0.6758,
      "step": 41
    },
    {
      "epoch": 0.00045258132994256526,
      "grad_norm": 0.11504288762807846,
      "learning_rate": 0.00047793326157158237,
      "loss": 0.6953,
      "step": 42
    },
    {
      "epoch": 0.0004633570758935787,
      "grad_norm": 5.859048366546631,
      "learning_rate": 0.0004773950484391819,
      "loss": 0.8359,
      "step": 43
    },
    {
      "epoch": 0.0004741328218445922,
      "grad_norm": 0.8226392865180969,
      "learning_rate": 0.0004768568353067815,
      "loss": 0.6953,
      "step": 44
    },
    {
      "epoch": 0.00048490856779560566,
      "grad_norm": 4.871356010437012,
      "learning_rate": 0.00047631862217438107,
      "loss": 0.7422,
      "step": 45
    },
    {
      "epoch": 0.0004956843137466191,
      "grad_norm": 0.7852871417999268,
      "learning_rate": 0.0004757804090419806,
      "loss": 0.6797,
      "step": 46
    },
    {
      "epoch": 0.0005064600596976326,
      "grad_norm": 1.2970898151397705,
      "learning_rate": 0.0004752421959095802,
      "loss": 0.7227,
      "step": 47
    },
    {
      "epoch": 0.000517235805648646,
      "grad_norm": 1.3379088640213013,
      "learning_rate": 0.0004747039827771798,
      "loss": 0.7109,
      "step": 48
    },
    {
      "epoch": 0.0005280115515996595,
      "grad_norm": 0.7468075752258301,
      "learning_rate": 0.00047416576964477936,
      "loss": 0.7031,
      "step": 49
    },
    {
      "epoch": 0.0005387872975506729,
      "grad_norm": 0.44925615191459656,
      "learning_rate": 0.00047362755651237894,
      "loss": 0.7031,
      "step": 50
    },
    {
      "epoch": 0.0005495630435016864,
      "grad_norm": 1.7216135263442993,
      "learning_rate": 0.0004730893433799785,
      "loss": 0.7969,
      "step": 51
    },
    {
      "epoch": 0.0005603387894526999,
      "grad_norm": 0.4494679570198059,
      "learning_rate": 0.00047255113024757806,
      "loss": 0.7031,
      "step": 52
    },
    {
      "epoch": 0.0005711145354037133,
      "grad_norm": 0.16350017488002777,
      "learning_rate": 0.00047201291711517764,
      "loss": 0.6875,
      "step": 53
    },
    {
      "epoch": 0.0005818902813547268,
      "grad_norm": 0.22198203206062317,
      "learning_rate": 0.0004714747039827772,
      "loss": 0.6953,
      "step": 54
    },
    {
      "epoch": 0.0005926660273057403,
      "grad_norm": 0.28978458046913147,
      "learning_rate": 0.00047093649085037676,
      "loss": 0.7031,
      "step": 55
    },
    {
      "epoch": 0.0006034417732567537,
      "grad_norm": 0.11231286823749542,
      "learning_rate": 0.00047039827771797635,
      "loss": 0.6953,
      "step": 56
    },
    {
      "epoch": 0.0006142175192077672,
      "grad_norm": 0.062344279140233994,
      "learning_rate": 0.0004698600645855759,
      "loss": 0.6953,
      "step": 57
    },
    {
      "epoch": 0.0006249932651587806,
      "grad_norm": 0.2770012319087982,
      "learning_rate": 0.00046932185145317546,
      "loss": 0.6953,
      "step": 58
    },
    {
      "epoch": 0.0006357690111097941,
      "grad_norm": 0.15838752686977386,
      "learning_rate": 0.00046878363832077505,
      "loss": 0.6953,
      "step": 59
    },
    {
      "epoch": 0.0006465447570608076,
      "grad_norm": 0.041051506996154785,
      "learning_rate": 0.0004682454251883746,
      "loss": 0.6953,
      "step": 60
    },
    {
      "epoch": 0.000657320503011821,
      "grad_norm": 0.030132250860333443,
      "learning_rate": 0.00046770721205597417,
      "loss": 0.6953,
      "step": 61
    },
    {
      "epoch": 0.0006680962489628345,
      "grad_norm": 0.14417271316051483,
      "learning_rate": 0.00046716899892357375,
      "loss": 0.6992,
      "step": 62
    },
    {
      "epoch": 0.0006788719949138479,
      "grad_norm": 0.06584625691175461,
      "learning_rate": 0.0004666307857911733,
      "loss": 0.6953,
      "step": 63
    },
    {
      "epoch": 0.0006896477408648614,
      "grad_norm": 0.14320559799671173,
      "learning_rate": 0.00046609257265877287,
      "loss": 0.6953,
      "step": 64
    },
    {
      "epoch": 0.0007004234868158749,
      "grad_norm": 0.03932982310652733,
      "learning_rate": 0.00046555435952637245,
      "loss": 0.6953,
      "step": 65
    },
    {
      "epoch": 0.0007111992327668883,
      "grad_norm": 0.024386074393987656,
      "learning_rate": 0.000465016146393972,
      "loss": 0.6953,
      "step": 66
    },
    {
      "epoch": 0.0007219749787179018,
      "grad_norm": 0.06202773377299309,
      "learning_rate": 0.0004644779332615716,
      "loss": 0.6953,
      "step": 67
    },
    {
      "epoch": 0.0007327507246689152,
      "grad_norm": 0.026047902181744576,
      "learning_rate": 0.00046393972012917116,
      "loss": 0.6953,
      "step": 68
    },
    {
      "epoch": 0.0007435264706199287,
      "grad_norm": 0.021200066432356834,
      "learning_rate": 0.00046340150699677074,
      "loss": 0.6953,
      "step": 69
    },
    {
      "epoch": 0.0007543022165709422,
      "grad_norm": 0.1150161400437355,
      "learning_rate": 0.00046286329386437033,
      "loss": 0.6992,
      "step": 70
    },
    {
      "epoch": 0.0007650779625219556,
      "grad_norm": 0.11491447687149048,
      "learning_rate": 0.00046232508073196986,
      "loss": 0.6992,
      "step": 71
    },
    {
      "epoch": 0.000775853708472969,
      "grad_norm": 0.049712248146533966,
      "learning_rate": 0.00046178686759956944,
      "loss": 0.6953,
      "step": 72
    },
    {
      "epoch": 0.0007866294544239826,
      "grad_norm": 0.03645993396639824,
      "learning_rate": 0.00046124865446716903,
      "loss": 0.6953,
      "step": 73
    },
    {
      "epoch": 0.000797405200374996,
      "grad_norm": 0.001515377894975245,
      "learning_rate": 0.00046071044133476856,
      "loss": 0.6953,
      "step": 74
    },
    {
      "epoch": 0.0008081809463260094,
      "grad_norm": 0.06143084168434143,
      "learning_rate": 0.00046017222820236815,
      "loss": 0.6953,
      "step": 75
    },
    {
      "epoch": 0.0008189566922770228,
      "grad_norm": 0.020924333482980728,
      "learning_rate": 0.00045963401506996773,
      "loss": 0.6953,
      "step": 76
    },
    {
      "epoch": 0.0008297324382280363,
      "grad_norm": 0.0008198375580832362,
      "learning_rate": 0.00045909580193756726,
      "loss": 0.6914,
      "step": 77
    },
    {
      "epoch": 0.0008405081841790498,
      "grad_norm": 0.0071079605259001255,
      "learning_rate": 0.00045855758880516685,
      "loss": 0.6953,
      "step": 78
    },
    {
      "epoch": 0.0008512839301300632,
      "grad_norm": 0.014495369978249073,
      "learning_rate": 0.00045801937567276643,
      "loss": 0.6953,
      "step": 79
    },
    {
      "epoch": 0.0008620596760810767,
      "grad_norm": 0.01886623352766037,
      "learning_rate": 0.00045748116254036597,
      "loss": 0.6914,
      "step": 80
    },
    {
      "epoch": 0.0008728354220320901,
      "grad_norm": 0.048742953687906265,
      "learning_rate": 0.00045694294940796555,
      "loss": 0.6953,
      "step": 81
    },
    {
      "epoch": 0.0008836111679831036,
      "grad_norm": 0.014850815758109093,
      "learning_rate": 0.00045640473627556514,
      "loss": 0.6953,
      "step": 82
    },
    {
      "epoch": 0.0008943869139341171,
      "grad_norm": 0.0009989471873268485,
      "learning_rate": 0.00045586652314316467,
      "loss": 0.6914,
      "step": 83
    },
    {
      "epoch": 0.0009051626598851305,
      "grad_norm": 0.018819093704223633,
      "learning_rate": 0.00045532831001076425,
      "loss": 0.6953,
      "step": 84
    },
    {
      "epoch": 0.000915938405836144,
      "grad_norm": 0.012720396742224693,
      "learning_rate": 0.0004547900968783639,
      "loss": 0.6953,
      "step": 85
    },
    {
      "epoch": 0.0009267141517871574,
      "grad_norm": 0.028048809617757797,
      "learning_rate": 0.0004542518837459634,
      "loss": 0.6953,
      "step": 86
    },
    {
      "epoch": 0.0009374898977381709,
      "grad_norm": 0.06389182060956955,
      "learning_rate": 0.000453713670613563,
      "loss": 0.6992,
      "step": 87
    },
    {
      "epoch": 0.0009482656436891844,
      "grad_norm": 0.032145436853170395,
      "learning_rate": 0.00045317545748116254,
      "loss": 0.6914,
      "step": 88
    },
    {
      "epoch": 0.0009590413896401978,
      "grad_norm": 0.012306541204452515,
      "learning_rate": 0.0004526372443487621,
      "loss": 0.6953,
      "step": 89
    },
    {
      "epoch": 0.0009698171355912113,
      "grad_norm": 0.03073877841234207,
      "learning_rate": 0.0004520990312163617,
      "loss": 0.6875,
      "step": 90
    },
    {
      "epoch": 0.0009805928815422248,
      "grad_norm": 0.0021830834448337555,
      "learning_rate": 0.00045156081808396124,
      "loss": 0.6914,
      "step": 91
    },
    {
      "epoch": 0.0009913686274932382,
      "grad_norm": 0.023216793313622475,
      "learning_rate": 0.00045102260495156083,
      "loss": 0.6953,
      "step": 92
    },
    {
      "epoch": 0.0010021443734442516,
      "grad_norm": 0.020133869722485542,
      "learning_rate": 0.0004504843918191604,
      "loss": 0.6953,
      "step": 93
    },
    {
      "epoch": 0.0010129201193952652,
      "grad_norm": 0.030482027679681778,
      "learning_rate": 0.00044994617868675995,
      "loss": 0.6875,
      "step": 94
    },
    {
      "epoch": 0.0010236958653462786,
      "grad_norm": 0.12721019983291626,
      "learning_rate": 0.00044940796555435953,
      "loss": 0.6953,
      "step": 95
    },
    {
      "epoch": 0.001034471611297292,
      "grad_norm": 0.029001332819461823,
      "learning_rate": 0.0004488697524219591,
      "loss": 0.6953,
      "step": 96
    },
    {
      "epoch": 0.0010452473572483056,
      "grad_norm": 0.03435803949832916,
      "learning_rate": 0.00044833153928955865,
      "loss": 0.6953,
      "step": 97
    },
    {
      "epoch": 0.001056023103199319,
      "grad_norm": 0.0018838801188394427,
      "learning_rate": 0.00044779332615715823,
      "loss": 0.6953,
      "step": 98
    },
    {
      "epoch": 0.0010667988491503324,
      "grad_norm": 0.018102528527379036,
      "learning_rate": 0.0004472551130247578,
      "loss": 0.6953,
      "step": 99
    },
    {
      "epoch": 0.0010775745951013458,
      "grad_norm": 0.011329111643135548,
      "learning_rate": 0.00044671689989235735,
      "loss": 0.6953,
      "step": 100
    }
  ],
  "logging_steps": 1,
  "max_steps": 929,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 100,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
