{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7216308858019124,
  "eval_steps": 500,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007216308858019123,
      "grad_norm": 18.95465660095215,
      "learning_rate": 0.0,
      "loss": 28.2603,
      "step": 1
    },
    {
      "epoch": 0.0014432617716038246,
      "grad_norm": 11.818717956542969,
      "learning_rate": 2.5e-07,
      "loss": 28.0854,
      "step": 2
    },
    {
      "epoch": 0.002164892657405737,
      "grad_norm": 12.667237281799316,
      "learning_rate": 5e-07,
      "loss": 28.1762,
      "step": 3
    },
    {
      "epoch": 0.002886523543207649,
      "grad_norm": 15.911036491394043,
      "learning_rate": 7.5e-07,
      "loss": 28.2053,
      "step": 4
    },
    {
      "epoch": 0.0036081544290095615,
      "grad_norm": 10.541961669921875,
      "learning_rate": 1e-06,
      "loss": 28.1661,
      "step": 5
    },
    {
      "epoch": 0.004329785314811474,
      "grad_norm": 10.470471382141113,
      "learning_rate": 1.25e-06,
      "loss": 28.1917,
      "step": 6
    },
    {
      "epoch": 0.0050514162006133866,
      "grad_norm": 10.154924392700195,
      "learning_rate": 1.5e-06,
      "loss": 28.1091,
      "step": 7
    },
    {
      "epoch": 0.005773047086415298,
      "grad_norm": 10.80612850189209,
      "learning_rate": 1.75e-06,
      "loss": 28.2068,
      "step": 8
    },
    {
      "epoch": 0.006494677972217211,
      "grad_norm": 10.231884002685547,
      "learning_rate": 2e-06,
      "loss": 28.0648,
      "step": 9
    },
    {
      "epoch": 0.007216308858019123,
      "grad_norm": 10.253085136413574,
      "learning_rate": 2.25e-06,
      "loss": 28.0271,
      "step": 10
    },
    {
      "epoch": 0.007937939743821036,
      "grad_norm": 10.603231430053711,
      "learning_rate": 2.5e-06,
      "loss": 28.1228,
      "step": 11
    },
    {
      "epoch": 0.008659570629622948,
      "grad_norm": 10.29156494140625,
      "learning_rate": 2.75e-06,
      "loss": 28.0709,
      "step": 12
    },
    {
      "epoch": 0.00938120151542486,
      "grad_norm": 10.864797592163086,
      "learning_rate": 3e-06,
      "loss": 28.1977,
      "step": 13
    },
    {
      "epoch": 0.010102832401226773,
      "grad_norm": 11.46993350982666,
      "learning_rate": 3.25e-06,
      "loss": 28.0867,
      "step": 14
    },
    {
      "epoch": 0.010824463287028685,
      "grad_norm": 10.653419494628906,
      "learning_rate": 3.5e-06,
      "loss": 28.1523,
      "step": 15
    },
    {
      "epoch": 0.011546094172830597,
      "grad_norm": 10.731359481811523,
      "learning_rate": 3.75e-06,
      "loss": 28.1358,
      "step": 16
    },
    {
      "epoch": 0.012267725058632509,
      "grad_norm": 12.860328674316406,
      "learning_rate": 4e-06,
      "loss": 28.0633,
      "step": 17
    },
    {
      "epoch": 0.012989355944434422,
      "grad_norm": 9.783295631408691,
      "learning_rate": 4.250000000000001e-06,
      "loss": 28.1193,
      "step": 18
    },
    {
      "epoch": 0.013710986830236334,
      "grad_norm": 10.505288124084473,
      "learning_rate": 4.5e-06,
      "loss": 28.0568,
      "step": 19
    },
    {
      "epoch": 0.014432617716038246,
      "grad_norm": 9.824366569519043,
      "learning_rate": 4.75e-06,
      "loss": 27.9635,
      "step": 20
    },
    {
      "epoch": 0.015154248601840158,
      "grad_norm": 9.62389087677002,
      "learning_rate": 5e-06,
      "loss": 28.112,
      "step": 21
    },
    {
      "epoch": 0.01587587948764207,
      "grad_norm": 9.012532234191895,
      "learning_rate": 5.2500000000000006e-06,
      "loss": 28.0026,
      "step": 22
    },
    {
      "epoch": 0.016597510373443983,
      "grad_norm": 8.8641939163208,
      "learning_rate": 5.5e-06,
      "loss": 28.0652,
      "step": 23
    },
    {
      "epoch": 0.017319141259245895,
      "grad_norm": 10.27700138092041,
      "learning_rate": 5.75e-06,
      "loss": 28.2289,
      "step": 24
    },
    {
      "epoch": 0.018040772145047807,
      "grad_norm": 8.663352012634277,
      "learning_rate": 6e-06,
      "loss": 28.0419,
      "step": 25
    },
    {
      "epoch": 0.01876240303084972,
      "grad_norm": 9.896668434143066,
      "learning_rate": 6.25e-06,
      "loss": 28.0959,
      "step": 26
    },
    {
      "epoch": 0.019484033916651634,
      "grad_norm": 8.751734733581543,
      "learning_rate": 6.5e-06,
      "loss": 28.1442,
      "step": 27
    },
    {
      "epoch": 0.020205664802453546,
      "grad_norm": 11.638421058654785,
      "learning_rate": 6.75e-06,
      "loss": 28.1013,
      "step": 28
    },
    {
      "epoch": 0.020927295688255458,
      "grad_norm": 9.028443336486816,
      "learning_rate": 7e-06,
      "loss": 28.116,
      "step": 29
    },
    {
      "epoch": 0.02164892657405737,
      "grad_norm": 9.643932342529297,
      "learning_rate": 7.250000000000001e-06,
      "loss": 28.0642,
      "step": 30
    },
    {
      "epoch": 0.022370557459859282,
      "grad_norm": 9.98048210144043,
      "learning_rate": 7.5e-06,
      "loss": 28.0481,
      "step": 31
    },
    {
      "epoch": 0.023092188345661194,
      "grad_norm": 8.922532081604004,
      "learning_rate": 7.75e-06,
      "loss": 28.0245,
      "step": 32
    },
    {
      "epoch": 0.023813819231463106,
      "grad_norm": 10.240415573120117,
      "learning_rate": 8e-06,
      "loss": 28.0206,
      "step": 33
    },
    {
      "epoch": 0.024535450117265017,
      "grad_norm": 8.243294715881348,
      "learning_rate": 8.25e-06,
      "loss": 27.9991,
      "step": 34
    },
    {
      "epoch": 0.025257081003066933,
      "grad_norm": 7.914638042449951,
      "learning_rate": 8.500000000000002e-06,
      "loss": 28.0152,
      "step": 35
    },
    {
      "epoch": 0.025978711888868845,
      "grad_norm": 8.505498886108398,
      "learning_rate": 8.750000000000001e-06,
      "loss": 28.0381,
      "step": 36
    },
    {
      "epoch": 0.026700342774670757,
      "grad_norm": 7.684713363647461,
      "learning_rate": 9e-06,
      "loss": 28.0257,
      "step": 37
    },
    {
      "epoch": 0.02742197366047267,
      "grad_norm": 8.707976341247559,
      "learning_rate": 9.25e-06,
      "loss": 28.0207,
      "step": 38
    },
    {
      "epoch": 0.02814360454627458,
      "grad_norm": 8.254925727844238,
      "learning_rate": 9.5e-06,
      "loss": 27.9785,
      "step": 39
    },
    {
      "epoch": 0.028865235432076492,
      "grad_norm": 8.12933349609375,
      "learning_rate": 9.75e-06,
      "loss": 27.9651,
      "step": 40
    },
    {
      "epoch": 0.029586866317878404,
      "grad_norm": 8.593865394592285,
      "learning_rate": 1e-05,
      "loss": 27.9783,
      "step": 41
    },
    {
      "epoch": 0.030308497203680316,
      "grad_norm": 8.000003814697266,
      "learning_rate": 1.025e-05,
      "loss": 27.9299,
      "step": 42
    },
    {
      "epoch": 0.03103012808948223,
      "grad_norm": 7.354893684387207,
      "learning_rate": 1.0500000000000001e-05,
      "loss": 27.8949,
      "step": 43
    },
    {
      "epoch": 0.03175175897528414,
      "grad_norm": 8.013198852539062,
      "learning_rate": 1.0749999999999999e-05,
      "loss": 27.8373,
      "step": 44
    },
    {
      "epoch": 0.032473389861086055,
      "grad_norm": 8.153124809265137,
      "learning_rate": 1.1e-05,
      "loss": 27.8653,
      "step": 45
    },
    {
      "epoch": 0.03319502074688797,
      "grad_norm": 7.7599568367004395,
      "learning_rate": 1.1249999999999999e-05,
      "loss": 27.9197,
      "step": 46
    },
    {
      "epoch": 0.03391665163268988,
      "grad_norm": 7.6983842849731445,
      "learning_rate": 1.15e-05,
      "loss": 27.8794,
      "step": 47
    },
    {
      "epoch": 0.03463828251849179,
      "grad_norm": 7.777843475341797,
      "learning_rate": 1.1750000000000001e-05,
      "loss": 27.905,
      "step": 48
    },
    {
      "epoch": 0.0353599134042937,
      "grad_norm": 7.272737979888916,
      "learning_rate": 1.2e-05,
      "loss": 27.866,
      "step": 49
    },
    {
      "epoch": 0.036081544290095614,
      "grad_norm": 7.567008018493652,
      "learning_rate": 1.2250000000000001e-05,
      "loss": 27.8936,
      "step": 50
    },
    {
      "epoch": 0.036803175175897526,
      "grad_norm": 6.192119121551514,
      "learning_rate": 1.25e-05,
      "loss": 27.8849,
      "step": 51
    },
    {
      "epoch": 0.03752480606169944,
      "grad_norm": 6.441014766693115,
      "learning_rate": 1.275e-05,
      "loss": 27.9307,
      "step": 52
    },
    {
      "epoch": 0.03824643694750135,
      "grad_norm": 6.281425952911377,
      "learning_rate": 1.3e-05,
      "loss": 27.8767,
      "step": 53
    },
    {
      "epoch": 0.03896806783330327,
      "grad_norm": 7.045971393585205,
      "learning_rate": 1.325e-05,
      "loss": 27.8564,
      "step": 54
    },
    {
      "epoch": 0.03968969871910518,
      "grad_norm": 5.650415420532227,
      "learning_rate": 1.35e-05,
      "loss": 27.8133,
      "step": 55
    },
    {
      "epoch": 0.04041132960490709,
      "grad_norm": 7.127648830413818,
      "learning_rate": 1.375e-05,
      "loss": 27.8799,
      "step": 56
    },
    {
      "epoch": 0.041132960490709004,
      "grad_norm": 5.203252792358398,
      "learning_rate": 1.4e-05,
      "loss": 27.8575,
      "step": 57
    },
    {
      "epoch": 0.041854591376510916,
      "grad_norm": 6.236037731170654,
      "learning_rate": 1.425e-05,
      "loss": 27.7969,
      "step": 58
    },
    {
      "epoch": 0.04257622226231283,
      "grad_norm": 6.168045520782471,
      "learning_rate": 1.4500000000000002e-05,
      "loss": 27.7954,
      "step": 59
    },
    {
      "epoch": 0.04329785314811474,
      "grad_norm": 6.008028984069824,
      "learning_rate": 1.475e-05,
      "loss": 27.8602,
      "step": 60
    },
    {
      "epoch": 0.04401948403391665,
      "grad_norm": 7.720793724060059,
      "learning_rate": 1.5e-05,
      "loss": 27.7812,
      "step": 61
    },
    {
      "epoch": 0.044741114919718564,
      "grad_norm": 4.598146915435791,
      "learning_rate": 1.525e-05,
      "loss": 27.8263,
      "step": 62
    },
    {
      "epoch": 0.045462745805520476,
      "grad_norm": 7.328726768493652,
      "learning_rate": 1.55e-05,
      "loss": 27.9022,
      "step": 63
    },
    {
      "epoch": 0.04618437669132239,
      "grad_norm": 5.2075324058532715,
      "learning_rate": 1.575e-05,
      "loss": 27.8061,
      "step": 64
    },
    {
      "epoch": 0.0469060075771243,
      "grad_norm": 4.927052021026611,
      "learning_rate": 1.6e-05,
      "loss": 27.7925,
      "step": 65
    },
    {
      "epoch": 0.04762763846292621,
      "grad_norm": 5.40087366104126,
      "learning_rate": 1.6250000000000002e-05,
      "loss": 27.8498,
      "step": 66
    },
    {
      "epoch": 0.04834926934872812,
      "grad_norm": 4.708817481994629,
      "learning_rate": 1.65e-05,
      "loss": 27.7621,
      "step": 67
    },
    {
      "epoch": 0.049070900234530035,
      "grad_norm": 5.249624252319336,
      "learning_rate": 1.675e-05,
      "loss": 27.8126,
      "step": 68
    },
    {
      "epoch": 0.04979253112033195,
      "grad_norm": 4.393229007720947,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 27.7574,
      "step": 69
    },
    {
      "epoch": 0.050514162006133866,
      "grad_norm": 4.29892110824585,
      "learning_rate": 1.7250000000000003e-05,
      "loss": 27.7787,
      "step": 70
    },
    {
      "epoch": 0.05123579289193578,
      "grad_norm": 4.768892765045166,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 27.8019,
      "step": 71
    },
    {
      "epoch": 0.05195742377773769,
      "grad_norm": 4.601158142089844,
      "learning_rate": 1.7749999999999998e-05,
      "loss": 27.8055,
      "step": 72
    },
    {
      "epoch": 0.0526790546635396,
      "grad_norm": 3.992032527923584,
      "learning_rate": 1.8e-05,
      "loss": 27.7507,
      "step": 73
    },
    {
      "epoch": 0.05340068554934151,
      "grad_norm": 4.769450664520264,
      "learning_rate": 1.825e-05,
      "loss": 27.795,
      "step": 74
    },
    {
      "epoch": 0.054122316435143425,
      "grad_norm": 3.8284835815429688,
      "learning_rate": 1.85e-05,
      "loss": 27.8202,
      "step": 75
    },
    {
      "epoch": 0.05484394732094534,
      "grad_norm": 3.9189696311950684,
      "learning_rate": 1.875e-05,
      "loss": 27.7648,
      "step": 76
    },
    {
      "epoch": 0.05556557820674725,
      "grad_norm": 4.129658222198486,
      "learning_rate": 1.9e-05,
      "loss": 27.7605,
      "step": 77
    },
    {
      "epoch": 0.05628720909254916,
      "grad_norm": 4.175869464874268,
      "learning_rate": 1.925e-05,
      "loss": 27.7534,
      "step": 78
    },
    {
      "epoch": 0.05700883997835107,
      "grad_norm": 4.393113613128662,
      "learning_rate": 1.95e-05,
      "loss": 27.7505,
      "step": 79
    },
    {
      "epoch": 0.057730470864152984,
      "grad_norm": 4.138369083404541,
      "learning_rate": 1.975e-05,
      "loss": 27.6963,
      "step": 80
    },
    {
      "epoch": 0.058452101749954896,
      "grad_norm": 4.991808891296387,
      "learning_rate": 2e-05,
      "loss": 27.7006,
      "step": 81
    },
    {
      "epoch": 0.05917373263575681,
      "grad_norm": 3.217952013015747,
      "learning_rate": 2.025e-05,
      "loss": 27.7413,
      "step": 82
    },
    {
      "epoch": 0.05989536352155872,
      "grad_norm": 3.350095748901367,
      "learning_rate": 2.05e-05,
      "loss": 27.7005,
      "step": 83
    },
    {
      "epoch": 0.06061699440736063,
      "grad_norm": 3.412418842315674,
      "learning_rate": 2.0750000000000003e-05,
      "loss": 27.7645,
      "step": 84
    },
    {
      "epoch": 0.06133862529316255,
      "grad_norm": 3.30617618560791,
      "learning_rate": 2.1000000000000002e-05,
      "loss": 27.699,
      "step": 85
    },
    {
      "epoch": 0.06206025617896446,
      "grad_norm": 3.5113017559051514,
      "learning_rate": 2.125e-05,
      "loss": 27.7152,
      "step": 86
    },
    {
      "epoch": 0.06278188706476637,
      "grad_norm": 3.422590732574463,
      "learning_rate": 2.1499999999999997e-05,
      "loss": 27.6913,
      "step": 87
    },
    {
      "epoch": 0.06350351795056829,
      "grad_norm": 3.1620960235595703,
      "learning_rate": 2.175e-05,
      "loss": 27.7259,
      "step": 88
    },
    {
      "epoch": 0.06422514883637019,
      "grad_norm": 3.9169931411743164,
      "learning_rate": 2.2e-05,
      "loss": 27.6927,
      "step": 89
    },
    {
      "epoch": 0.06494677972217211,
      "grad_norm": 3.497143030166626,
      "learning_rate": 2.225e-05,
      "loss": 27.697,
      "step": 90
    },
    {
      "epoch": 0.06566841060797401,
      "grad_norm": 3.2006285190582275,
      "learning_rate": 2.2499999999999998e-05,
      "loss": 27.6899,
      "step": 91
    },
    {
      "epoch": 0.06639004149377593,
      "grad_norm": 4.194845199584961,
      "learning_rate": 2.275e-05,
      "loss": 27.7243,
      "step": 92
    },
    {
      "epoch": 0.06711167237957785,
      "grad_norm": 3.410010814666748,
      "learning_rate": 2.3e-05,
      "loss": 27.7172,
      "step": 93
    },
    {
      "epoch": 0.06783330326537976,
      "grad_norm": 4.010298728942871,
      "learning_rate": 2.325e-05,
      "loss": 27.6264,
      "step": 94
    },
    {
      "epoch": 0.06855493415118168,
      "grad_norm": 3.9952683448791504,
      "learning_rate": 2.3500000000000002e-05,
      "loss": 27.7197,
      "step": 95
    },
    {
      "epoch": 0.06927656503698358,
      "grad_norm": 3.6541130542755127,
      "learning_rate": 2.375e-05,
      "loss": 27.6644,
      "step": 96
    },
    {
      "epoch": 0.0699981959227855,
      "grad_norm": 3.3707971572875977,
      "learning_rate": 2.4e-05,
      "loss": 27.6864,
      "step": 97
    },
    {
      "epoch": 0.0707198268085874,
      "grad_norm": 3.896512746810913,
      "learning_rate": 2.425e-05,
      "loss": 27.6682,
      "step": 98
    },
    {
      "epoch": 0.07144145769438932,
      "grad_norm": 4.151542663574219,
      "learning_rate": 2.4500000000000003e-05,
      "loss": 27.64,
      "step": 99
    },
    {
      "epoch": 0.07216308858019123,
      "grad_norm": 3.6034114360809326,
      "learning_rate": 2.4750000000000002e-05,
      "loss": 27.6713,
      "step": 100
    },
    {
      "epoch": 0.07288471946599315,
      "grad_norm": 3.290820360183716,
      "learning_rate": 2.5e-05,
      "loss": 27.677,
      "step": 101
    },
    {
      "epoch": 0.07360635035179505,
      "grad_norm": 4.361812591552734,
      "learning_rate": 2.525e-05,
      "loss": 27.6779,
      "step": 102
    },
    {
      "epoch": 0.07432798123759697,
      "grad_norm": 4.183075428009033,
      "learning_rate": 2.55e-05,
      "loss": 27.6283,
      "step": 103
    },
    {
      "epoch": 0.07504961212339888,
      "grad_norm": 4.45012092590332,
      "learning_rate": 2.575e-05,
      "loss": 27.6035,
      "step": 104
    },
    {
      "epoch": 0.0757712430092008,
      "grad_norm": 4.676006317138672,
      "learning_rate": 2.6e-05,
      "loss": 27.5739,
      "step": 105
    },
    {
      "epoch": 0.0764928738950027,
      "grad_norm": 5.10124397277832,
      "learning_rate": 2.625e-05,
      "loss": 27.5739,
      "step": 106
    },
    {
      "epoch": 0.07721450478080462,
      "grad_norm": 5.399838924407959,
      "learning_rate": 2.65e-05,
      "loss": 27.4759,
      "step": 107
    },
    {
      "epoch": 0.07793613566660654,
      "grad_norm": 6.75576639175415,
      "learning_rate": 2.675e-05,
      "loss": 27.4009,
      "step": 108
    },
    {
      "epoch": 0.07865776655240844,
      "grad_norm": 5.919723987579346,
      "learning_rate": 2.7e-05,
      "loss": 27.5331,
      "step": 109
    },
    {
      "epoch": 0.07937939743821036,
      "grad_norm": 6.054313659667969,
      "learning_rate": 2.725e-05,
      "loss": 27.5782,
      "step": 110
    },
    {
      "epoch": 0.08010102832401227,
      "grad_norm": 7.132836818695068,
      "learning_rate": 2.75e-05,
      "loss": 27.4466,
      "step": 111
    },
    {
      "epoch": 0.08082265920981418,
      "grad_norm": 9.062591552734375,
      "learning_rate": 2.775e-05,
      "loss": 27.4718,
      "step": 112
    },
    {
      "epoch": 0.08154429009561609,
      "grad_norm": 7.146115303039551,
      "learning_rate": 2.8e-05,
      "loss": 27.5365,
      "step": 113
    },
    {
      "epoch": 0.08226592098141801,
      "grad_norm": 7.362349987030029,
      "learning_rate": 2.8250000000000002e-05,
      "loss": 27.5937,
      "step": 114
    },
    {
      "epoch": 0.08298755186721991,
      "grad_norm": 7.140463829040527,
      "learning_rate": 2.85e-05,
      "loss": 27.4697,
      "step": 115
    },
    {
      "epoch": 0.08370918275302183,
      "grad_norm": 8.680808067321777,
      "learning_rate": 2.875e-05,
      "loss": 27.4817,
      "step": 116
    },
    {
      "epoch": 0.08443081363882374,
      "grad_norm": 10.795662879943848,
      "learning_rate": 2.9000000000000004e-05,
      "loss": 27.6263,
      "step": 117
    },
    {
      "epoch": 0.08515244452462566,
      "grad_norm": 10.680480003356934,
      "learning_rate": 2.9250000000000003e-05,
      "loss": 27.3142,
      "step": 118
    },
    {
      "epoch": 0.08587407541042756,
      "grad_norm": 10.578190803527832,
      "learning_rate": 2.95e-05,
      "loss": 27.28,
      "step": 119
    },
    {
      "epoch": 0.08659570629622948,
      "grad_norm": 10.457042694091797,
      "learning_rate": 2.9749999999999998e-05,
      "loss": 27.4058,
      "step": 120
    },
    {
      "epoch": 0.08731733718203138,
      "grad_norm": 13.507946014404297,
      "learning_rate": 3e-05,
      "loss": 27.4065,
      "step": 121
    },
    {
      "epoch": 0.0880389680678333,
      "grad_norm": 12.146306037902832,
      "learning_rate": 3.025e-05,
      "loss": 27.308,
      "step": 122
    },
    {
      "epoch": 0.08876059895363522,
      "grad_norm": 11.13593578338623,
      "learning_rate": 3.05e-05,
      "loss": 27.2945,
      "step": 123
    },
    {
      "epoch": 0.08948222983943713,
      "grad_norm": 18.19460678100586,
      "learning_rate": 3.075e-05,
      "loss": 27.5014,
      "step": 124
    },
    {
      "epoch": 0.09020386072523905,
      "grad_norm": 13.412571907043457,
      "learning_rate": 3.1e-05,
      "loss": 27.12,
      "step": 125
    },
    {
      "epoch": 0.09092549161104095,
      "grad_norm": 14.1115083694458,
      "learning_rate": 3.125e-05,
      "loss": 27.2775,
      "step": 126
    },
    {
      "epoch": 0.09164712249684287,
      "grad_norm": 13.612897872924805,
      "learning_rate": 3.15e-05,
      "loss": 27.4133,
      "step": 127
    },
    {
      "epoch": 0.09236875338264477,
      "grad_norm": 13.778083801269531,
      "learning_rate": 3.175e-05,
      "loss": 27.2679,
      "step": 128
    },
    {
      "epoch": 0.0930903842684467,
      "grad_norm": 13.223956108093262,
      "learning_rate": 3.2e-05,
      "loss": 27.3148,
      "step": 129
    },
    {
      "epoch": 0.0938120151542486,
      "grad_norm": 18.562232971191406,
      "learning_rate": 3.2250000000000005e-05,
      "loss": 27.1064,
      "step": 130
    },
    {
      "epoch": 0.09453364604005052,
      "grad_norm": 14.165409088134766,
      "learning_rate": 3.2500000000000004e-05,
      "loss": 27.0707,
      "step": 131
    },
    {
      "epoch": 0.09525527692585242,
      "grad_norm": 15.728447914123535,
      "learning_rate": 3.275e-05,
      "loss": 27.1732,
      "step": 132
    },
    {
      "epoch": 0.09597690781165434,
      "grad_norm": 23.205596923828125,
      "learning_rate": 3.3e-05,
      "loss": 27.1108,
      "step": 133
    },
    {
      "epoch": 0.09669853869745625,
      "grad_norm": 16.06588363647461,
      "learning_rate": 3.325e-05,
      "loss": 27.0209,
      "step": 134
    },
    {
      "epoch": 0.09742016958325816,
      "grad_norm": 14.09426212310791,
      "learning_rate": 3.35e-05,
      "loss": 27.0158,
      "step": 135
    },
    {
      "epoch": 0.09814180046906007,
      "grad_norm": 16.24164390563965,
      "learning_rate": 3.375e-05,
      "loss": 27.0477,
      "step": 136
    },
    {
      "epoch": 0.09886343135486199,
      "grad_norm": 18.623849868774414,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 27.1262,
      "step": 137
    },
    {
      "epoch": 0.0995850622406639,
      "grad_norm": 16.59412956237793,
      "learning_rate": 3.4250000000000006e-05,
      "loss": 27.1519,
      "step": 138
    },
    {
      "epoch": 0.10030669312646581,
      "grad_norm": 15.146452903747559,
      "learning_rate": 3.4500000000000005e-05,
      "loss": 27.1564,
      "step": 139
    },
    {
      "epoch": 0.10102832401226773,
      "grad_norm": 16.5521183013916,
      "learning_rate": 3.4750000000000004e-05,
      "loss": 27.2449,
      "step": 140
    },
    {
      "epoch": 0.10174995489806964,
      "grad_norm": 15.227888107299805,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 26.9085,
      "step": 141
    },
    {
      "epoch": 0.10247158578387155,
      "grad_norm": 16.130691528320312,
      "learning_rate": 3.5249999999999996e-05,
      "loss": 27.0359,
      "step": 142
    },
    {
      "epoch": 0.10319321666967346,
      "grad_norm": 19.033308029174805,
      "learning_rate": 3.5499999999999996e-05,
      "loss": 26.9245,
      "step": 143
    },
    {
      "epoch": 0.10391484755547538,
      "grad_norm": 16.34724235534668,
      "learning_rate": 3.5749999999999995e-05,
      "loss": 26.8964,
      "step": 144
    },
    {
      "epoch": 0.10463647844127728,
      "grad_norm": 15.18592643737793,
      "learning_rate": 3.6e-05,
      "loss": 26.8669,
      "step": 145
    },
    {
      "epoch": 0.1053581093270792,
      "grad_norm": 16.48932647705078,
      "learning_rate": 3.625e-05,
      "loss": 27.1644,
      "step": 146
    },
    {
      "epoch": 0.10607974021288111,
      "grad_norm": 26.09713363647461,
      "learning_rate": 3.65e-05,
      "loss": 26.8624,
      "step": 147
    },
    {
      "epoch": 0.10680137109868303,
      "grad_norm": 22.517789840698242,
      "learning_rate": 3.675e-05,
      "loss": 26.98,
      "step": 148
    },
    {
      "epoch": 0.10752300198448493,
      "grad_norm": 16.754953384399414,
      "learning_rate": 3.7e-05,
      "loss": 26.921,
      "step": 149
    },
    {
      "epoch": 0.10824463287028685,
      "grad_norm": 23.424482345581055,
      "learning_rate": 3.725e-05,
      "loss": 26.7666,
      "step": 150
    },
    {
      "epoch": 0.10896626375608875,
      "grad_norm": 23.278766632080078,
      "learning_rate": 3.75e-05,
      "loss": 26.6679,
      "step": 151
    },
    {
      "epoch": 0.10968789464189067,
      "grad_norm": 18.9605655670166,
      "learning_rate": 3.775e-05,
      "loss": 26.9305,
      "step": 152
    },
    {
      "epoch": 0.11040952552769258,
      "grad_norm": 18.412696838378906,
      "learning_rate": 3.8e-05,
      "loss": 26.8966,
      "step": 153
    },
    {
      "epoch": 0.1111311564134945,
      "grad_norm": 32.85456466674805,
      "learning_rate": 3.825e-05,
      "loss": 26.6809,
      "step": 154
    },
    {
      "epoch": 0.11185278729929642,
      "grad_norm": 28.412784576416016,
      "learning_rate": 3.85e-05,
      "loss": 26.6017,
      "step": 155
    },
    {
      "epoch": 0.11257441818509832,
      "grad_norm": 21.103639602661133,
      "learning_rate": 3.875e-05,
      "loss": 26.6523,
      "step": 156
    },
    {
      "epoch": 0.11329604907090024,
      "grad_norm": 44.84275817871094,
      "learning_rate": 3.9e-05,
      "loss": 26.7628,
      "step": 157
    },
    {
      "epoch": 0.11401767995670214,
      "grad_norm": 48.00034713745117,
      "learning_rate": 3.925e-05,
      "loss": 26.855,
      "step": 158
    },
    {
      "epoch": 0.11473931084250406,
      "grad_norm": 43.470176696777344,
      "learning_rate": 3.95e-05,
      "loss": 26.4971,
      "step": 159
    },
    {
      "epoch": 0.11546094172830597,
      "grad_norm": 21.441734313964844,
      "learning_rate": 3.9750000000000004e-05,
      "loss": 26.3835,
      "step": 160
    },
    {
      "epoch": 0.11618257261410789,
      "grad_norm": 24.007707595825195,
      "learning_rate": 4e-05,
      "loss": 26.6087,
      "step": 161
    },
    {
      "epoch": 0.11690420349990979,
      "grad_norm": 35.16439437866211,
      "learning_rate": 4.025e-05,
      "loss": 27.0302,
      "step": 162
    },
    {
      "epoch": 0.11762583438571171,
      "grad_norm": 23.325544357299805,
      "learning_rate": 4.05e-05,
      "loss": 26.5322,
      "step": 163
    },
    {
      "epoch": 0.11834746527151362,
      "grad_norm": 20.601314544677734,
      "learning_rate": 4.075e-05,
      "loss": 26.9663,
      "step": 164
    },
    {
      "epoch": 0.11906909615731553,
      "grad_norm": 23.14185333251953,
      "learning_rate": 4.1e-05,
      "loss": 26.3173,
      "step": 165
    },
    {
      "epoch": 0.11979072704311744,
      "grad_norm": 31.1231746673584,
      "learning_rate": 4.125e-05,
      "loss": 26.4701,
      "step": 166
    },
    {
      "epoch": 0.12051235792891936,
      "grad_norm": 28.501611709594727,
      "learning_rate": 4.1500000000000006e-05,
      "loss": 26.7119,
      "step": 167
    },
    {
      "epoch": 0.12123398881472126,
      "grad_norm": 28.403554916381836,
      "learning_rate": 4.1750000000000005e-05,
      "loss": 26.8165,
      "step": 168
    },
    {
      "epoch": 0.12195561970052318,
      "grad_norm": 25.06870460510254,
      "learning_rate": 4.2000000000000004e-05,
      "loss": 26.6424,
      "step": 169
    },
    {
      "epoch": 0.1226772505863251,
      "grad_norm": 25.603879928588867,
      "learning_rate": 4.2250000000000004e-05,
      "loss": 26.4498,
      "step": 170
    },
    {
      "epoch": 0.123398881472127,
      "grad_norm": 22.547985076904297,
      "learning_rate": 4.25e-05,
      "loss": 26.4535,
      "step": 171
    },
    {
      "epoch": 0.12412051235792892,
      "grad_norm": 26.762365341186523,
      "learning_rate": 4.275e-05,
      "loss": 26.6167,
      "step": 172
    },
    {
      "epoch": 0.12484214324373083,
      "grad_norm": 23.407054901123047,
      "learning_rate": 4.2999999999999995e-05,
      "loss": 26.5118,
      "step": 173
    },
    {
      "epoch": 0.12556377412953273,
      "grad_norm": 28.50487518310547,
      "learning_rate": 4.325e-05,
      "loss": 26.1285,
      "step": 174
    },
    {
      "epoch": 0.12628540501533467,
      "grad_norm": 42.86870574951172,
      "learning_rate": 4.35e-05,
      "loss": 26.0321,
      "step": 175
    },
    {
      "epoch": 0.12700703590113657,
      "grad_norm": 42.767147064208984,
      "learning_rate": 4.375e-05,
      "loss": 26.5661,
      "step": 176
    },
    {
      "epoch": 0.12772866678693848,
      "grad_norm": 39.33689498901367,
      "learning_rate": 4.4e-05,
      "loss": 26.6415,
      "step": 177
    },
    {
      "epoch": 0.12845029767274038,
      "grad_norm": 29.1854248046875,
      "learning_rate": 4.425e-05,
      "loss": 26.3081,
      "step": 178
    },
    {
      "epoch": 0.12917192855854231,
      "grad_norm": 32.37710189819336,
      "learning_rate": 4.45e-05,
      "loss": 25.9333,
      "step": 179
    },
    {
      "epoch": 0.12989355944434422,
      "grad_norm": 57.66329574584961,
      "learning_rate": 4.475e-05,
      "loss": 26.0104,
      "step": 180
    },
    {
      "epoch": 0.13061519033014612,
      "grad_norm": 80.00399780273438,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 26.4829,
      "step": 181
    },
    {
      "epoch": 0.13133682121594803,
      "grad_norm": 43.42656326293945,
      "learning_rate": 4.525e-05,
      "loss": 26.485,
      "step": 182
    },
    {
      "epoch": 0.13205845210174996,
      "grad_norm": 56.701629638671875,
      "learning_rate": 4.55e-05,
      "loss": 25.7863,
      "step": 183
    },
    {
      "epoch": 0.13278008298755187,
      "grad_norm": 51.76976776123047,
      "learning_rate": 4.575e-05,
      "loss": 25.7458,
      "step": 184
    },
    {
      "epoch": 0.13350171387335377,
      "grad_norm": 35.60039138793945,
      "learning_rate": 4.6e-05,
      "loss": 26.5153,
      "step": 185
    },
    {
      "epoch": 0.1342233447591557,
      "grad_norm": 35.445865631103516,
      "learning_rate": 4.625e-05,
      "loss": 25.9802,
      "step": 186
    },
    {
      "epoch": 0.1349449756449576,
      "grad_norm": 51.72404861450195,
      "learning_rate": 4.65e-05,
      "loss": 26.1671,
      "step": 187
    },
    {
      "epoch": 0.13566660653075951,
      "grad_norm": 81.45526123046875,
      "learning_rate": 4.675e-05,
      "loss": 26.0753,
      "step": 188
    },
    {
      "epoch": 0.13638823741656142,
      "grad_norm": 47.53855514526367,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 26.0026,
      "step": 189
    },
    {
      "epoch": 0.13710986830236335,
      "grad_norm": 35.924903869628906,
      "learning_rate": 4.725e-05,
      "loss": 25.9098,
      "step": 190
    },
    {
      "epoch": 0.13783149918816526,
      "grad_norm": 54.370018005371094,
      "learning_rate": 4.75e-05,
      "loss": 25.7615,
      "step": 191
    },
    {
      "epoch": 0.13855313007396716,
      "grad_norm": 31.43960189819336,
      "learning_rate": 4.775e-05,
      "loss": 26.1651,
      "step": 192
    },
    {
      "epoch": 0.13927476095976907,
      "grad_norm": 30.914955139160156,
      "learning_rate": 4.8e-05,
      "loss": 25.8459,
      "step": 193
    },
    {
      "epoch": 0.139996391845571,
      "grad_norm": 44.084716796875,
      "learning_rate": 4.825e-05,
      "loss": 25.9404,
      "step": 194
    },
    {
      "epoch": 0.1407180227313729,
      "grad_norm": 35.10764694213867,
      "learning_rate": 4.85e-05,
      "loss": 25.6199,
      "step": 195
    },
    {
      "epoch": 0.1414396536171748,
      "grad_norm": 44.43756103515625,
      "learning_rate": 4.8750000000000006e-05,
      "loss": 25.8345,
      "step": 196
    },
    {
      "epoch": 0.14216128450297671,
      "grad_norm": 45.32453155517578,
      "learning_rate": 4.9000000000000005e-05,
      "loss": 25.862,
      "step": 197
    },
    {
      "epoch": 0.14288291538877865,
      "grad_norm": 57.325897216796875,
      "learning_rate": 4.9250000000000004e-05,
      "loss": 25.7391,
      "step": 198
    },
    {
      "epoch": 0.14360454627458055,
      "grad_norm": 39.3889045715332,
      "learning_rate": 4.9500000000000004e-05,
      "loss": 25.7323,
      "step": 199
    },
    {
      "epoch": 0.14432617716038246,
      "grad_norm": 98.62346649169922,
      "learning_rate": 4.975e-05,
      "loss": 25.4056,
      "step": 200
    },
    {
      "epoch": 0.1450478080461844,
      "grad_norm": 61.92494583129883,
      "learning_rate": 5e-05,
      "loss": 25.6912,
      "step": 201
    },
    {
      "epoch": 0.1457694389319863,
      "grad_norm": 39.65530014038086,
      "learning_rate": 5.025e-05,
      "loss": 25.1669,
      "step": 202
    },
    {
      "epoch": 0.1464910698177882,
      "grad_norm": 53.236907958984375,
      "learning_rate": 5.05e-05,
      "loss": 25.1839,
      "step": 203
    },
    {
      "epoch": 0.1472127007035901,
      "grad_norm": 49.23750305175781,
      "learning_rate": 5.075000000000001e-05,
      "loss": 25.729,
      "step": 204
    },
    {
      "epoch": 0.14793433158939204,
      "grad_norm": 41.08412170410156,
      "learning_rate": 5.1e-05,
      "loss": 24.9664,
      "step": 205
    },
    {
      "epoch": 0.14865596247519394,
      "grad_norm": 96.0106201171875,
      "learning_rate": 5.125e-05,
      "loss": 25.1812,
      "step": 206
    },
    {
      "epoch": 0.14937759336099585,
      "grad_norm": 65.08269500732422,
      "learning_rate": 5.15e-05,
      "loss": 25.2878,
      "step": 207
    },
    {
      "epoch": 0.15009922424679775,
      "grad_norm": 90.53666687011719,
      "learning_rate": 5.175e-05,
      "loss": 25.5626,
      "step": 208
    },
    {
      "epoch": 0.15082085513259968,
      "grad_norm": 69.79690551757812,
      "learning_rate": 5.2e-05,
      "loss": 25.1289,
      "step": 209
    },
    {
      "epoch": 0.1515424860184016,
      "grad_norm": 63.284297943115234,
      "learning_rate": 5.2249999999999996e-05,
      "loss": 24.8167,
      "step": 210
    },
    {
      "epoch": 0.1522641169042035,
      "grad_norm": 47.74066162109375,
      "learning_rate": 5.25e-05,
      "loss": 25.2523,
      "step": 211
    },
    {
      "epoch": 0.1529857477900054,
      "grad_norm": 41.12062454223633,
      "learning_rate": 5.275e-05,
      "loss": 25.1412,
      "step": 212
    },
    {
      "epoch": 0.15370737867580733,
      "grad_norm": 75.05960083007812,
      "learning_rate": 5.3e-05,
      "loss": 24.7081,
      "step": 213
    },
    {
      "epoch": 0.15442900956160924,
      "grad_norm": 68.82237243652344,
      "learning_rate": 5.325e-05,
      "loss": 25.0177,
      "step": 214
    },
    {
      "epoch": 0.15515064044741114,
      "grad_norm": 60.63269805908203,
      "learning_rate": 5.35e-05,
      "loss": 24.9607,
      "step": 215
    },
    {
      "epoch": 0.15587227133321307,
      "grad_norm": 61.61161804199219,
      "learning_rate": 5.375e-05,
      "loss": 24.6342,
      "step": 216
    },
    {
      "epoch": 0.15659390221901498,
      "grad_norm": 63.29292297363281,
      "learning_rate": 5.4e-05,
      "loss": 24.6214,
      "step": 217
    },
    {
      "epoch": 0.15731553310481688,
      "grad_norm": 77.14925384521484,
      "learning_rate": 5.4250000000000004e-05,
      "loss": 24.943,
      "step": 218
    },
    {
      "epoch": 0.1580371639906188,
      "grad_norm": 63.458168029785156,
      "learning_rate": 5.45e-05,
      "loss": 24.9727,
      "step": 219
    },
    {
      "epoch": 0.15875879487642072,
      "grad_norm": 58.43949508666992,
      "learning_rate": 5.475e-05,
      "loss": 24.6694,
      "step": 220
    },
    {
      "epoch": 0.15948042576222263,
      "grad_norm": 53.516639709472656,
      "learning_rate": 5.5e-05,
      "loss": 24.4394,
      "step": 221
    },
    {
      "epoch": 0.16020205664802453,
      "grad_norm": 66.4712905883789,
      "learning_rate": 5.525e-05,
      "loss": 24.4106,
      "step": 222
    },
    {
      "epoch": 0.16092368753382644,
      "grad_norm": 74.04080963134766,
      "learning_rate": 5.55e-05,
      "loss": 23.9252,
      "step": 223
    },
    {
      "epoch": 0.16164531841962837,
      "grad_norm": 61.53580093383789,
      "learning_rate": 5.575e-05,
      "loss": 24.7455,
      "step": 224
    },
    {
      "epoch": 0.16236694930543027,
      "grad_norm": 133.8290557861328,
      "learning_rate": 5.6e-05,
      "loss": 24.9212,
      "step": 225
    },
    {
      "epoch": 0.16308858019123218,
      "grad_norm": 66.14763641357422,
      "learning_rate": 5.6250000000000005e-05,
      "loss": 24.5223,
      "step": 226
    },
    {
      "epoch": 0.16381021107703408,
      "grad_norm": 63.36686706542969,
      "learning_rate": 5.6500000000000005e-05,
      "loss": 24.307,
      "step": 227
    },
    {
      "epoch": 0.16453184196283602,
      "grad_norm": 108.16033935546875,
      "learning_rate": 5.6750000000000004e-05,
      "loss": 24.146,
      "step": 228
    },
    {
      "epoch": 0.16525347284863792,
      "grad_norm": 82.5708236694336,
      "learning_rate": 5.7e-05,
      "loss": 24.1528,
      "step": 229
    },
    {
      "epoch": 0.16597510373443983,
      "grad_norm": 79.0079574584961,
      "learning_rate": 5.725e-05,
      "loss": 23.429,
      "step": 230
    },
    {
      "epoch": 0.16669673462024176,
      "grad_norm": 85.09661102294922,
      "learning_rate": 5.75e-05,
      "loss": 23.9156,
      "step": 231
    },
    {
      "epoch": 0.16741836550604366,
      "grad_norm": 54.28445053100586,
      "learning_rate": 5.775e-05,
      "loss": 24.0607,
      "step": 232
    },
    {
      "epoch": 0.16813999639184557,
      "grad_norm": 60.4278564453125,
      "learning_rate": 5.800000000000001e-05,
      "loss": 23.8022,
      "step": 233
    },
    {
      "epoch": 0.16886162727764747,
      "grad_norm": 85.20455169677734,
      "learning_rate": 5.8250000000000006e-05,
      "loss": 23.612,
      "step": 234
    },
    {
      "epoch": 0.1695832581634494,
      "grad_norm": 89.46427917480469,
      "learning_rate": 5.8500000000000006e-05,
      "loss": 24.2461,
      "step": 235
    },
    {
      "epoch": 0.1703048890492513,
      "grad_norm": 83.85401153564453,
      "learning_rate": 5.875e-05,
      "loss": 23.6805,
      "step": 236
    },
    {
      "epoch": 0.17102651993505322,
      "grad_norm": 66.14202880859375,
      "learning_rate": 5.9e-05,
      "loss": 23.4641,
      "step": 237
    },
    {
      "epoch": 0.17174815082085512,
      "grad_norm": 72.26224517822266,
      "learning_rate": 5.925e-05,
      "loss": 23.7426,
      "step": 238
    },
    {
      "epoch": 0.17246978170665705,
      "grad_norm": 73.9217758178711,
      "learning_rate": 5.9499999999999996e-05,
      "loss": 24.2734,
      "step": 239
    },
    {
      "epoch": 0.17319141259245896,
      "grad_norm": 73.23078155517578,
      "learning_rate": 5.9749999999999995e-05,
      "loss": 24.4224,
      "step": 240
    },
    {
      "epoch": 0.17391304347826086,
      "grad_norm": 93.6952133178711,
      "learning_rate": 6e-05,
      "loss": 23.2578,
      "step": 241
    },
    {
      "epoch": 0.17463467436406277,
      "grad_norm": 74.9744644165039,
      "learning_rate": 6.025e-05,
      "loss": 24.1061,
      "step": 242
    },
    {
      "epoch": 0.1753563052498647,
      "grad_norm": 68.3718490600586,
      "learning_rate": 6.05e-05,
      "loss": 23.4138,
      "step": 243
    },
    {
      "epoch": 0.1760779361356666,
      "grad_norm": 63.539215087890625,
      "learning_rate": 6.075e-05,
      "loss": 23.2895,
      "step": 244
    },
    {
      "epoch": 0.1767995670214685,
      "grad_norm": 68.61719512939453,
      "learning_rate": 6.1e-05,
      "loss": 22.753,
      "step": 245
    },
    {
      "epoch": 0.17752119790727044,
      "grad_norm": 62.188777923583984,
      "learning_rate": 6.125e-05,
      "loss": 23.5383,
      "step": 246
    },
    {
      "epoch": 0.17824282879307235,
      "grad_norm": 75.78491973876953,
      "learning_rate": 6.15e-05,
      "loss": 23.3645,
      "step": 247
    },
    {
      "epoch": 0.17896445967887425,
      "grad_norm": 66.57562255859375,
      "learning_rate": 6.175e-05,
      "loss": 24.2638,
      "step": 248
    },
    {
      "epoch": 0.17968609056467616,
      "grad_norm": 72.22679901123047,
      "learning_rate": 6.2e-05,
      "loss": 23.6567,
      "step": 249
    },
    {
      "epoch": 0.1804077214504781,
      "grad_norm": 68.87679290771484,
      "learning_rate": 6.225e-05,
      "loss": 23.795,
      "step": 250
    },
    {
      "epoch": 0.18112935233628,
      "grad_norm": 119.96003723144531,
      "learning_rate": 6.25e-05,
      "loss": 23.5098,
      "step": 251
    },
    {
      "epoch": 0.1818509832220819,
      "grad_norm": 87.98988342285156,
      "learning_rate": 6.275000000000001e-05,
      "loss": 22.7317,
      "step": 252
    },
    {
      "epoch": 0.1825726141078838,
      "grad_norm": 92.03120422363281,
      "learning_rate": 6.3e-05,
      "loss": 23.1416,
      "step": 253
    },
    {
      "epoch": 0.18329424499368574,
      "grad_norm": 87.74315643310547,
      "learning_rate": 6.325e-05,
      "loss": 23.3632,
      "step": 254
    },
    {
      "epoch": 0.18401587587948764,
      "grad_norm": 93.01870727539062,
      "learning_rate": 6.35e-05,
      "loss": 23.2213,
      "step": 255
    },
    {
      "epoch": 0.18473750676528955,
      "grad_norm": 115.55628967285156,
      "learning_rate": 6.375e-05,
      "loss": 22.632,
      "step": 256
    },
    {
      "epoch": 0.18545913765109145,
      "grad_norm": 128.9244384765625,
      "learning_rate": 6.4e-05,
      "loss": 22.4041,
      "step": 257
    },
    {
      "epoch": 0.1861807685368934,
      "grad_norm": 97.84650421142578,
      "learning_rate": 6.425e-05,
      "loss": 23.0697,
      "step": 258
    },
    {
      "epoch": 0.1869023994226953,
      "grad_norm": 127.88082122802734,
      "learning_rate": 6.450000000000001e-05,
      "loss": 22.7026,
      "step": 259
    },
    {
      "epoch": 0.1876240303084972,
      "grad_norm": 108.66128540039062,
      "learning_rate": 6.475e-05,
      "loss": 22.9011,
      "step": 260
    },
    {
      "epoch": 0.1883456611942991,
      "grad_norm": 91.80811309814453,
      "learning_rate": 6.500000000000001e-05,
      "loss": 22.8688,
      "step": 261
    },
    {
      "epoch": 0.18906729208010103,
      "grad_norm": 125.00308227539062,
      "learning_rate": 6.525e-05,
      "loss": 22.4557,
      "step": 262
    },
    {
      "epoch": 0.18978892296590294,
      "grad_norm": 127.65741729736328,
      "learning_rate": 6.55e-05,
      "loss": 23.3003,
      "step": 263
    },
    {
      "epoch": 0.19051055385170484,
      "grad_norm": 104.47916412353516,
      "learning_rate": 6.575e-05,
      "loss": 22.4191,
      "step": 264
    },
    {
      "epoch": 0.19123218473750678,
      "grad_norm": 118.56585693359375,
      "learning_rate": 6.6e-05,
      "loss": 22.5959,
      "step": 265
    },
    {
      "epoch": 0.19195381562330868,
      "grad_norm": 116.93587493896484,
      "learning_rate": 6.625000000000001e-05,
      "loss": 22.3366,
      "step": 266
    },
    {
      "epoch": 0.1926754465091106,
      "grad_norm": 78.39677429199219,
      "learning_rate": 6.65e-05,
      "loss": 22.5365,
      "step": 267
    },
    {
      "epoch": 0.1933970773949125,
      "grad_norm": 119.20577239990234,
      "learning_rate": 6.675000000000001e-05,
      "loss": 21.9732,
      "step": 268
    },
    {
      "epoch": 0.19411870828071442,
      "grad_norm": 83.9002456665039,
      "learning_rate": 6.7e-05,
      "loss": 21.2477,
      "step": 269
    },
    {
      "epoch": 0.19484033916651633,
      "grad_norm": 105.17659759521484,
      "learning_rate": 6.725000000000001e-05,
      "loss": 21.9188,
      "step": 270
    },
    {
      "epoch": 0.19556197005231823,
      "grad_norm": 146.25448608398438,
      "learning_rate": 6.75e-05,
      "loss": 21.8803,
      "step": 271
    },
    {
      "epoch": 0.19628360093812014,
      "grad_norm": 93.36370849609375,
      "learning_rate": 6.775000000000001e-05,
      "loss": 20.9505,
      "step": 272
    },
    {
      "epoch": 0.19700523182392207,
      "grad_norm": 113.21519470214844,
      "learning_rate": 6.800000000000001e-05,
      "loss": 21.5707,
      "step": 273
    },
    {
      "epoch": 0.19772686270972398,
      "grad_norm": 116.95878601074219,
      "learning_rate": 6.825e-05,
      "loss": 21.8333,
      "step": 274
    },
    {
      "epoch": 0.19844849359552588,
      "grad_norm": 99.77437591552734,
      "learning_rate": 6.850000000000001e-05,
      "loss": 22.354,
      "step": 275
    },
    {
      "epoch": 0.1991701244813278,
      "grad_norm": 111.15299224853516,
      "learning_rate": 6.875e-05,
      "loss": 21.5043,
      "step": 276
    },
    {
      "epoch": 0.19989175536712972,
      "grad_norm": 96.8077621459961,
      "learning_rate": 6.900000000000001e-05,
      "loss": 20.3629,
      "step": 277
    },
    {
      "epoch": 0.20061338625293162,
      "grad_norm": 96.42042541503906,
      "learning_rate": 6.925e-05,
      "loss": 21.256,
      "step": 278
    },
    {
      "epoch": 0.20133501713873353,
      "grad_norm": 112.71603393554688,
      "learning_rate": 6.950000000000001e-05,
      "loss": 22.0098,
      "step": 279
    },
    {
      "epoch": 0.20205664802453546,
      "grad_norm": 132.22976684570312,
      "learning_rate": 6.975e-05,
      "loss": 21.5008,
      "step": 280
    },
    {
      "epoch": 0.20277827891033737,
      "grad_norm": 131.66310119628906,
      "learning_rate": 7.000000000000001e-05,
      "loss": 21.526,
      "step": 281
    },
    {
      "epoch": 0.20349990979613927,
      "grad_norm": 118.11505889892578,
      "learning_rate": 7.025000000000001e-05,
      "loss": 21.3985,
      "step": 282
    },
    {
      "epoch": 0.20422154068194118,
      "grad_norm": 104.69857788085938,
      "learning_rate": 7.049999999999999e-05,
      "loss": 20.6975,
      "step": 283
    },
    {
      "epoch": 0.2049431715677431,
      "grad_norm": 116.7916488647461,
      "learning_rate": 7.075e-05,
      "loss": 21.7482,
      "step": 284
    },
    {
      "epoch": 0.20566480245354501,
      "grad_norm": 96.15574645996094,
      "learning_rate": 7.099999999999999e-05,
      "loss": 20.7738,
      "step": 285
    },
    {
      "epoch": 0.20638643333934692,
      "grad_norm": 95.08125305175781,
      "learning_rate": 7.125e-05,
      "loss": 20.9836,
      "step": 286
    },
    {
      "epoch": 0.20710806422514882,
      "grad_norm": 160.58428955078125,
      "learning_rate": 7.149999999999999e-05,
      "loss": 20.7218,
      "step": 287
    },
    {
      "epoch": 0.20782969511095076,
      "grad_norm": 162.59951782226562,
      "learning_rate": 7.175e-05,
      "loss": 20.9739,
      "step": 288
    },
    {
      "epoch": 0.20855132599675266,
      "grad_norm": 109.85807037353516,
      "learning_rate": 7.2e-05,
      "loss": 21.2262,
      "step": 289
    },
    {
      "epoch": 0.20927295688255457,
      "grad_norm": 151.03622436523438,
      "learning_rate": 7.225e-05,
      "loss": 20.9384,
      "step": 290
    },
    {
      "epoch": 0.20999458776835647,
      "grad_norm": 109.45704650878906,
      "learning_rate": 7.25e-05,
      "loss": 21.3245,
      "step": 291
    },
    {
      "epoch": 0.2107162186541584,
      "grad_norm": 126.75468444824219,
      "learning_rate": 7.274999999999999e-05,
      "loss": 21.0392,
      "step": 292
    },
    {
      "epoch": 0.2114378495399603,
      "grad_norm": 179.14051818847656,
      "learning_rate": 7.3e-05,
      "loss": 21.2501,
      "step": 293
    },
    {
      "epoch": 0.21215948042576221,
      "grad_norm": 107.9770736694336,
      "learning_rate": 7.324999999999999e-05,
      "loss": 20.6922,
      "step": 294
    },
    {
      "epoch": 0.21288111131156415,
      "grad_norm": 136.5093231201172,
      "learning_rate": 7.35e-05,
      "loss": 20.3683,
      "step": 295
    },
    {
      "epoch": 0.21360274219736605,
      "grad_norm": 175.46339416503906,
      "learning_rate": 7.375e-05,
      "loss": 20.4395,
      "step": 296
    },
    {
      "epoch": 0.21432437308316796,
      "grad_norm": 93.19518280029297,
      "learning_rate": 7.4e-05,
      "loss": 20.4095,
      "step": 297
    },
    {
      "epoch": 0.21504600396896986,
      "grad_norm": 142.78370666503906,
      "learning_rate": 7.425e-05,
      "loss": 19.9514,
      "step": 298
    },
    {
      "epoch": 0.2157676348547718,
      "grad_norm": 170.2735595703125,
      "learning_rate": 7.45e-05,
      "loss": 21.5399,
      "step": 299
    },
    {
      "epoch": 0.2164892657405737,
      "grad_norm": 127.27388763427734,
      "learning_rate": 7.475e-05,
      "loss": 20.9812,
      "step": 300
    },
    {
      "epoch": 0.2172108966263756,
      "grad_norm": 88.88377380371094,
      "learning_rate": 7.5e-05,
      "loss": 20.2587,
      "step": 301
    },
    {
      "epoch": 0.2179325275121775,
      "grad_norm": 192.97262573242188,
      "learning_rate": 7.525e-05,
      "loss": 19.6022,
      "step": 302
    },
    {
      "epoch": 0.21865415839797944,
      "grad_norm": 112.36576843261719,
      "learning_rate": 7.55e-05,
      "loss": 20.3087,
      "step": 303
    },
    {
      "epoch": 0.21937578928378135,
      "grad_norm": 126.70963287353516,
      "learning_rate": 7.575e-05,
      "loss": 20.072,
      "step": 304
    },
    {
      "epoch": 0.22009742016958325,
      "grad_norm": 158.2771453857422,
      "learning_rate": 7.6e-05,
      "loss": 20.206,
      "step": 305
    },
    {
      "epoch": 0.22081905105538516,
      "grad_norm": 98.54317474365234,
      "learning_rate": 7.625e-05,
      "loss": 20.2335,
      "step": 306
    },
    {
      "epoch": 0.2215406819411871,
      "grad_norm": 137.03695678710938,
      "learning_rate": 7.65e-05,
      "loss": 20.8045,
      "step": 307
    },
    {
      "epoch": 0.222262312826989,
      "grad_norm": 123.48090362548828,
      "learning_rate": 7.675e-05,
      "loss": 19.3805,
      "step": 308
    },
    {
      "epoch": 0.2229839437127909,
      "grad_norm": 112.20282745361328,
      "learning_rate": 7.7e-05,
      "loss": 20.7244,
      "step": 309
    },
    {
      "epoch": 0.22370557459859283,
      "grad_norm": 114.83842468261719,
      "learning_rate": 7.725000000000001e-05,
      "loss": 18.5354,
      "step": 310
    },
    {
      "epoch": 0.22442720548439474,
      "grad_norm": 101.28981018066406,
      "learning_rate": 7.75e-05,
      "loss": 21.1916,
      "step": 311
    },
    {
      "epoch": 0.22514883637019664,
      "grad_norm": 104.46487426757812,
      "learning_rate": 7.775e-05,
      "loss": 19.6738,
      "step": 312
    },
    {
      "epoch": 0.22587046725599855,
      "grad_norm": 170.33839416503906,
      "learning_rate": 7.8e-05,
      "loss": 19.9933,
      "step": 313
    },
    {
      "epoch": 0.22659209814180048,
      "grad_norm": 137.97679138183594,
      "learning_rate": 7.825e-05,
      "loss": 20.3739,
      "step": 314
    },
    {
      "epoch": 0.22731372902760238,
      "grad_norm": 127.218505859375,
      "learning_rate": 7.85e-05,
      "loss": 20.0708,
      "step": 315
    },
    {
      "epoch": 0.2280353599134043,
      "grad_norm": 155.10450744628906,
      "learning_rate": 7.875e-05,
      "loss": 18.7498,
      "step": 316
    },
    {
      "epoch": 0.2287569907992062,
      "grad_norm": 138.22120666503906,
      "learning_rate": 7.9e-05,
      "loss": 18.9259,
      "step": 317
    },
    {
      "epoch": 0.22947862168500813,
      "grad_norm": 155.56509399414062,
      "learning_rate": 7.925e-05,
      "loss": 20.0904,
      "step": 318
    },
    {
      "epoch": 0.23020025257081003,
      "grad_norm": 151.7017822265625,
      "learning_rate": 7.950000000000001e-05,
      "loss": 19.7968,
      "step": 319
    },
    {
      "epoch": 0.23092188345661194,
      "grad_norm": 118.40596771240234,
      "learning_rate": 7.975e-05,
      "loss": 19.6959,
      "step": 320
    },
    {
      "epoch": 0.23164351434241384,
      "grad_norm": 92.08985137939453,
      "learning_rate": 8e-05,
      "loss": 18.8152,
      "step": 321
    },
    {
      "epoch": 0.23236514522821577,
      "grad_norm": 93.8642349243164,
      "learning_rate": 8.025e-05,
      "loss": 19.1919,
      "step": 322
    },
    {
      "epoch": 0.23308677611401768,
      "grad_norm": 130.9145965576172,
      "learning_rate": 8.05e-05,
      "loss": 20.0129,
      "step": 323
    },
    {
      "epoch": 0.23380840699981958,
      "grad_norm": 137.91766357421875,
      "learning_rate": 8.075e-05,
      "loss": 19.196,
      "step": 324
    },
    {
      "epoch": 0.23453003788562152,
      "grad_norm": 127.68315887451172,
      "learning_rate": 8.1e-05,
      "loss": 19.4164,
      "step": 325
    },
    {
      "epoch": 0.23525166877142342,
      "grad_norm": 125.00452423095703,
      "learning_rate": 8.125000000000001e-05,
      "loss": 18.8042,
      "step": 326
    },
    {
      "epoch": 0.23597329965722533,
      "grad_norm": 125.27141571044922,
      "learning_rate": 8.15e-05,
      "loss": 18.8728,
      "step": 327
    },
    {
      "epoch": 0.23669493054302723,
      "grad_norm": 117.35179901123047,
      "learning_rate": 8.175000000000001e-05,
      "loss": 19.2881,
      "step": 328
    },
    {
      "epoch": 0.23741656142882916,
      "grad_norm": 116.45773315429688,
      "learning_rate": 8.2e-05,
      "loss": 18.7089,
      "step": 329
    },
    {
      "epoch": 0.23813819231463107,
      "grad_norm": 136.78671264648438,
      "learning_rate": 8.225000000000001e-05,
      "loss": 19.4714,
      "step": 330
    },
    {
      "epoch": 0.23885982320043297,
      "grad_norm": 119.02189636230469,
      "learning_rate": 8.25e-05,
      "loss": 19.5822,
      "step": 331
    },
    {
      "epoch": 0.23958145408623488,
      "grad_norm": 118.3829574584961,
      "learning_rate": 8.275e-05,
      "loss": 19.085,
      "step": 332
    },
    {
      "epoch": 0.2403030849720368,
      "grad_norm": 160.94290161132812,
      "learning_rate": 8.300000000000001e-05,
      "loss": 18.1068,
      "step": 333
    },
    {
      "epoch": 0.24102471585783872,
      "grad_norm": 180.03744506835938,
      "learning_rate": 8.325e-05,
      "loss": 21.2773,
      "step": 334
    },
    {
      "epoch": 0.24174634674364062,
      "grad_norm": 153.03012084960938,
      "learning_rate": 8.350000000000001e-05,
      "loss": 18.9825,
      "step": 335
    },
    {
      "epoch": 0.24246797762944253,
      "grad_norm": 139.05543518066406,
      "learning_rate": 8.375e-05,
      "loss": 18.3576,
      "step": 336
    },
    {
      "epoch": 0.24318960851524446,
      "grad_norm": 130.83961486816406,
      "learning_rate": 8.400000000000001e-05,
      "loss": 18.9537,
      "step": 337
    },
    {
      "epoch": 0.24391123940104636,
      "grad_norm": 112.37144470214844,
      "learning_rate": 8.425e-05,
      "loss": 19.4248,
      "step": 338
    },
    {
      "epoch": 0.24463287028684827,
      "grad_norm": 126.2109603881836,
      "learning_rate": 8.450000000000001e-05,
      "loss": 18.9519,
      "step": 339
    },
    {
      "epoch": 0.2453545011726502,
      "grad_norm": 124.213623046875,
      "learning_rate": 8.475000000000001e-05,
      "loss": 18.7013,
      "step": 340
    },
    {
      "epoch": 0.2460761320584521,
      "grad_norm": 113.76982879638672,
      "learning_rate": 8.5e-05,
      "loss": 18.847,
      "step": 341
    },
    {
      "epoch": 0.246797762944254,
      "grad_norm": 104.35054779052734,
      "learning_rate": 8.525000000000001e-05,
      "loss": 19.4533,
      "step": 342
    },
    {
      "epoch": 0.24751939383005592,
      "grad_norm": 132.9560089111328,
      "learning_rate": 8.55e-05,
      "loss": 18.7036,
      "step": 343
    },
    {
      "epoch": 0.24824102471585785,
      "grad_norm": 109.11954498291016,
      "learning_rate": 8.575000000000001e-05,
      "loss": 17.3055,
      "step": 344
    },
    {
      "epoch": 0.24896265560165975,
      "grad_norm": 138.00889587402344,
      "learning_rate": 8.599999999999999e-05,
      "loss": 19.4947,
      "step": 345
    },
    {
      "epoch": 0.24968428648746166,
      "grad_norm": 135.359130859375,
      "learning_rate": 8.625e-05,
      "loss": 18.0803,
      "step": 346
    },
    {
      "epoch": 0.25040591737326356,
      "grad_norm": 170.78872680664062,
      "learning_rate": 8.65e-05,
      "loss": 18.2432,
      "step": 347
    },
    {
      "epoch": 0.25112754825906547,
      "grad_norm": 146.4090118408203,
      "learning_rate": 8.675e-05,
      "loss": 18.1359,
      "step": 348
    },
    {
      "epoch": 0.2518491791448674,
      "grad_norm": 125.81953430175781,
      "learning_rate": 8.7e-05,
      "loss": 18.8427,
      "step": 349
    },
    {
      "epoch": 0.25257081003066933,
      "grad_norm": 133.2216796875,
      "learning_rate": 8.724999999999999e-05,
      "loss": 18.2136,
      "step": 350
    },
    {
      "epoch": 0.25329244091647124,
      "grad_norm": 100.14163970947266,
      "learning_rate": 8.75e-05,
      "loss": 18.2598,
      "step": 351
    },
    {
      "epoch": 0.25401407180227314,
      "grad_norm": 120.14801788330078,
      "learning_rate": 8.774999999999999e-05,
      "loss": 18.1744,
      "step": 352
    },
    {
      "epoch": 0.25473570268807505,
      "grad_norm": 160.1026153564453,
      "learning_rate": 8.8e-05,
      "loss": 17.7493,
      "step": 353
    },
    {
      "epoch": 0.25545733357387695,
      "grad_norm": 115.47818756103516,
      "learning_rate": 8.824999999999999e-05,
      "loss": 17.189,
      "step": 354
    },
    {
      "epoch": 0.25617896445967886,
      "grad_norm": 156.73260498046875,
      "learning_rate": 8.85e-05,
      "loss": 17.9315,
      "step": 355
    },
    {
      "epoch": 0.25690059534548076,
      "grad_norm": 118.1387939453125,
      "learning_rate": 8.875e-05,
      "loss": 18.7396,
      "step": 356
    },
    {
      "epoch": 0.2576222262312827,
      "grad_norm": 133.78359985351562,
      "learning_rate": 8.9e-05,
      "loss": 17.5012,
      "step": 357
    },
    {
      "epoch": 0.25834385711708463,
      "grad_norm": 129.79676818847656,
      "learning_rate": 8.925e-05,
      "loss": 18.1831,
      "step": 358
    },
    {
      "epoch": 0.25906548800288653,
      "grad_norm": 120.4685287475586,
      "learning_rate": 8.95e-05,
      "loss": 17.5266,
      "step": 359
    },
    {
      "epoch": 0.25978711888868844,
      "grad_norm": 131.00367736816406,
      "learning_rate": 8.975e-05,
      "loss": 19.023,
      "step": 360
    },
    {
      "epoch": 0.26050874977449034,
      "grad_norm": 168.77804565429688,
      "learning_rate": 8.999999999999999e-05,
      "loss": 18.6152,
      "step": 361
    },
    {
      "epoch": 0.26123038066029225,
      "grad_norm": 146.9300079345703,
      "learning_rate": 9.025e-05,
      "loss": 18.7781,
      "step": 362
    },
    {
      "epoch": 0.26195201154609415,
      "grad_norm": 118.14533233642578,
      "learning_rate": 9.05e-05,
      "loss": 17.0214,
      "step": 363
    },
    {
      "epoch": 0.26267364243189606,
      "grad_norm": 132.3089141845703,
      "learning_rate": 9.075e-05,
      "loss": 18.7186,
      "step": 364
    },
    {
      "epoch": 0.263395273317698,
      "grad_norm": 119.42560577392578,
      "learning_rate": 9.1e-05,
      "loss": 18.8496,
      "step": 365
    },
    {
      "epoch": 0.2641169042034999,
      "grad_norm": 142.12835693359375,
      "learning_rate": 9.125e-05,
      "loss": 18.4723,
      "step": 366
    },
    {
      "epoch": 0.26483853508930183,
      "grad_norm": 145.63473510742188,
      "learning_rate": 9.15e-05,
      "loss": 18.2792,
      "step": 367
    },
    {
      "epoch": 0.26556016597510373,
      "grad_norm": 113.65492248535156,
      "learning_rate": 9.175e-05,
      "loss": 16.0877,
      "step": 368
    },
    {
      "epoch": 0.26628179686090564,
      "grad_norm": 153.2076416015625,
      "learning_rate": 9.2e-05,
      "loss": 18.354,
      "step": 369
    },
    {
      "epoch": 0.26700342774670754,
      "grad_norm": 154.29705810546875,
      "learning_rate": 9.225e-05,
      "loss": 18.9751,
      "step": 370
    },
    {
      "epoch": 0.26772505863250945,
      "grad_norm": 142.6280975341797,
      "learning_rate": 9.25e-05,
      "loss": 17.818,
      "step": 371
    },
    {
      "epoch": 0.2684466895183114,
      "grad_norm": 157.59078979492188,
      "learning_rate": 9.275e-05,
      "loss": 18.9138,
      "step": 372
    },
    {
      "epoch": 0.2691683204041133,
      "grad_norm": 134.91729736328125,
      "learning_rate": 9.3e-05,
      "loss": 18.0871,
      "step": 373
    },
    {
      "epoch": 0.2698899512899152,
      "grad_norm": 108.84959411621094,
      "learning_rate": 9.325e-05,
      "loss": 16.7653,
      "step": 374
    },
    {
      "epoch": 0.2706115821757171,
      "grad_norm": 130.676513671875,
      "learning_rate": 9.35e-05,
      "loss": 17.5736,
      "step": 375
    },
    {
      "epoch": 0.27133321306151903,
      "grad_norm": 140.1624755859375,
      "learning_rate": 9.375e-05,
      "loss": 17.3549,
      "step": 376
    },
    {
      "epoch": 0.27205484394732093,
      "grad_norm": 123.37458038330078,
      "learning_rate": 9.400000000000001e-05,
      "loss": 16.8391,
      "step": 377
    },
    {
      "epoch": 0.27277647483312284,
      "grad_norm": 143.4231414794922,
      "learning_rate": 9.425e-05,
      "loss": 17.5084,
      "step": 378
    },
    {
      "epoch": 0.27349810571892474,
      "grad_norm": 135.41468811035156,
      "learning_rate": 9.45e-05,
      "loss": 16.902,
      "step": 379
    },
    {
      "epoch": 0.2742197366047267,
      "grad_norm": 130.99246215820312,
      "learning_rate": 9.475e-05,
      "loss": 16.961,
      "step": 380
    },
    {
      "epoch": 0.2749413674905286,
      "grad_norm": 124.35911560058594,
      "learning_rate": 9.5e-05,
      "loss": 18.1886,
      "step": 381
    },
    {
      "epoch": 0.2756629983763305,
      "grad_norm": 139.18826293945312,
      "learning_rate": 9.525e-05,
      "loss": 17.9021,
      "step": 382
    },
    {
      "epoch": 0.2763846292621324,
      "grad_norm": 127.82727813720703,
      "learning_rate": 9.55e-05,
      "loss": 16.7671,
      "step": 383
    },
    {
      "epoch": 0.2771062601479343,
      "grad_norm": 129.71592712402344,
      "learning_rate": 9.575000000000001e-05,
      "loss": 18.1073,
      "step": 384
    },
    {
      "epoch": 0.27782789103373623,
      "grad_norm": 133.40960693359375,
      "learning_rate": 9.6e-05,
      "loss": 17.63,
      "step": 385
    },
    {
      "epoch": 0.27854952191953813,
      "grad_norm": 125.99302673339844,
      "learning_rate": 9.625000000000001e-05,
      "loss": 18.1475,
      "step": 386
    },
    {
      "epoch": 0.2792711528053401,
      "grad_norm": 100.42431640625,
      "learning_rate": 9.65e-05,
      "loss": 17.3278,
      "step": 387
    },
    {
      "epoch": 0.279992783691142,
      "grad_norm": 109.15745544433594,
      "learning_rate": 9.675000000000001e-05,
      "loss": 17.5777,
      "step": 388
    },
    {
      "epoch": 0.2807144145769439,
      "grad_norm": 130.2150421142578,
      "learning_rate": 9.7e-05,
      "loss": 17.9706,
      "step": 389
    },
    {
      "epoch": 0.2814360454627458,
      "grad_norm": 129.9481658935547,
      "learning_rate": 9.725e-05,
      "loss": 16.5424,
      "step": 390
    },
    {
      "epoch": 0.2821576763485477,
      "grad_norm": 116.29962921142578,
      "learning_rate": 9.750000000000001e-05,
      "loss": 16.5387,
      "step": 391
    },
    {
      "epoch": 0.2828793072343496,
      "grad_norm": 122.28446197509766,
      "learning_rate": 9.775e-05,
      "loss": 16.6216,
      "step": 392
    },
    {
      "epoch": 0.2836009381201515,
      "grad_norm": 138.8499298095703,
      "learning_rate": 9.800000000000001e-05,
      "loss": 17.389,
      "step": 393
    },
    {
      "epoch": 0.28432256900595343,
      "grad_norm": 109.56756591796875,
      "learning_rate": 9.825e-05,
      "loss": 17.2592,
      "step": 394
    },
    {
      "epoch": 0.2850441998917554,
      "grad_norm": 116.53446960449219,
      "learning_rate": 9.850000000000001e-05,
      "loss": 17.0041,
      "step": 395
    },
    {
      "epoch": 0.2857658307775573,
      "grad_norm": 102.33375549316406,
      "learning_rate": 9.875e-05,
      "loss": 16.1172,
      "step": 396
    },
    {
      "epoch": 0.2864874616633592,
      "grad_norm": 150.864501953125,
      "learning_rate": 9.900000000000001e-05,
      "loss": 15.9061,
      "step": 397
    },
    {
      "epoch": 0.2872090925491611,
      "grad_norm": 114.42489624023438,
      "learning_rate": 9.925000000000001e-05,
      "loss": 16.8392,
      "step": 398
    },
    {
      "epoch": 0.287930723434963,
      "grad_norm": 120.93428802490234,
      "learning_rate": 9.95e-05,
      "loss": 16.7203,
      "step": 399
    },
    {
      "epoch": 0.2886523543207649,
      "grad_norm": 123.52486419677734,
      "learning_rate": 9.975000000000001e-05,
      "loss": 18.3893,
      "step": 400
    },
    {
      "epoch": 0.2893739852065668,
      "grad_norm": 104.7626953125,
      "learning_rate": 0.0001,
      "loss": 16.8932,
      "step": 401
    },
    {
      "epoch": 0.2900956160923688,
      "grad_norm": 128.4590301513672,
      "learning_rate": 0.00010025000000000001,
      "loss": 16.0376,
      "step": 402
    },
    {
      "epoch": 0.2908172469781707,
      "grad_norm": 119.28690338134766,
      "learning_rate": 0.0001005,
      "loss": 15.9752,
      "step": 403
    },
    {
      "epoch": 0.2915388778639726,
      "grad_norm": 125.42201232910156,
      "learning_rate": 0.00010075000000000001,
      "loss": 16.1994,
      "step": 404
    },
    {
      "epoch": 0.2922605087497745,
      "grad_norm": 116.57315826416016,
      "learning_rate": 0.000101,
      "loss": 16.7423,
      "step": 405
    },
    {
      "epoch": 0.2929821396355764,
      "grad_norm": 128.3560028076172,
      "learning_rate": 0.00010125000000000001,
      "loss": 16.4519,
      "step": 406
    },
    {
      "epoch": 0.2937037705213783,
      "grad_norm": 117.78568267822266,
      "learning_rate": 0.00010150000000000001,
      "loss": 17.0578,
      "step": 407
    },
    {
      "epoch": 0.2944254014071802,
      "grad_norm": 112.90748596191406,
      "learning_rate": 0.00010174999999999999,
      "loss": 16.8461,
      "step": 408
    },
    {
      "epoch": 0.2951470322929821,
      "grad_norm": 116.85572814941406,
      "learning_rate": 0.000102,
      "loss": 16.5531,
      "step": 409
    },
    {
      "epoch": 0.2958686631787841,
      "grad_norm": 112.38812255859375,
      "learning_rate": 0.00010224999999999999,
      "loss": 15.7827,
      "step": 410
    },
    {
      "epoch": 0.296590294064586,
      "grad_norm": 94.1716537475586,
      "learning_rate": 0.0001025,
      "loss": 15.6499,
      "step": 411
    },
    {
      "epoch": 0.2973119249503879,
      "grad_norm": 120.0166015625,
      "learning_rate": 0.00010274999999999999,
      "loss": 16.2576,
      "step": 412
    },
    {
      "epoch": 0.2980335558361898,
      "grad_norm": 128.22219848632812,
      "learning_rate": 0.000103,
      "loss": 16.1042,
      "step": 413
    },
    {
      "epoch": 0.2987551867219917,
      "grad_norm": 126.17545318603516,
      "learning_rate": 0.00010325,
      "loss": 16.9984,
      "step": 414
    },
    {
      "epoch": 0.2994768176077936,
      "grad_norm": 112.67796325683594,
      "learning_rate": 0.0001035,
      "loss": 15.7934,
      "step": 415
    },
    {
      "epoch": 0.3001984484935955,
      "grad_norm": 131.03280639648438,
      "learning_rate": 0.00010375,
      "loss": 16.0067,
      "step": 416
    },
    {
      "epoch": 0.30092007937939746,
      "grad_norm": 147.8217010498047,
      "learning_rate": 0.000104,
      "loss": 16.0716,
      "step": 417
    },
    {
      "epoch": 0.30164171026519937,
      "grad_norm": 136.98776245117188,
      "learning_rate": 0.00010425,
      "loss": 17.2837,
      "step": 418
    },
    {
      "epoch": 0.3023633411510013,
      "grad_norm": 134.31947326660156,
      "learning_rate": 0.00010449999999999999,
      "loss": 16.6959,
      "step": 419
    },
    {
      "epoch": 0.3030849720368032,
      "grad_norm": 125.67848205566406,
      "learning_rate": 0.00010475,
      "loss": 16.7846,
      "step": 420
    },
    {
      "epoch": 0.3038066029226051,
      "grad_norm": 122.89647674560547,
      "learning_rate": 0.000105,
      "loss": 15.4823,
      "step": 421
    },
    {
      "epoch": 0.304528233808407,
      "grad_norm": 112.93553924560547,
      "learning_rate": 0.00010525,
      "loss": 15.2046,
      "step": 422
    },
    {
      "epoch": 0.3052498646942089,
      "grad_norm": 114.73391723632812,
      "learning_rate": 0.0001055,
      "loss": 16.8201,
      "step": 423
    },
    {
      "epoch": 0.3059714955800108,
      "grad_norm": 113.83206939697266,
      "learning_rate": 0.00010575,
      "loss": 15.7547,
      "step": 424
    },
    {
      "epoch": 0.30669312646581276,
      "grad_norm": 134.71484375,
      "learning_rate": 0.000106,
      "loss": 16.3297,
      "step": 425
    },
    {
      "epoch": 0.30741475735161466,
      "grad_norm": 152.6114044189453,
      "learning_rate": 0.00010625,
      "loss": 16.0097,
      "step": 426
    },
    {
      "epoch": 0.30813638823741657,
      "grad_norm": 102.86438751220703,
      "learning_rate": 0.0001065,
      "loss": 15.5677,
      "step": 427
    },
    {
      "epoch": 0.3088580191232185,
      "grad_norm": 124.10894775390625,
      "learning_rate": 0.00010675,
      "loss": 16.4717,
      "step": 428
    },
    {
      "epoch": 0.3095796500090204,
      "grad_norm": 166.02056884765625,
      "learning_rate": 0.000107,
      "loss": 16.9823,
      "step": 429
    },
    {
      "epoch": 0.3103012808948223,
      "grad_norm": 114.19197082519531,
      "learning_rate": 0.00010725,
      "loss": 16.4131,
      "step": 430
    },
    {
      "epoch": 0.3110229117806242,
      "grad_norm": 148.10362243652344,
      "learning_rate": 0.0001075,
      "loss": 15.2628,
      "step": 431
    },
    {
      "epoch": 0.31174454266642615,
      "grad_norm": 140.69073486328125,
      "learning_rate": 0.00010775,
      "loss": 16.9466,
      "step": 432
    },
    {
      "epoch": 0.31246617355222805,
      "grad_norm": 133.1749725341797,
      "learning_rate": 0.000108,
      "loss": 16.6604,
      "step": 433
    },
    {
      "epoch": 0.31318780443802996,
      "grad_norm": 138.84628295898438,
      "learning_rate": 0.00010825,
      "loss": 15.6488,
      "step": 434
    },
    {
      "epoch": 0.31390943532383186,
      "grad_norm": 128.75006103515625,
      "learning_rate": 0.00010850000000000001,
      "loss": 14.9495,
      "step": 435
    },
    {
      "epoch": 0.31463106620963377,
      "grad_norm": 126.56932830810547,
      "learning_rate": 0.00010875,
      "loss": 15.9176,
      "step": 436
    },
    {
      "epoch": 0.3153526970954357,
      "grad_norm": 118.45378112792969,
      "learning_rate": 0.000109,
      "loss": 15.9274,
      "step": 437
    },
    {
      "epoch": 0.3160743279812376,
      "grad_norm": 113.36019897460938,
      "learning_rate": 0.00010925,
      "loss": 14.5827,
      "step": 438
    },
    {
      "epoch": 0.3167959588670395,
      "grad_norm": 151.25831604003906,
      "learning_rate": 0.0001095,
      "loss": 16.7967,
      "step": 439
    },
    {
      "epoch": 0.31751758975284144,
      "grad_norm": 119.27442169189453,
      "learning_rate": 0.00010975,
      "loss": 15.6881,
      "step": 440
    },
    {
      "epoch": 0.31823922063864335,
      "grad_norm": 117.97734069824219,
      "learning_rate": 0.00011,
      "loss": 15.9703,
      "step": 441
    },
    {
      "epoch": 0.31896085152444525,
      "grad_norm": 102.5130615234375,
      "learning_rate": 0.00011025,
      "loss": 15.0206,
      "step": 442
    },
    {
      "epoch": 0.31968248241024716,
      "grad_norm": 106.73612213134766,
      "learning_rate": 0.0001105,
      "loss": 14.6628,
      "step": 443
    },
    {
      "epoch": 0.32040411329604906,
      "grad_norm": 111.0027084350586,
      "learning_rate": 0.00011075000000000001,
      "loss": 15.2579,
      "step": 444
    },
    {
      "epoch": 0.32112574418185097,
      "grad_norm": 117.79106903076172,
      "learning_rate": 0.000111,
      "loss": 16.3045,
      "step": 445
    },
    {
      "epoch": 0.3218473750676529,
      "grad_norm": 121.61648559570312,
      "learning_rate": 0.00011125000000000001,
      "loss": 15.7337,
      "step": 446
    },
    {
      "epoch": 0.32256900595345483,
      "grad_norm": 144.28421020507812,
      "learning_rate": 0.0001115,
      "loss": 16.0971,
      "step": 447
    },
    {
      "epoch": 0.32329063683925674,
      "grad_norm": 121.18041229248047,
      "learning_rate": 0.00011175,
      "loss": 15.6497,
      "step": 448
    },
    {
      "epoch": 0.32401226772505864,
      "grad_norm": 102.39279174804688,
      "learning_rate": 0.000112,
      "loss": 15.8451,
      "step": 449
    },
    {
      "epoch": 0.32473389861086055,
      "grad_norm": 134.04490661621094,
      "learning_rate": 0.00011225,
      "loss": 15.2721,
      "step": 450
    },
    {
      "epoch": 0.32545552949666245,
      "grad_norm": 118.1585922241211,
      "learning_rate": 0.00011250000000000001,
      "loss": 14.7991,
      "step": 451
    },
    {
      "epoch": 0.32617716038246436,
      "grad_norm": 95.02972412109375,
      "learning_rate": 0.00011275,
      "loss": 16.2542,
      "step": 452
    },
    {
      "epoch": 0.32689879126826626,
      "grad_norm": 132.89320373535156,
      "learning_rate": 0.00011300000000000001,
      "loss": 15.643,
      "step": 453
    },
    {
      "epoch": 0.32762042215406817,
      "grad_norm": 122.76760864257812,
      "learning_rate": 0.00011325,
      "loss": 16.2246,
      "step": 454
    },
    {
      "epoch": 0.32834205303987013,
      "grad_norm": 106.75033569335938,
      "learning_rate": 0.00011350000000000001,
      "loss": 14.7724,
      "step": 455
    },
    {
      "epoch": 0.32906368392567203,
      "grad_norm": 118.45003509521484,
      "learning_rate": 0.00011375,
      "loss": 16.8014,
      "step": 456
    },
    {
      "epoch": 0.32978531481147394,
      "grad_norm": 132.58628845214844,
      "learning_rate": 0.000114,
      "loss": 15.6749,
      "step": 457
    },
    {
      "epoch": 0.33050694569727584,
      "grad_norm": 111.89917755126953,
      "learning_rate": 0.00011425000000000001,
      "loss": 15.8451,
      "step": 458
    },
    {
      "epoch": 0.33122857658307775,
      "grad_norm": 105.05799865722656,
      "learning_rate": 0.0001145,
      "loss": 14.7814,
      "step": 459
    },
    {
      "epoch": 0.33195020746887965,
      "grad_norm": 117.79509735107422,
      "learning_rate": 0.00011475000000000001,
      "loss": 14.6376,
      "step": 460
    },
    {
      "epoch": 0.33267183835468156,
      "grad_norm": 133.0767822265625,
      "learning_rate": 0.000115,
      "loss": 14.4033,
      "step": 461
    },
    {
      "epoch": 0.3333934692404835,
      "grad_norm": 122.99678802490234,
      "learning_rate": 0.00011525000000000001,
      "loss": 16.1108,
      "step": 462
    },
    {
      "epoch": 0.3341151001262854,
      "grad_norm": 105.92047119140625,
      "learning_rate": 0.0001155,
      "loss": 15.1535,
      "step": 463
    },
    {
      "epoch": 0.33483673101208733,
      "grad_norm": 137.38909912109375,
      "learning_rate": 0.00011575000000000001,
      "loss": 15.2791,
      "step": 464
    },
    {
      "epoch": 0.33555836189788923,
      "grad_norm": 101.18803405761719,
      "learning_rate": 0.00011600000000000001,
      "loss": 14.9132,
      "step": 465
    },
    {
      "epoch": 0.33627999278369114,
      "grad_norm": 128.7403106689453,
      "learning_rate": 0.00011625,
      "loss": 14.3428,
      "step": 466
    },
    {
      "epoch": 0.33700162366949304,
      "grad_norm": 99.07862091064453,
      "learning_rate": 0.00011650000000000001,
      "loss": 14.4247,
      "step": 467
    },
    {
      "epoch": 0.33772325455529495,
      "grad_norm": 104.44768524169922,
      "learning_rate": 0.00011675,
      "loss": 15.5221,
      "step": 468
    },
    {
      "epoch": 0.33844488544109685,
      "grad_norm": 118.3937759399414,
      "learning_rate": 0.00011700000000000001,
      "loss": 15.3607,
      "step": 469
    },
    {
      "epoch": 0.3391665163268988,
      "grad_norm": 100.55596923828125,
      "learning_rate": 0.00011724999999999999,
      "loss": 14.569,
      "step": 470
    },
    {
      "epoch": 0.3398881472127007,
      "grad_norm": 115.41246032714844,
      "learning_rate": 0.0001175,
      "loss": 15.0631,
      "step": 471
    },
    {
      "epoch": 0.3406097780985026,
      "grad_norm": 140.80032348632812,
      "learning_rate": 0.00011775,
      "loss": 15.7517,
      "step": 472
    },
    {
      "epoch": 0.34133140898430453,
      "grad_norm": 101.7921371459961,
      "learning_rate": 0.000118,
      "loss": 14.7718,
      "step": 473
    },
    {
      "epoch": 0.34205303987010643,
      "grad_norm": 120.31768035888672,
      "learning_rate": 0.00011825,
      "loss": 14.2171,
      "step": 474
    },
    {
      "epoch": 0.34277467075590834,
      "grad_norm": 137.0221405029297,
      "learning_rate": 0.0001185,
      "loss": 15.2065,
      "step": 475
    },
    {
      "epoch": 0.34349630164171024,
      "grad_norm": 113.1919937133789,
      "learning_rate": 0.00011875,
      "loss": 15.3379,
      "step": 476
    },
    {
      "epoch": 0.3442179325275122,
      "grad_norm": 101.14954376220703,
      "learning_rate": 0.00011899999999999999,
      "loss": 14.644,
      "step": 477
    },
    {
      "epoch": 0.3449395634133141,
      "grad_norm": 106.9769058227539,
      "learning_rate": 0.00011925,
      "loss": 14.3173,
      "step": 478
    },
    {
      "epoch": 0.345661194299116,
      "grad_norm": 103.82056427001953,
      "learning_rate": 0.00011949999999999999,
      "loss": 15.4348,
      "step": 479
    },
    {
      "epoch": 0.3463828251849179,
      "grad_norm": 123.85338592529297,
      "learning_rate": 0.00011975,
      "loss": 15.4253,
      "step": 480
    },
    {
      "epoch": 0.3471044560707198,
      "grad_norm": 110.21123504638672,
      "learning_rate": 0.00012,
      "loss": 14.8645,
      "step": 481
    },
    {
      "epoch": 0.34782608695652173,
      "grad_norm": 117.39234161376953,
      "learning_rate": 0.00012025,
      "loss": 15.152,
      "step": 482
    },
    {
      "epoch": 0.34854771784232363,
      "grad_norm": 110.96614074707031,
      "learning_rate": 0.0001205,
      "loss": 15.3053,
      "step": 483
    },
    {
      "epoch": 0.34926934872812554,
      "grad_norm": 106.48312377929688,
      "learning_rate": 0.00012075,
      "loss": 14.5868,
      "step": 484
    },
    {
      "epoch": 0.3499909796139275,
      "grad_norm": 143.08621215820312,
      "learning_rate": 0.000121,
      "loss": 16.3622,
      "step": 485
    },
    {
      "epoch": 0.3507126104997294,
      "grad_norm": 130.59957885742188,
      "learning_rate": 0.00012124999999999999,
      "loss": 15.7419,
      "step": 486
    },
    {
      "epoch": 0.3514342413855313,
      "grad_norm": 113.50983428955078,
      "learning_rate": 0.0001215,
      "loss": 14.6491,
      "step": 487
    },
    {
      "epoch": 0.3521558722713332,
      "grad_norm": 126.748779296875,
      "learning_rate": 0.00012175,
      "loss": 15.7498,
      "step": 488
    },
    {
      "epoch": 0.3528775031571351,
      "grad_norm": 118.31085205078125,
      "learning_rate": 0.000122,
      "loss": 14.8652,
      "step": 489
    },
    {
      "epoch": 0.353599134042937,
      "grad_norm": 159.935546875,
      "learning_rate": 0.00012225,
      "loss": 16.1499,
      "step": 490
    },
    {
      "epoch": 0.35432076492873893,
      "grad_norm": 129.91650390625,
      "learning_rate": 0.0001225,
      "loss": 15.9319,
      "step": 491
    },
    {
      "epoch": 0.3550423958145409,
      "grad_norm": 108.9507827758789,
      "learning_rate": 0.00012275,
      "loss": 14.6397,
      "step": 492
    },
    {
      "epoch": 0.3557640267003428,
      "grad_norm": 188.1666259765625,
      "learning_rate": 0.000123,
      "loss": 15.475,
      "step": 493
    },
    {
      "epoch": 0.3564856575861447,
      "grad_norm": 160.82164001464844,
      "learning_rate": 0.00012325000000000001,
      "loss": 16.045,
      "step": 494
    },
    {
      "epoch": 0.3572072884719466,
      "grad_norm": 96.79639434814453,
      "learning_rate": 0.0001235,
      "loss": 14.4639,
      "step": 495
    },
    {
      "epoch": 0.3579289193577485,
      "grad_norm": 188.7710723876953,
      "learning_rate": 0.00012375,
      "loss": 15.1925,
      "step": 496
    },
    {
      "epoch": 0.3586505502435504,
      "grad_norm": 195.452392578125,
      "learning_rate": 0.000124,
      "loss": 15.4585,
      "step": 497
    },
    {
      "epoch": 0.3593721811293523,
      "grad_norm": 106.49018096923828,
      "learning_rate": 0.00012425,
      "loss": 15.5745,
      "step": 498
    },
    {
      "epoch": 0.3600938120151542,
      "grad_norm": 129.33445739746094,
      "learning_rate": 0.0001245,
      "loss": 14.4817,
      "step": 499
    },
    {
      "epoch": 0.3608154429009562,
      "grad_norm": 139.05648803710938,
      "learning_rate": 0.00012475,
      "loss": 15.5735,
      "step": 500
    },
    {
      "epoch": 0.3615370737867581,
      "grad_norm": 101.97750854492188,
      "learning_rate": 0.000125,
      "loss": 15.7274,
      "step": 501
    },
    {
      "epoch": 0.36225870467256,
      "grad_norm": 94.15388488769531,
      "learning_rate": 0.0001248589164785553,
      "loss": 14.8232,
      "step": 502
    },
    {
      "epoch": 0.3629803355583619,
      "grad_norm": 123.03327941894531,
      "learning_rate": 0.0001247178329571106,
      "loss": 14.3928,
      "step": 503
    },
    {
      "epoch": 0.3637019664441638,
      "grad_norm": 107.7138671875,
      "learning_rate": 0.00012457674943566592,
      "loss": 14.9177,
      "step": 504
    },
    {
      "epoch": 0.3644235973299657,
      "grad_norm": 111.56623077392578,
      "learning_rate": 0.00012443566591422122,
      "loss": 15.4275,
      "step": 505
    },
    {
      "epoch": 0.3651452282157676,
      "grad_norm": 110.10736083984375,
      "learning_rate": 0.00012429458239277653,
      "loss": 15.1506,
      "step": 506
    },
    {
      "epoch": 0.3658668591015696,
      "grad_norm": 109.06208038330078,
      "learning_rate": 0.00012415349887133183,
      "loss": 14.4577,
      "step": 507
    },
    {
      "epoch": 0.3665884899873715,
      "grad_norm": 127.45963287353516,
      "learning_rate": 0.00012401241534988714,
      "loss": 15.6089,
      "step": 508
    },
    {
      "epoch": 0.3673101208731734,
      "grad_norm": 94.2429428100586,
      "learning_rate": 0.00012387133182844244,
      "loss": 14.3223,
      "step": 509
    },
    {
      "epoch": 0.3680317517589753,
      "grad_norm": 118.36947631835938,
      "learning_rate": 0.00012373024830699775,
      "loss": 15.1374,
      "step": 510
    },
    {
      "epoch": 0.3687533826447772,
      "grad_norm": 105.57395935058594,
      "learning_rate": 0.00012358916478555305,
      "loss": 15.1502,
      "step": 511
    },
    {
      "epoch": 0.3694750135305791,
      "grad_norm": 111.07711029052734,
      "learning_rate": 0.00012344808126410836,
      "loss": 14.6353,
      "step": 512
    },
    {
      "epoch": 0.370196644416381,
      "grad_norm": 122.30259704589844,
      "learning_rate": 0.00012330699774266366,
      "loss": 14.9811,
      "step": 513
    },
    {
      "epoch": 0.3709182753021829,
      "grad_norm": 102.59977722167969,
      "learning_rate": 0.00012316591422121897,
      "loss": 13.489,
      "step": 514
    },
    {
      "epoch": 0.37163990618798487,
      "grad_norm": 93.66645812988281,
      "learning_rate": 0.00012302483069977427,
      "loss": 14.4031,
      "step": 515
    },
    {
      "epoch": 0.3723615370737868,
      "grad_norm": 102.31845092773438,
      "learning_rate": 0.00012288374717832958,
      "loss": 13.9767,
      "step": 516
    },
    {
      "epoch": 0.3730831679595887,
      "grad_norm": 129.65292358398438,
      "learning_rate": 0.00012274266365688488,
      "loss": 14.8661,
      "step": 517
    },
    {
      "epoch": 0.3738047988453906,
      "grad_norm": 118.78986358642578,
      "learning_rate": 0.0001226015801354402,
      "loss": 15.7023,
      "step": 518
    },
    {
      "epoch": 0.3745264297311925,
      "grad_norm": 111.90496826171875,
      "learning_rate": 0.0001224604966139955,
      "loss": 13.9988,
      "step": 519
    },
    {
      "epoch": 0.3752480606169944,
      "grad_norm": 130.30540466308594,
      "learning_rate": 0.0001223194130925508,
      "loss": 14.1668,
      "step": 520
    },
    {
      "epoch": 0.3759696915027963,
      "grad_norm": 97.40294647216797,
      "learning_rate": 0.0001221783295711061,
      "loss": 14.3807,
      "step": 521
    },
    {
      "epoch": 0.3766913223885982,
      "grad_norm": 108.43800354003906,
      "learning_rate": 0.00012203724604966141,
      "loss": 14.6728,
      "step": 522
    },
    {
      "epoch": 0.37741295327440016,
      "grad_norm": 101.33528137207031,
      "learning_rate": 0.00012189616252821671,
      "loss": 14.9731,
      "step": 523
    },
    {
      "epoch": 0.37813458416020207,
      "grad_norm": 90.9267578125,
      "learning_rate": 0.00012175507900677202,
      "loss": 13.5173,
      "step": 524
    },
    {
      "epoch": 0.378856215046004,
      "grad_norm": 117.5036849975586,
      "learning_rate": 0.00012161399548532731,
      "loss": 14.2397,
      "step": 525
    },
    {
      "epoch": 0.3795778459318059,
      "grad_norm": 90.33573150634766,
      "learning_rate": 0.00012147291196388262,
      "loss": 13.4485,
      "step": 526
    },
    {
      "epoch": 0.3802994768176078,
      "grad_norm": 111.95309448242188,
      "learning_rate": 0.00012133182844243792,
      "loss": 14.6135,
      "step": 527
    },
    {
      "epoch": 0.3810211077034097,
      "grad_norm": 100.55919647216797,
      "learning_rate": 0.00012119074492099323,
      "loss": 15.2512,
      "step": 528
    },
    {
      "epoch": 0.3817427385892116,
      "grad_norm": 89.58853149414062,
      "learning_rate": 0.00012104966139954853,
      "loss": 15.0263,
      "step": 529
    },
    {
      "epoch": 0.38246436947501355,
      "grad_norm": 90.38316345214844,
      "learning_rate": 0.00012090857787810385,
      "loss": 14.3207,
      "step": 530
    },
    {
      "epoch": 0.38318600036081546,
      "grad_norm": 109.44115447998047,
      "learning_rate": 0.00012076749435665914,
      "loss": 16.0014,
      "step": 531
    },
    {
      "epoch": 0.38390763124661736,
      "grad_norm": 106.12820434570312,
      "learning_rate": 0.00012062641083521445,
      "loss": 14.1644,
      "step": 532
    },
    {
      "epoch": 0.38462926213241927,
      "grad_norm": 97.64425659179688,
      "learning_rate": 0.00012048532731376975,
      "loss": 13.6517,
      "step": 533
    },
    {
      "epoch": 0.3853508930182212,
      "grad_norm": 96.9660873413086,
      "learning_rate": 0.00012034424379232506,
      "loss": 15.0527,
      "step": 534
    },
    {
      "epoch": 0.3860725239040231,
      "grad_norm": 103.27698516845703,
      "learning_rate": 0.00012020316027088036,
      "loss": 14.5855,
      "step": 535
    },
    {
      "epoch": 0.386794154789825,
      "grad_norm": 111.63204956054688,
      "learning_rate": 0.00012006207674943567,
      "loss": 15.0781,
      "step": 536
    },
    {
      "epoch": 0.3875157856756269,
      "grad_norm": 112.74964141845703,
      "learning_rate": 0.00011992099322799097,
      "loss": 14.4077,
      "step": 537
    },
    {
      "epoch": 0.38823741656142885,
      "grad_norm": 92.49209594726562,
      "learning_rate": 0.00011977990970654628,
      "loss": 14.6495,
      "step": 538
    },
    {
      "epoch": 0.38895904744723075,
      "grad_norm": 104.56832885742188,
      "learning_rate": 0.00011963882618510158,
      "loss": 14.0355,
      "step": 539
    },
    {
      "epoch": 0.38968067833303266,
      "grad_norm": 107.10399627685547,
      "learning_rate": 0.00011949774266365689,
      "loss": 14.3165,
      "step": 540
    },
    {
      "epoch": 0.39040230921883456,
      "grad_norm": 102.74498748779297,
      "learning_rate": 0.0001193566591422122,
      "loss": 15.1019,
      "step": 541
    },
    {
      "epoch": 0.39112394010463647,
      "grad_norm": 96.18663024902344,
      "learning_rate": 0.0001192155756207675,
      "loss": 13.6234,
      "step": 542
    },
    {
      "epoch": 0.3918455709904384,
      "grad_norm": 86.39244842529297,
      "learning_rate": 0.0001190744920993228,
      "loss": 13.6117,
      "step": 543
    },
    {
      "epoch": 0.3925672018762403,
      "grad_norm": 86.12174224853516,
      "learning_rate": 0.0001189334085778781,
      "loss": 14.2035,
      "step": 544
    },
    {
      "epoch": 0.39328883276204224,
      "grad_norm": 103.39188385009766,
      "learning_rate": 0.0001187923250564334,
      "loss": 14.0919,
      "step": 545
    },
    {
      "epoch": 0.39401046364784414,
      "grad_norm": 92.8990249633789,
      "learning_rate": 0.00011865124153498872,
      "loss": 14.8843,
      "step": 546
    },
    {
      "epoch": 0.39473209453364605,
      "grad_norm": 118.48664093017578,
      "learning_rate": 0.00011851015801354402,
      "loss": 13.6491,
      "step": 547
    },
    {
      "epoch": 0.39545372541944795,
      "grad_norm": 96.70562744140625,
      "learning_rate": 0.00011836907449209933,
      "loss": 14.6202,
      "step": 548
    },
    {
      "epoch": 0.39617535630524986,
      "grad_norm": 110.56517791748047,
      "learning_rate": 0.00011822799097065463,
      "loss": 14.0693,
      "step": 549
    },
    {
      "epoch": 0.39689698719105176,
      "grad_norm": 88.38846588134766,
      "learning_rate": 0.00011808690744920994,
      "loss": 13.9012,
      "step": 550
    },
    {
      "epoch": 0.39761861807685367,
      "grad_norm": 91.24594116210938,
      "learning_rate": 0.00011794582392776523,
      "loss": 13.5162,
      "step": 551
    },
    {
      "epoch": 0.3983402489626556,
      "grad_norm": 109.93029022216797,
      "learning_rate": 0.00011780474040632054,
      "loss": 13.3766,
      "step": 552
    },
    {
      "epoch": 0.39906187984845753,
      "grad_norm": 103.53567504882812,
      "learning_rate": 0.00011766365688487584,
      "loss": 13.717,
      "step": 553
    },
    {
      "epoch": 0.39978351073425944,
      "grad_norm": 86.23466491699219,
      "learning_rate": 0.00011752257336343116,
      "loss": 13.9589,
      "step": 554
    },
    {
      "epoch": 0.40050514162006134,
      "grad_norm": 107.09442138671875,
      "learning_rate": 0.00011738148984198646,
      "loss": 13.4732,
      "step": 555
    },
    {
      "epoch": 0.40122677250586325,
      "grad_norm": 90.00965118408203,
      "learning_rate": 0.00011724040632054177,
      "loss": 12.8527,
      "step": 556
    },
    {
      "epoch": 0.40194840339166515,
      "grad_norm": 93.91171264648438,
      "learning_rate": 0.00011709932279909707,
      "loss": 13.8265,
      "step": 557
    },
    {
      "epoch": 0.40267003427746706,
      "grad_norm": 89.9669418334961,
      "learning_rate": 0.00011695823927765237,
      "loss": 13.706,
      "step": 558
    },
    {
      "epoch": 0.40339166516326896,
      "grad_norm": 87.6710433959961,
      "learning_rate": 0.00011681715575620767,
      "loss": 13.1498,
      "step": 559
    },
    {
      "epoch": 0.4041132960490709,
      "grad_norm": 96.84847259521484,
      "learning_rate": 0.00011667607223476298,
      "loss": 13.5037,
      "step": 560
    },
    {
      "epoch": 0.40483492693487283,
      "grad_norm": 83.51012420654297,
      "learning_rate": 0.00011653498871331828,
      "loss": 14.3493,
      "step": 561
    },
    {
      "epoch": 0.40555655782067473,
      "grad_norm": 92.87759399414062,
      "learning_rate": 0.0001163939051918736,
      "loss": 13.752,
      "step": 562
    },
    {
      "epoch": 0.40627818870647664,
      "grad_norm": 100.40760803222656,
      "learning_rate": 0.0001162528216704289,
      "loss": 14.2797,
      "step": 563
    },
    {
      "epoch": 0.40699981959227854,
      "grad_norm": 100.45477294921875,
      "learning_rate": 0.0001161117381489842,
      "loss": 14.029,
      "step": 564
    },
    {
      "epoch": 0.40772145047808045,
      "grad_norm": 96.00150299072266,
      "learning_rate": 0.0001159706546275395,
      "loss": 13.2598,
      "step": 565
    },
    {
      "epoch": 0.40844308136388235,
      "grad_norm": 122.12696838378906,
      "learning_rate": 0.00011582957110609481,
      "loss": 14.5196,
      "step": 566
    },
    {
      "epoch": 0.40916471224968426,
      "grad_norm": 95.76515197753906,
      "learning_rate": 0.00011568848758465011,
      "loss": 12.6844,
      "step": 567
    },
    {
      "epoch": 0.4098863431354862,
      "grad_norm": 106.6192855834961,
      "learning_rate": 0.00011554740406320542,
      "loss": 13.6793,
      "step": 568
    },
    {
      "epoch": 0.4106079740212881,
      "grad_norm": 99.8768539428711,
      "learning_rate": 0.00011540632054176072,
      "loss": 13.3968,
      "step": 569
    },
    {
      "epoch": 0.41132960490709003,
      "grad_norm": 105.91264343261719,
      "learning_rate": 0.00011526523702031604,
      "loss": 13.19,
      "step": 570
    },
    {
      "epoch": 0.41205123579289193,
      "grad_norm": 98.67652130126953,
      "learning_rate": 0.00011512415349887133,
      "loss": 13.045,
      "step": 571
    },
    {
      "epoch": 0.41277286667869384,
      "grad_norm": 88.93312072753906,
      "learning_rate": 0.00011498306997742664,
      "loss": 12.5237,
      "step": 572
    },
    {
      "epoch": 0.41349449756449574,
      "grad_norm": 98.9939193725586,
      "learning_rate": 0.00011484198645598194,
      "loss": 13.9018,
      "step": 573
    },
    {
      "epoch": 0.41421612845029765,
      "grad_norm": 116.89205169677734,
      "learning_rate": 0.00011470090293453725,
      "loss": 13.7565,
      "step": 574
    },
    {
      "epoch": 0.4149377593360996,
      "grad_norm": 99.4476547241211,
      "learning_rate": 0.00011455981941309255,
      "loss": 13.8062,
      "step": 575
    },
    {
      "epoch": 0.4156593902219015,
      "grad_norm": 102.70655822753906,
      "learning_rate": 0.00011441873589164786,
      "loss": 15.0257,
      "step": 576
    },
    {
      "epoch": 0.4163810211077034,
      "grad_norm": 88.51380920410156,
      "learning_rate": 0.00011427765237020316,
      "loss": 14.1221,
      "step": 577
    },
    {
      "epoch": 0.4171026519935053,
      "grad_norm": 89.39110565185547,
      "learning_rate": 0.00011413656884875847,
      "loss": 14.2695,
      "step": 578
    },
    {
      "epoch": 0.41782428287930723,
      "grad_norm": 107.13184356689453,
      "learning_rate": 0.00011399548532731377,
      "loss": 12.882,
      "step": 579
    },
    {
      "epoch": 0.41854591376510913,
      "grad_norm": 85.8575210571289,
      "learning_rate": 0.00011385440180586908,
      "loss": 12.9901,
      "step": 580
    },
    {
      "epoch": 0.41926754465091104,
      "grad_norm": 96.99238586425781,
      "learning_rate": 0.00011371331828442438,
      "loss": 13.2631,
      "step": 581
    },
    {
      "epoch": 0.41998917553671294,
      "grad_norm": 101.64766693115234,
      "learning_rate": 0.00011357223476297969,
      "loss": 14.0139,
      "step": 582
    },
    {
      "epoch": 0.4207108064225149,
      "grad_norm": 91.51809692382812,
      "learning_rate": 0.000113431151241535,
      "loss": 12.6754,
      "step": 583
    },
    {
      "epoch": 0.4214324373083168,
      "grad_norm": 96.91360473632812,
      "learning_rate": 0.00011329006772009029,
      "loss": 13.0701,
      "step": 584
    },
    {
      "epoch": 0.4221540681941187,
      "grad_norm": 108.7840347290039,
      "learning_rate": 0.00011314898419864559,
      "loss": 13.4396,
      "step": 585
    },
    {
      "epoch": 0.4228756990799206,
      "grad_norm": 118.00025177001953,
      "learning_rate": 0.00011300790067720091,
      "loss": 14.1289,
      "step": 586
    },
    {
      "epoch": 0.4235973299657225,
      "grad_norm": 116.08101654052734,
      "learning_rate": 0.00011286681715575621,
      "loss": 13.5901,
      "step": 587
    },
    {
      "epoch": 0.42431896085152443,
      "grad_norm": 113.78700256347656,
      "learning_rate": 0.00011272573363431152,
      "loss": 14.0773,
      "step": 588
    },
    {
      "epoch": 0.42504059173732633,
      "grad_norm": 121.30341339111328,
      "learning_rate": 0.00011258465011286682,
      "loss": 14.2263,
      "step": 589
    },
    {
      "epoch": 0.4257622226231283,
      "grad_norm": 106.71513366699219,
      "learning_rate": 0.00011244356659142213,
      "loss": 14.4568,
      "step": 590
    },
    {
      "epoch": 0.4264838535089302,
      "grad_norm": 81.98831939697266,
      "learning_rate": 0.00011230248306997742,
      "loss": 13.3999,
      "step": 591
    },
    {
      "epoch": 0.4272054843947321,
      "grad_norm": 96.3595962524414,
      "learning_rate": 0.00011216139954853273,
      "loss": 13.2822,
      "step": 592
    },
    {
      "epoch": 0.427927115280534,
      "grad_norm": 89.15560150146484,
      "learning_rate": 0.00011202031602708803,
      "loss": 12.0795,
      "step": 593
    },
    {
      "epoch": 0.4286487461663359,
      "grad_norm": 83.31126403808594,
      "learning_rate": 0.00011187923250564335,
      "loss": 12.7014,
      "step": 594
    },
    {
      "epoch": 0.4293703770521378,
      "grad_norm": 111.7543716430664,
      "learning_rate": 0.00011173814898419866,
      "loss": 12.8726,
      "step": 595
    },
    {
      "epoch": 0.4300920079379397,
      "grad_norm": 95.83319854736328,
      "learning_rate": 0.00011159706546275396,
      "loss": 13.9532,
      "step": 596
    },
    {
      "epoch": 0.43081363882374163,
      "grad_norm": 107.92778015136719,
      "learning_rate": 0.00011145598194130925,
      "loss": 12.3464,
      "step": 597
    },
    {
      "epoch": 0.4315352697095436,
      "grad_norm": 92.03345489501953,
      "learning_rate": 0.00011131489841986456,
      "loss": 13.2921,
      "step": 598
    },
    {
      "epoch": 0.4322569005953455,
      "grad_norm": 88.37442779541016,
      "learning_rate": 0.00011117381489841986,
      "loss": 13.6214,
      "step": 599
    },
    {
      "epoch": 0.4329785314811474,
      "grad_norm": 87.6718521118164,
      "learning_rate": 0.00011103273137697517,
      "loss": 13.4753,
      "step": 600
    },
    {
      "epoch": 0.4337001623669493,
      "grad_norm": 108.83466339111328,
      "learning_rate": 0.00011089164785553047,
      "loss": 13.7724,
      "step": 601
    },
    {
      "epoch": 0.4344217932527512,
      "grad_norm": 102.25979614257812,
      "learning_rate": 0.00011075056433408579,
      "loss": 13.3689,
      "step": 602
    },
    {
      "epoch": 0.4351434241385531,
      "grad_norm": 116.1436996459961,
      "learning_rate": 0.0001106094808126411,
      "loss": 13.7307,
      "step": 603
    },
    {
      "epoch": 0.435865055024355,
      "grad_norm": 83.8226318359375,
      "learning_rate": 0.00011046839729119639,
      "loss": 13.3559,
      "step": 604
    },
    {
      "epoch": 0.436586685910157,
      "grad_norm": 104.93334197998047,
      "learning_rate": 0.00011032731376975169,
      "loss": 14.0033,
      "step": 605
    },
    {
      "epoch": 0.4373083167959589,
      "grad_norm": 118.91468048095703,
      "learning_rate": 0.000110186230248307,
      "loss": 12.679,
      "step": 606
    },
    {
      "epoch": 0.4380299476817608,
      "grad_norm": 89.45987701416016,
      "learning_rate": 0.0001100451467268623,
      "loss": 12.7694,
      "step": 607
    },
    {
      "epoch": 0.4387515785675627,
      "grad_norm": 82.9248046875,
      "learning_rate": 0.00010990406320541761,
      "loss": 12.481,
      "step": 608
    },
    {
      "epoch": 0.4394732094533646,
      "grad_norm": 120.43325805664062,
      "learning_rate": 0.00010976297968397291,
      "loss": 13.1024,
      "step": 609
    },
    {
      "epoch": 0.4401948403391665,
      "grad_norm": 106.62741088867188,
      "learning_rate": 0.00010962189616252823,
      "loss": 13.879,
      "step": 610
    },
    {
      "epoch": 0.4409164712249684,
      "grad_norm": 92.0052719116211,
      "learning_rate": 0.00010948081264108352,
      "loss": 14.3169,
      "step": 611
    },
    {
      "epoch": 0.4416381021107703,
      "grad_norm": 110.99105834960938,
      "learning_rate": 0.00010933972911963883,
      "loss": 13.0437,
      "step": 612
    },
    {
      "epoch": 0.4423597329965723,
      "grad_norm": 85.39056396484375,
      "learning_rate": 0.00010919864559819413,
      "loss": 13.1998,
      "step": 613
    },
    {
      "epoch": 0.4430813638823742,
      "grad_norm": 101.80738067626953,
      "learning_rate": 0.00010905756207674944,
      "loss": 13.7956,
      "step": 614
    },
    {
      "epoch": 0.4438029947681761,
      "grad_norm": 104.85050201416016,
      "learning_rate": 0.00010891647855530474,
      "loss": 14.2438,
      "step": 615
    },
    {
      "epoch": 0.444524625653978,
      "grad_norm": 86.3118667602539,
      "learning_rate": 0.00010877539503386005,
      "loss": 13.5729,
      "step": 616
    },
    {
      "epoch": 0.4452462565397799,
      "grad_norm": 82.41910552978516,
      "learning_rate": 0.00010863431151241534,
      "loss": 12.6527,
      "step": 617
    },
    {
      "epoch": 0.4459678874255818,
      "grad_norm": 99.43305206298828,
      "learning_rate": 0.00010849322799097066,
      "loss": 13.8682,
      "step": 618
    },
    {
      "epoch": 0.4466895183113837,
      "grad_norm": 103.4736328125,
      "learning_rate": 0.00010835214446952596,
      "loss": 13.3942,
      "step": 619
    },
    {
      "epoch": 0.44741114919718566,
      "grad_norm": 95.51641845703125,
      "learning_rate": 0.00010821106094808127,
      "loss": 14.2415,
      "step": 620
    },
    {
      "epoch": 0.44813278008298757,
      "grad_norm": 127.47482299804688,
      "learning_rate": 0.00010806997742663657,
      "loss": 13.1604,
      "step": 621
    },
    {
      "epoch": 0.4488544109687895,
      "grad_norm": 100.02677154541016,
      "learning_rate": 0.00010792889390519188,
      "loss": 12.696,
      "step": 622
    },
    {
      "epoch": 0.4495760418545914,
      "grad_norm": 80.3219985961914,
      "learning_rate": 0.00010778781038374718,
      "loss": 12.9829,
      "step": 623
    },
    {
      "epoch": 0.4502976727403933,
      "grad_norm": 117.77849578857422,
      "learning_rate": 0.00010764672686230248,
      "loss": 14.0596,
      "step": 624
    },
    {
      "epoch": 0.4510193036261952,
      "grad_norm": 86.93912506103516,
      "learning_rate": 0.00010750564334085778,
      "loss": 12.5347,
      "step": 625
    },
    {
      "epoch": 0.4517409345119971,
      "grad_norm": 96.33490753173828,
      "learning_rate": 0.0001073645598194131,
      "loss": 14.6049,
      "step": 626
    },
    {
      "epoch": 0.452462565397799,
      "grad_norm": 80.06140899658203,
      "learning_rate": 0.0001072234762979684,
      "loss": 13.3141,
      "step": 627
    },
    {
      "epoch": 0.45318419628360096,
      "grad_norm": 74.47271728515625,
      "learning_rate": 0.00010708239277652371,
      "loss": 12.6988,
      "step": 628
    },
    {
      "epoch": 0.45390582716940286,
      "grad_norm": 93.03837585449219,
      "learning_rate": 0.00010694130925507902,
      "loss": 12.4619,
      "step": 629
    },
    {
      "epoch": 0.45462745805520477,
      "grad_norm": 89.75064086914062,
      "learning_rate": 0.00010680022573363431,
      "loss": 12.0222,
      "step": 630
    },
    {
      "epoch": 0.4553490889410067,
      "grad_norm": 78.06835174560547,
      "learning_rate": 0.00010665914221218961,
      "loss": 13.2799,
      "step": 631
    },
    {
      "epoch": 0.4560707198268086,
      "grad_norm": 85.0858383178711,
      "learning_rate": 0.00010651805869074492,
      "loss": 13.2786,
      "step": 632
    },
    {
      "epoch": 0.4567923507126105,
      "grad_norm": 87.6370620727539,
      "learning_rate": 0.00010637697516930022,
      "loss": 13.0639,
      "step": 633
    },
    {
      "epoch": 0.4575139815984124,
      "grad_norm": 82.34672546386719,
      "learning_rate": 0.00010623589164785554,
      "loss": 13.2672,
      "step": 634
    },
    {
      "epoch": 0.45823561248421435,
      "grad_norm": 87.238525390625,
      "learning_rate": 0.00010609480812641085,
      "loss": 13.422,
      "step": 635
    },
    {
      "epoch": 0.45895724337001625,
      "grad_norm": 82.10929870605469,
      "learning_rate": 0.00010595372460496615,
      "loss": 12.6411,
      "step": 636
    },
    {
      "epoch": 0.45967887425581816,
      "grad_norm": 85.94368743896484,
      "learning_rate": 0.00010581264108352144,
      "loss": 12.5319,
      "step": 637
    },
    {
      "epoch": 0.46040050514162006,
      "grad_norm": 91.06353759765625,
      "learning_rate": 0.00010567155756207675,
      "loss": 13.0128,
      "step": 638
    },
    {
      "epoch": 0.46112213602742197,
      "grad_norm": 87.8818359375,
      "learning_rate": 0.00010553047404063205,
      "loss": 13.6336,
      "step": 639
    },
    {
      "epoch": 0.4618437669132239,
      "grad_norm": 94.73768615722656,
      "learning_rate": 0.00010538939051918736,
      "loss": 12.58,
      "step": 640
    },
    {
      "epoch": 0.4625653977990258,
      "grad_norm": 79.07640838623047,
      "learning_rate": 0.00010524830699774266,
      "loss": 12.3637,
      "step": 641
    },
    {
      "epoch": 0.4632870286848277,
      "grad_norm": 92.489990234375,
      "learning_rate": 0.00010510722347629798,
      "loss": 13.056,
      "step": 642
    },
    {
      "epoch": 0.46400865957062964,
      "grad_norm": 100.58576202392578,
      "learning_rate": 0.00010496613995485329,
      "loss": 13.1683,
      "step": 643
    },
    {
      "epoch": 0.46473029045643155,
      "grad_norm": 84.3514404296875,
      "learning_rate": 0.00010482505643340858,
      "loss": 12.9588,
      "step": 644
    },
    {
      "epoch": 0.46545192134223345,
      "grad_norm": 96.27488708496094,
      "learning_rate": 0.00010468397291196388,
      "loss": 13.2994,
      "step": 645
    },
    {
      "epoch": 0.46617355222803536,
      "grad_norm": 102.00543975830078,
      "learning_rate": 0.00010454288939051919,
      "loss": 11.7685,
      "step": 646
    },
    {
      "epoch": 0.46689518311383726,
      "grad_norm": 95.55345916748047,
      "learning_rate": 0.0001044018058690745,
      "loss": 13.3954,
      "step": 647
    },
    {
      "epoch": 0.46761681399963917,
      "grad_norm": 93.48489379882812,
      "learning_rate": 0.0001042607223476298,
      "loss": 12.6682,
      "step": 648
    },
    {
      "epoch": 0.4683384448854411,
      "grad_norm": 99.07994079589844,
      "learning_rate": 0.0001041196388261851,
      "loss": 12.7108,
      "step": 649
    },
    {
      "epoch": 0.46906007577124303,
      "grad_norm": 95.75413513183594,
      "learning_rate": 0.0001039785553047404,
      "loss": 13.1884,
      "step": 650
    },
    {
      "epoch": 0.46978170665704494,
      "grad_norm": 78.4039535522461,
      "learning_rate": 0.00010383747178329571,
      "loss": 12.6184,
      "step": 651
    },
    {
      "epoch": 0.47050333754284684,
      "grad_norm": 80.81806182861328,
      "learning_rate": 0.00010369638826185102,
      "loss": 11.9802,
      "step": 652
    },
    {
      "epoch": 0.47122496842864875,
      "grad_norm": 102.00237274169922,
      "learning_rate": 0.00010355530474040632,
      "loss": 13.9649,
      "step": 653
    },
    {
      "epoch": 0.47194659931445065,
      "grad_norm": 85.51598358154297,
      "learning_rate": 0.00010341422121896163,
      "loss": 12.6312,
      "step": 654
    },
    {
      "epoch": 0.47266823020025256,
      "grad_norm": 83.99607849121094,
      "learning_rate": 0.00010327313769751693,
      "loss": 13.0793,
      "step": 655
    },
    {
      "epoch": 0.47338986108605446,
      "grad_norm": 90.86685180664062,
      "learning_rate": 0.00010313205417607224,
      "loss": 13.9678,
      "step": 656
    },
    {
      "epoch": 0.47411149197185637,
      "grad_norm": 95.53041076660156,
      "learning_rate": 0.00010299097065462753,
      "loss": 12.9942,
      "step": 657
    },
    {
      "epoch": 0.47483312285765833,
      "grad_norm": 73.63922119140625,
      "learning_rate": 0.00010284988713318284,
      "loss": 12.7665,
      "step": 658
    },
    {
      "epoch": 0.47555475374346023,
      "grad_norm": 93.86263275146484,
      "learning_rate": 0.00010270880361173815,
      "loss": 11.4957,
      "step": 659
    },
    {
      "epoch": 0.47627638462926214,
      "grad_norm": 84.24411010742188,
      "learning_rate": 0.00010256772009029346,
      "loss": 11.9598,
      "step": 660
    },
    {
      "epoch": 0.47699801551506404,
      "grad_norm": 92.21031951904297,
      "learning_rate": 0.00010242663656884877,
      "loss": 12.9699,
      "step": 661
    },
    {
      "epoch": 0.47771964640086595,
      "grad_norm": 77.28047943115234,
      "learning_rate": 0.00010228555304740407,
      "loss": 12.1119,
      "step": 662
    },
    {
      "epoch": 0.47844127728666785,
      "grad_norm": 81.31893920898438,
      "learning_rate": 0.00010214446952595936,
      "loss": 12.4585,
      "step": 663
    },
    {
      "epoch": 0.47916290817246976,
      "grad_norm": 90.51937103271484,
      "learning_rate": 0.00010200338600451467,
      "loss": 13.8158,
      "step": 664
    },
    {
      "epoch": 0.4798845390582717,
      "grad_norm": 94.69541931152344,
      "learning_rate": 0.00010186230248306997,
      "loss": 13.7301,
      "step": 665
    },
    {
      "epoch": 0.4806061699440736,
      "grad_norm": 86.94699096679688,
      "learning_rate": 0.00010172121896162528,
      "loss": 13.2009,
      "step": 666
    },
    {
      "epoch": 0.48132780082987553,
      "grad_norm": 70.67619323730469,
      "learning_rate": 0.0001015801354401806,
      "loss": 11.5006,
      "step": 667
    },
    {
      "epoch": 0.48204943171567743,
      "grad_norm": 93.92106628417969,
      "learning_rate": 0.0001014390519187359,
      "loss": 12.879,
      "step": 668
    },
    {
      "epoch": 0.48277106260147934,
      "grad_norm": 105.28347778320312,
      "learning_rate": 0.0001012979683972912,
      "loss": 13.2651,
      "step": 669
    },
    {
      "epoch": 0.48349269348728124,
      "grad_norm": 81.05976104736328,
      "learning_rate": 0.0001011568848758465,
      "loss": 11.7151,
      "step": 670
    },
    {
      "epoch": 0.48421432437308315,
      "grad_norm": 78.24513244628906,
      "learning_rate": 0.0001010158013544018,
      "loss": 12.3579,
      "step": 671
    },
    {
      "epoch": 0.48493595525888505,
      "grad_norm": 101.492431640625,
      "learning_rate": 0.00010087471783295711,
      "loss": 13.4105,
      "step": 672
    },
    {
      "epoch": 0.485657586144687,
      "grad_norm": 87.78677368164062,
      "learning_rate": 0.00010073363431151241,
      "loss": 12.8416,
      "step": 673
    },
    {
      "epoch": 0.4863792170304889,
      "grad_norm": 82.22223663330078,
      "learning_rate": 0.00010059255079006772,
      "loss": 13.6085,
      "step": 674
    },
    {
      "epoch": 0.4871008479162908,
      "grad_norm": 97.35003662109375,
      "learning_rate": 0.00010045146726862304,
      "loss": 13.3614,
      "step": 675
    },
    {
      "epoch": 0.48782247880209273,
      "grad_norm": 82.89742279052734,
      "learning_rate": 0.00010031038374717834,
      "loss": 11.5411,
      "step": 676
    },
    {
      "epoch": 0.48854410968789463,
      "grad_norm": 81.66832733154297,
      "learning_rate": 0.00010016930022573363,
      "loss": 12.6214,
      "step": 677
    },
    {
      "epoch": 0.48926574057369654,
      "grad_norm": 84.4952163696289,
      "learning_rate": 0.00010002821670428894,
      "loss": 12.3984,
      "step": 678
    },
    {
      "epoch": 0.48998737145949844,
      "grad_norm": 81.41690063476562,
      "learning_rate": 9.988713318284424e-05,
      "loss": 12.9836,
      "step": 679
    },
    {
      "epoch": 0.4907090023453004,
      "grad_norm": 80.92134857177734,
      "learning_rate": 9.974604966139955e-05,
      "loss": 12.237,
      "step": 680
    },
    {
      "epoch": 0.4914306332311023,
      "grad_norm": 86.71517944335938,
      "learning_rate": 9.960496613995485e-05,
      "loss": 12.3456,
      "step": 681
    },
    {
      "epoch": 0.4921522641169042,
      "grad_norm": 90.72042083740234,
      "learning_rate": 9.946388261851016e-05,
      "loss": 11.4088,
      "step": 682
    },
    {
      "epoch": 0.4928738950027061,
      "grad_norm": 78.86740112304688,
      "learning_rate": 9.932279909706546e-05,
      "loss": 12.5479,
      "step": 683
    },
    {
      "epoch": 0.493595525888508,
      "grad_norm": 94.35279083251953,
      "learning_rate": 9.918171557562077e-05,
      "loss": 13.8863,
      "step": 684
    },
    {
      "epoch": 0.49431715677430993,
      "grad_norm": 85.07140350341797,
      "learning_rate": 9.904063205417607e-05,
      "loss": 12.4362,
      "step": 685
    },
    {
      "epoch": 0.49503878766011183,
      "grad_norm": 84.79291534423828,
      "learning_rate": 9.889954853273138e-05,
      "loss": 12.4712,
      "step": 686
    },
    {
      "epoch": 0.49576041854591374,
      "grad_norm": 88.05317687988281,
      "learning_rate": 9.875846501128668e-05,
      "loss": 11.5779,
      "step": 687
    },
    {
      "epoch": 0.4964820494317157,
      "grad_norm": 94.61351013183594,
      "learning_rate": 9.861738148984199e-05,
      "loss": 13.2769,
      "step": 688
    },
    {
      "epoch": 0.4972036803175176,
      "grad_norm": 112.1016616821289,
      "learning_rate": 9.84762979683973e-05,
      "loss": 12.9383,
      "step": 689
    },
    {
      "epoch": 0.4979253112033195,
      "grad_norm": 85.61279296875,
      "learning_rate": 9.833521444695259e-05,
      "loss": 12.1359,
      "step": 690
    },
    {
      "epoch": 0.4986469420891214,
      "grad_norm": 85.24128723144531,
      "learning_rate": 9.81941309255079e-05,
      "loss": 13.0065,
      "step": 691
    },
    {
      "epoch": 0.4993685729749233,
      "grad_norm": 83.55330657958984,
      "learning_rate": 9.805304740406321e-05,
      "loss": 12.685,
      "step": 692
    },
    {
      "epoch": 0.5000902038607252,
      "grad_norm": 96.72206115722656,
      "learning_rate": 9.791196388261851e-05,
      "loss": 12.6586,
      "step": 693
    },
    {
      "epoch": 0.5008118347465271,
      "grad_norm": 89.1855697631836,
      "learning_rate": 9.777088036117382e-05,
      "loss": 12.9596,
      "step": 694
    },
    {
      "epoch": 0.501533465632329,
      "grad_norm": 90.14330291748047,
      "learning_rate": 9.762979683972913e-05,
      "loss": 11.9696,
      "step": 695
    },
    {
      "epoch": 0.5022550965181309,
      "grad_norm": 108.54988861083984,
      "learning_rate": 9.748871331828443e-05,
      "loss": 12.8935,
      "step": 696
    },
    {
      "epoch": 0.5029767274039328,
      "grad_norm": 74.86002349853516,
      "learning_rate": 9.734762979683972e-05,
      "loss": 12.4487,
      "step": 697
    },
    {
      "epoch": 0.5036983582897347,
      "grad_norm": 87.67378997802734,
      "learning_rate": 9.720654627539503e-05,
      "loss": 12.773,
      "step": 698
    },
    {
      "epoch": 0.5044199891755368,
      "grad_norm": 84.11434173583984,
      "learning_rate": 9.706546275395035e-05,
      "loss": 11.9354,
      "step": 699
    },
    {
      "epoch": 0.5051416200613387,
      "grad_norm": 102.86473846435547,
      "learning_rate": 9.692437923250565e-05,
      "loss": 13.0093,
      "step": 700
    },
    {
      "epoch": 0.5058632509471406,
      "grad_norm": 80.49070739746094,
      "learning_rate": 9.678329571106096e-05,
      "loss": 12.0481,
      "step": 701
    },
    {
      "epoch": 0.5065848818329425,
      "grad_norm": 90.2936782836914,
      "learning_rate": 9.664221218961626e-05,
      "loss": 12.9525,
      "step": 702
    },
    {
      "epoch": 0.5073065127187444,
      "grad_norm": 88.13471984863281,
      "learning_rate": 9.650112866817155e-05,
      "loss": 12.5695,
      "step": 703
    },
    {
      "epoch": 0.5080281436045463,
      "grad_norm": 76.30525207519531,
      "learning_rate": 9.636004514672686e-05,
      "loss": 11.0117,
      "step": 704
    },
    {
      "epoch": 0.5087497744903482,
      "grad_norm": 91.15892028808594,
      "learning_rate": 9.621896162528216e-05,
      "loss": 12.3018,
      "step": 705
    },
    {
      "epoch": 0.5094714053761501,
      "grad_norm": 78.84673309326172,
      "learning_rate": 9.607787810383747e-05,
      "loss": 11.7751,
      "step": 706
    },
    {
      "epoch": 0.510193036261952,
      "grad_norm": 92.97802734375,
      "learning_rate": 9.593679458239279e-05,
      "loss": 13.9975,
      "step": 707
    },
    {
      "epoch": 0.5109146671477539,
      "grad_norm": 75.7619400024414,
      "learning_rate": 9.579571106094809e-05,
      "loss": 12.4511,
      "step": 708
    },
    {
      "epoch": 0.5116362980335558,
      "grad_norm": 90.13046264648438,
      "learning_rate": 9.56546275395034e-05,
      "loss": 13.2156,
      "step": 709
    },
    {
      "epoch": 0.5123579289193577,
      "grad_norm": 92.82148742675781,
      "learning_rate": 9.551354401805869e-05,
      "loss": 12.2233,
      "step": 710
    },
    {
      "epoch": 0.5130795598051596,
      "grad_norm": 82.00482177734375,
      "learning_rate": 9.5372460496614e-05,
      "loss": 12.5661,
      "step": 711
    },
    {
      "epoch": 0.5138011906909615,
      "grad_norm": 83.72935485839844,
      "learning_rate": 9.52313769751693e-05,
      "loss": 12.5702,
      "step": 712
    },
    {
      "epoch": 0.5145228215767634,
      "grad_norm": 85.22624206542969,
      "learning_rate": 9.50902934537246e-05,
      "loss": 12.8843,
      "step": 713
    },
    {
      "epoch": 0.5152444524625654,
      "grad_norm": 69.87435150146484,
      "learning_rate": 9.494920993227991e-05,
      "loss": 13.3846,
      "step": 714
    },
    {
      "epoch": 0.5159660833483674,
      "grad_norm": 72.89051055908203,
      "learning_rate": 9.480812641083523e-05,
      "loss": 12.7495,
      "step": 715
    },
    {
      "epoch": 0.5166877142341693,
      "grad_norm": 85.20159912109375,
      "learning_rate": 9.466704288939052e-05,
      "loss": 10.8245,
      "step": 716
    },
    {
      "epoch": 0.5174093451199712,
      "grad_norm": 76.5858383178711,
      "learning_rate": 9.452595936794582e-05,
      "loss": 12.8879,
      "step": 717
    },
    {
      "epoch": 0.5181309760057731,
      "grad_norm": 67.74018096923828,
      "learning_rate": 9.438487584650113e-05,
      "loss": 11.9238,
      "step": 718
    },
    {
      "epoch": 0.518852606891575,
      "grad_norm": 84.66812896728516,
      "learning_rate": 9.424379232505643e-05,
      "loss": 12.6539,
      "step": 719
    },
    {
      "epoch": 0.5195742377773769,
      "grad_norm": 82.11048889160156,
      "learning_rate": 9.410270880361174e-05,
      "loss": 12.0807,
      "step": 720
    },
    {
      "epoch": 0.5202958686631788,
      "grad_norm": 71.04389190673828,
      "learning_rate": 9.396162528216704e-05,
      "loss": 11.9115,
      "step": 721
    },
    {
      "epoch": 0.5210174995489807,
      "grad_norm": 77.79650115966797,
      "learning_rate": 9.382054176072235e-05,
      "loss": 11.48,
      "step": 722
    },
    {
      "epoch": 0.5217391304347826,
      "grad_norm": 64.25723266601562,
      "learning_rate": 9.367945823927765e-05,
      "loss": 10.8325,
      "step": 723
    },
    {
      "epoch": 0.5224607613205845,
      "grad_norm": 93.23188781738281,
      "learning_rate": 9.353837471783296e-05,
      "loss": 12.0706,
      "step": 724
    },
    {
      "epoch": 0.5231823922063864,
      "grad_norm": 83.97286987304688,
      "learning_rate": 9.339729119638826e-05,
      "loss": 11.9713,
      "step": 725
    },
    {
      "epoch": 0.5239040230921883,
      "grad_norm": 75.68380737304688,
      "learning_rate": 9.325620767494357e-05,
      "loss": 11.5696,
      "step": 726
    },
    {
      "epoch": 0.5246256539779902,
      "grad_norm": 74.6787338256836,
      "learning_rate": 9.311512415349887e-05,
      "loss": 11.2456,
      "step": 727
    },
    {
      "epoch": 0.5253472848637921,
      "grad_norm": 78.78028869628906,
      "learning_rate": 9.297404063205418e-05,
      "loss": 13.1013,
      "step": 728
    },
    {
      "epoch": 0.5260689157495941,
      "grad_norm": 77.73463439941406,
      "learning_rate": 9.283295711060949e-05,
      "loss": 12.0227,
      "step": 729
    },
    {
      "epoch": 0.526790546635396,
      "grad_norm": 75.90663146972656,
      "learning_rate": 9.269187358916478e-05,
      "loss": 12.5308,
      "step": 730
    },
    {
      "epoch": 0.527512177521198,
      "grad_norm": 82.56885528564453,
      "learning_rate": 9.25507900677201e-05,
      "loss": 11.0731,
      "step": 731
    },
    {
      "epoch": 0.5282338084069998,
      "grad_norm": 78.40550994873047,
      "learning_rate": 9.24097065462754e-05,
      "loss": 12.6963,
      "step": 732
    },
    {
      "epoch": 0.5289554392928018,
      "grad_norm": 74.3733901977539,
      "learning_rate": 9.22686230248307e-05,
      "loss": 12.2888,
      "step": 733
    },
    {
      "epoch": 0.5296770701786037,
      "grad_norm": 85.59898376464844,
      "learning_rate": 9.212753950338601e-05,
      "loss": 12.8563,
      "step": 734
    },
    {
      "epoch": 0.5303987010644056,
      "grad_norm": 76.92655181884766,
      "learning_rate": 9.198645598194132e-05,
      "loss": 11.3225,
      "step": 735
    },
    {
      "epoch": 0.5311203319502075,
      "grad_norm": 81.78773498535156,
      "learning_rate": 9.184537246049661e-05,
      "loss": 12.2874,
      "step": 736
    },
    {
      "epoch": 0.5318419628360094,
      "grad_norm": 73.43538665771484,
      "learning_rate": 9.170428893905191e-05,
      "loss": 11.9466,
      "step": 737
    },
    {
      "epoch": 0.5325635937218113,
      "grad_norm": 82.03372192382812,
      "learning_rate": 9.156320541760722e-05,
      "loss": 12.0359,
      "step": 738
    },
    {
      "epoch": 0.5332852246076132,
      "grad_norm": 66.5977554321289,
      "learning_rate": 9.142212189616254e-05,
      "loss": 11.9489,
      "step": 739
    },
    {
      "epoch": 0.5340068554934151,
      "grad_norm": 83.32608795166016,
      "learning_rate": 9.128103837471784e-05,
      "loss": 11.0669,
      "step": 740
    },
    {
      "epoch": 0.534728486379217,
      "grad_norm": 82.74241638183594,
      "learning_rate": 9.113995485327315e-05,
      "loss": 12.1338,
      "step": 741
    },
    {
      "epoch": 0.5354501172650189,
      "grad_norm": 77.36337280273438,
      "learning_rate": 9.099887133182845e-05,
      "loss": 12.7239,
      "step": 742
    },
    {
      "epoch": 0.5361717481508208,
      "grad_norm": 84.33265686035156,
      "learning_rate": 9.085778781038374e-05,
      "loss": 12.5061,
      "step": 743
    },
    {
      "epoch": 0.5368933790366228,
      "grad_norm": 85.17447662353516,
      "learning_rate": 9.071670428893905e-05,
      "loss": 12.2858,
      "step": 744
    },
    {
      "epoch": 0.5376150099224247,
      "grad_norm": 80.76443481445312,
      "learning_rate": 9.057562076749435e-05,
      "loss": 11.4865,
      "step": 745
    },
    {
      "epoch": 0.5383366408082266,
      "grad_norm": 71.80066680908203,
      "learning_rate": 9.043453724604966e-05,
      "loss": 12.4226,
      "step": 746
    },
    {
      "epoch": 0.5390582716940285,
      "grad_norm": 76.07630920410156,
      "learning_rate": 9.029345372460498e-05,
      "loss": 12.0504,
      "step": 747
    },
    {
      "epoch": 0.5397799025798304,
      "grad_norm": 75.23616790771484,
      "learning_rate": 9.015237020316028e-05,
      "loss": 12.0668,
      "step": 748
    },
    {
      "epoch": 0.5405015334656323,
      "grad_norm": 81.2934799194336,
      "learning_rate": 9.001128668171557e-05,
      "loss": 12.1677,
      "step": 749
    },
    {
      "epoch": 0.5412231643514342,
      "grad_norm": 75.79821014404297,
      "learning_rate": 8.987020316027088e-05,
      "loss": 11.5675,
      "step": 750
    },
    {
      "epoch": 0.5419447952372362,
      "grad_norm": 85.20758056640625,
      "learning_rate": 8.972911963882618e-05,
      "loss": 14.3858,
      "step": 751
    },
    {
      "epoch": 0.5426664261230381,
      "grad_norm": 87.80135345458984,
      "learning_rate": 8.958803611738149e-05,
      "loss": 12.7276,
      "step": 752
    },
    {
      "epoch": 0.54338805700884,
      "grad_norm": 82.89993286132812,
      "learning_rate": 8.94469525959368e-05,
      "loss": 12.5391,
      "step": 753
    },
    {
      "epoch": 0.5441096878946419,
      "grad_norm": 79.15113830566406,
      "learning_rate": 8.93058690744921e-05,
      "loss": 11.84,
      "step": 754
    },
    {
      "epoch": 0.5448313187804438,
      "grad_norm": 81.30281829833984,
      "learning_rate": 8.916478555304742e-05,
      "loss": 12.043,
      "step": 755
    },
    {
      "epoch": 0.5455529496662457,
      "grad_norm": 80.70503997802734,
      "learning_rate": 8.902370203160271e-05,
      "loss": 13.5151,
      "step": 756
    },
    {
      "epoch": 0.5462745805520476,
      "grad_norm": 80.85409545898438,
      "learning_rate": 8.888261851015801e-05,
      "loss": 12.2336,
      "step": 757
    },
    {
      "epoch": 0.5469962114378495,
      "grad_norm": 87.0315933227539,
      "learning_rate": 8.874153498871332e-05,
      "loss": 12.3113,
      "step": 758
    },
    {
      "epoch": 0.5477178423236515,
      "grad_norm": 81.88933563232422,
      "learning_rate": 8.860045146726862e-05,
      "loss": 11.352,
      "step": 759
    },
    {
      "epoch": 0.5484394732094534,
      "grad_norm": 74.77474975585938,
      "learning_rate": 8.845936794582393e-05,
      "loss": 11.3271,
      "step": 760
    },
    {
      "epoch": 0.5491611040952553,
      "grad_norm": 72.82158660888672,
      "learning_rate": 8.831828442437923e-05,
      "loss": 11.9432,
      "step": 761
    },
    {
      "epoch": 0.5498827349810572,
      "grad_norm": 77.33661651611328,
      "learning_rate": 8.817720090293454e-05,
      "loss": 11.5918,
      "step": 762
    },
    {
      "epoch": 0.5506043658668591,
      "grad_norm": 81.3504638671875,
      "learning_rate": 8.803611738148985e-05,
      "loss": 12.2004,
      "step": 763
    },
    {
      "epoch": 0.551325996752661,
      "grad_norm": 89.2766342163086,
      "learning_rate": 8.789503386004515e-05,
      "loss": 12.3355,
      "step": 764
    },
    {
      "epoch": 0.5520476276384629,
      "grad_norm": 70.77387237548828,
      "learning_rate": 8.775395033860046e-05,
      "loss": 12.2987,
      "step": 765
    },
    {
      "epoch": 0.5527692585242648,
      "grad_norm": 81.07890319824219,
      "learning_rate": 8.761286681715576e-05,
      "loss": 11.1024,
      "step": 766
    },
    {
      "epoch": 0.5534908894100667,
      "grad_norm": 70.28512573242188,
      "learning_rate": 8.747178329571107e-05,
      "loss": 10.9944,
      "step": 767
    },
    {
      "epoch": 0.5542125202958686,
      "grad_norm": 84.49079895019531,
      "learning_rate": 8.733069977426637e-05,
      "loss": 12.0436,
      "step": 768
    },
    {
      "epoch": 0.5549341511816706,
      "grad_norm": 82.7942886352539,
      "learning_rate": 8.718961625282166e-05,
      "loss": 11.5423,
      "step": 769
    },
    {
      "epoch": 0.5556557820674725,
      "grad_norm": 70.34004974365234,
      "learning_rate": 8.704853273137697e-05,
      "loss": 11.7918,
      "step": 770
    },
    {
      "epoch": 0.5563774129532744,
      "grad_norm": 95.01191711425781,
      "learning_rate": 8.690744920993229e-05,
      "loss": 12.2934,
      "step": 771
    },
    {
      "epoch": 0.5570990438390763,
      "grad_norm": 67.59103393554688,
      "learning_rate": 8.676636568848759e-05,
      "loss": 10.8389,
      "step": 772
    },
    {
      "epoch": 0.5578206747248782,
      "grad_norm": 77.90829467773438,
      "learning_rate": 8.66252821670429e-05,
      "loss": 11.9875,
      "step": 773
    },
    {
      "epoch": 0.5585423056106802,
      "grad_norm": 91.91004180908203,
      "learning_rate": 8.64841986455982e-05,
      "loss": 11.6112,
      "step": 774
    },
    {
      "epoch": 0.5592639364964821,
      "grad_norm": 73.54650115966797,
      "learning_rate": 8.63431151241535e-05,
      "loss": 12.0817,
      "step": 775
    },
    {
      "epoch": 0.559985567382284,
      "grad_norm": 81.52980041503906,
      "learning_rate": 8.62020316027088e-05,
      "loss": 12.1133,
      "step": 776
    },
    {
      "epoch": 0.5607071982680859,
      "grad_norm": 87.27641296386719,
      "learning_rate": 8.60609480812641e-05,
      "loss": 10.9945,
      "step": 777
    },
    {
      "epoch": 0.5614288291538878,
      "grad_norm": 79.8983383178711,
      "learning_rate": 8.591986455981941e-05,
      "loss": 12.1865,
      "step": 778
    },
    {
      "epoch": 0.5621504600396897,
      "grad_norm": 79.45137023925781,
      "learning_rate": 8.577878103837473e-05,
      "loss": 11.5486,
      "step": 779
    },
    {
      "epoch": 0.5628720909254916,
      "grad_norm": 79.7534408569336,
      "learning_rate": 8.563769751693003e-05,
      "loss": 11.6999,
      "step": 780
    },
    {
      "epoch": 0.5635937218112935,
      "grad_norm": 72.2173080444336,
      "learning_rate": 8.549661399548534e-05,
      "loss": 11.708,
      "step": 781
    },
    {
      "epoch": 0.5643153526970954,
      "grad_norm": 69.35906219482422,
      "learning_rate": 8.535553047404064e-05,
      "loss": 11.0943,
      "step": 782
    },
    {
      "epoch": 0.5650369835828973,
      "grad_norm": 83.12548828125,
      "learning_rate": 8.521444695259593e-05,
      "loss": 11.8151,
      "step": 783
    },
    {
      "epoch": 0.5657586144686992,
      "grad_norm": 85.69876861572266,
      "learning_rate": 8.507336343115124e-05,
      "loss": 12.3066,
      "step": 784
    },
    {
      "epoch": 0.5664802453545011,
      "grad_norm": 73.11508178710938,
      "learning_rate": 8.493227990970654e-05,
      "loss": 11.3947,
      "step": 785
    },
    {
      "epoch": 0.567201876240303,
      "grad_norm": 76.81694030761719,
      "learning_rate": 8.479119638826185e-05,
      "loss": 12.1896,
      "step": 786
    },
    {
      "epoch": 0.567923507126105,
      "grad_norm": 87.4126205444336,
      "learning_rate": 8.465011286681717e-05,
      "loss": 12.8334,
      "step": 787
    },
    {
      "epoch": 0.5686451380119069,
      "grad_norm": 81.67354583740234,
      "learning_rate": 8.450902934537247e-05,
      "loss": 13.3174,
      "step": 788
    },
    {
      "epoch": 0.5693667688977089,
      "grad_norm": 70.49906921386719,
      "learning_rate": 8.436794582392776e-05,
      "loss": 11.3675,
      "step": 789
    },
    {
      "epoch": 0.5700883997835108,
      "grad_norm": 80.54932403564453,
      "learning_rate": 8.422686230248307e-05,
      "loss": 11.6555,
      "step": 790
    },
    {
      "epoch": 0.5708100306693127,
      "grad_norm": 76.172607421875,
      "learning_rate": 8.408577878103837e-05,
      "loss": 11.9371,
      "step": 791
    },
    {
      "epoch": 0.5715316615551146,
      "grad_norm": 78.67280578613281,
      "learning_rate": 8.394469525959368e-05,
      "loss": 11.5491,
      "step": 792
    },
    {
      "epoch": 0.5722532924409165,
      "grad_norm": 85.82328033447266,
      "learning_rate": 8.380361173814898e-05,
      "loss": 13.4935,
      "step": 793
    },
    {
      "epoch": 0.5729749233267184,
      "grad_norm": 72.15003967285156,
      "learning_rate": 8.366252821670429e-05,
      "loss": 11.6957,
      "step": 794
    },
    {
      "epoch": 0.5736965542125203,
      "grad_norm": 86.57063293457031,
      "learning_rate": 8.352144469525961e-05,
      "loss": 12.1526,
      "step": 795
    },
    {
      "epoch": 0.5744181850983222,
      "grad_norm": 72.18170166015625,
      "learning_rate": 8.33803611738149e-05,
      "loss": 12.0557,
      "step": 796
    },
    {
      "epoch": 0.5751398159841241,
      "grad_norm": 74.61033630371094,
      "learning_rate": 8.32392776523702e-05,
      "loss": 12.5032,
      "step": 797
    },
    {
      "epoch": 0.575861446869926,
      "grad_norm": 67.89912414550781,
      "learning_rate": 8.309819413092551e-05,
      "loss": 10.1737,
      "step": 798
    },
    {
      "epoch": 0.5765830777557279,
      "grad_norm": 91.95667266845703,
      "learning_rate": 8.295711060948082e-05,
      "loss": 12.305,
      "step": 799
    },
    {
      "epoch": 0.5773047086415298,
      "grad_norm": 77.76016235351562,
      "learning_rate": 8.281602708803612e-05,
      "loss": 11.5103,
      "step": 800
    },
    {
      "epoch": 0.5780263395273317,
      "grad_norm": 70.80819702148438,
      "learning_rate": 8.267494356659143e-05,
      "loss": 11.0153,
      "step": 801
    },
    {
      "epoch": 0.5787479704131336,
      "grad_norm": 78.53059387207031,
      "learning_rate": 8.253386004514672e-05,
      "loss": 12.0402,
      "step": 802
    },
    {
      "epoch": 0.5794696012989355,
      "grad_norm": 89.15531158447266,
      "learning_rate": 8.239277652370202e-05,
      "loss": 11.1995,
      "step": 803
    },
    {
      "epoch": 0.5801912321847376,
      "grad_norm": 73.1449966430664,
      "learning_rate": 8.225169300225734e-05,
      "loss": 10.9919,
      "step": 804
    },
    {
      "epoch": 0.5809128630705395,
      "grad_norm": 64.0901107788086,
      "learning_rate": 8.211060948081265e-05,
      "loss": 11.1833,
      "step": 805
    },
    {
      "epoch": 0.5816344939563414,
      "grad_norm": 80.32768249511719,
      "learning_rate": 8.196952595936795e-05,
      "loss": 11.9222,
      "step": 806
    },
    {
      "epoch": 0.5823561248421433,
      "grad_norm": 83.13113403320312,
      "learning_rate": 8.182844243792326e-05,
      "loss": 12.2236,
      "step": 807
    },
    {
      "epoch": 0.5830777557279452,
      "grad_norm": 66.86351013183594,
      "learning_rate": 8.168735891647856e-05,
      "loss": 11.8212,
      "step": 808
    },
    {
      "epoch": 0.5837993866137471,
      "grad_norm": 67.58548736572266,
      "learning_rate": 8.154627539503385e-05,
      "loss": 11.5728,
      "step": 809
    },
    {
      "epoch": 0.584521017499549,
      "grad_norm": 72.36335754394531,
      "learning_rate": 8.140519187358916e-05,
      "loss": 11.6266,
      "step": 810
    },
    {
      "epoch": 0.5852426483853509,
      "grad_norm": 83.09652709960938,
      "learning_rate": 8.126410835214446e-05,
      "loss": 10.8935,
      "step": 811
    },
    {
      "epoch": 0.5859642792711528,
      "grad_norm": 73.96922302246094,
      "learning_rate": 8.112302483069978e-05,
      "loss": 11.1249,
      "step": 812
    },
    {
      "epoch": 0.5866859101569547,
      "grad_norm": 81.00237274169922,
      "learning_rate": 8.098194130925509e-05,
      "loss": 11.8511,
      "step": 813
    },
    {
      "epoch": 0.5874075410427566,
      "grad_norm": 67.88966369628906,
      "learning_rate": 8.084085778781039e-05,
      "loss": 10.9318,
      "step": 814
    },
    {
      "epoch": 0.5881291719285585,
      "grad_norm": 74.3742904663086,
      "learning_rate": 8.06997742663657e-05,
      "loss": 11.7318,
      "step": 815
    },
    {
      "epoch": 0.5888508028143604,
      "grad_norm": 73.14603424072266,
      "learning_rate": 8.055869074492099e-05,
      "loss": 11.0204,
      "step": 816
    },
    {
      "epoch": 0.5895724337001623,
      "grad_norm": 76.49807739257812,
      "learning_rate": 8.04176072234763e-05,
      "loss": 12.8379,
      "step": 817
    },
    {
      "epoch": 0.5902940645859642,
      "grad_norm": 83.25508880615234,
      "learning_rate": 8.02765237020316e-05,
      "loss": 10.8026,
      "step": 818
    },
    {
      "epoch": 0.5910156954717662,
      "grad_norm": 70.82929229736328,
      "learning_rate": 8.01354401805869e-05,
      "loss": 12.3716,
      "step": 819
    },
    {
      "epoch": 0.5917373263575681,
      "grad_norm": 85.3388442993164,
      "learning_rate": 7.999435665914222e-05,
      "loss": 12.9579,
      "step": 820
    },
    {
      "epoch": 0.59245895724337,
      "grad_norm": 73.14507293701172,
      "learning_rate": 7.985327313769753e-05,
      "loss": 10.2375,
      "step": 821
    },
    {
      "epoch": 0.593180588129172,
      "grad_norm": 67.66259002685547,
      "learning_rate": 7.971218961625282e-05,
      "loss": 11.1189,
      "step": 822
    },
    {
      "epoch": 0.5939022190149739,
      "grad_norm": 72.152587890625,
      "learning_rate": 7.957110609480812e-05,
      "loss": 12.1798,
      "step": 823
    },
    {
      "epoch": 0.5946238499007758,
      "grad_norm": 72.51805114746094,
      "learning_rate": 7.943002257336343e-05,
      "loss": 11.3158,
      "step": 824
    },
    {
      "epoch": 0.5953454807865777,
      "grad_norm": 80.2782974243164,
      "learning_rate": 7.928893905191873e-05,
      "loss": 11.2775,
      "step": 825
    },
    {
      "epoch": 0.5960671116723796,
      "grad_norm": 73.48298645019531,
      "learning_rate": 7.914785553047404e-05,
      "loss": 11.4863,
      "step": 826
    },
    {
      "epoch": 0.5967887425581815,
      "grad_norm": 75.41964721679688,
      "learning_rate": 7.900677200902934e-05,
      "loss": 11.9047,
      "step": 827
    },
    {
      "epoch": 0.5975103734439834,
      "grad_norm": 77.99907684326172,
      "learning_rate": 7.886568848758466e-05,
      "loss": 10.6715,
      "step": 828
    },
    {
      "epoch": 0.5982320043297853,
      "grad_norm": 84.95028686523438,
      "learning_rate": 7.872460496613995e-05,
      "loss": 11.7033,
      "step": 829
    },
    {
      "epoch": 0.5989536352155872,
      "grad_norm": 73.31197357177734,
      "learning_rate": 7.858352144469526e-05,
      "loss": 11.3869,
      "step": 830
    },
    {
      "epoch": 0.5996752661013891,
      "grad_norm": 83.4442367553711,
      "learning_rate": 7.844243792325057e-05,
      "loss": 11.8607,
      "step": 831
    },
    {
      "epoch": 0.600396896987191,
      "grad_norm": 86.47467041015625,
      "learning_rate": 7.830135440180587e-05,
      "loss": 11.598,
      "step": 832
    },
    {
      "epoch": 0.6011185278729929,
      "grad_norm": 72.16179656982422,
      "learning_rate": 7.816027088036118e-05,
      "loss": 11.3896,
      "step": 833
    },
    {
      "epoch": 0.6018401587587949,
      "grad_norm": 83.80718231201172,
      "learning_rate": 7.801918735891648e-05,
      "loss": 12.2526,
      "step": 834
    },
    {
      "epoch": 0.6025617896445968,
      "grad_norm": 79.03260040283203,
      "learning_rate": 7.787810383747177e-05,
      "loss": 11.027,
      "step": 835
    },
    {
      "epoch": 0.6032834205303987,
      "grad_norm": 86.97102355957031,
      "learning_rate": 7.773702031602709e-05,
      "loss": 11.5927,
      "step": 836
    },
    {
      "epoch": 0.6040050514162006,
      "grad_norm": 92.98417663574219,
      "learning_rate": 7.75959367945824e-05,
      "loss": 12.277,
      "step": 837
    },
    {
      "epoch": 0.6047266823020025,
      "grad_norm": 73.31519317626953,
      "learning_rate": 7.74548532731377e-05,
      "loss": 12.3556,
      "step": 838
    },
    {
      "epoch": 0.6054483131878045,
      "grad_norm": 84.1241455078125,
      "learning_rate": 7.7313769751693e-05,
      "loss": 12.3708,
      "step": 839
    },
    {
      "epoch": 0.6061699440736064,
      "grad_norm": 78.87373352050781,
      "learning_rate": 7.717268623024831e-05,
      "loss": 11.9173,
      "step": 840
    },
    {
      "epoch": 0.6068915749594083,
      "grad_norm": 86.33120727539062,
      "learning_rate": 7.703160270880362e-05,
      "loss": 11.9683,
      "step": 841
    },
    {
      "epoch": 0.6076132058452102,
      "grad_norm": 82.92112731933594,
      "learning_rate": 7.689051918735891e-05,
      "loss": 11.5992,
      "step": 842
    },
    {
      "epoch": 0.6083348367310121,
      "grad_norm": 90.29188537597656,
      "learning_rate": 7.674943566591421e-05,
      "loss": 12.6335,
      "step": 843
    },
    {
      "epoch": 0.609056467616814,
      "grad_norm": 67.01173400878906,
      "learning_rate": 7.660835214446953e-05,
      "loss": 12.6958,
      "step": 844
    },
    {
      "epoch": 0.6097780985026159,
      "grad_norm": 81.06298065185547,
      "learning_rate": 7.646726862302484e-05,
      "loss": 13.3507,
      "step": 845
    },
    {
      "epoch": 0.6104997293884178,
      "grad_norm": 76.68209075927734,
      "learning_rate": 7.632618510158014e-05,
      "loss": 12.0851,
      "step": 846
    },
    {
      "epoch": 0.6112213602742197,
      "grad_norm": 73.05549621582031,
      "learning_rate": 7.618510158013545e-05,
      "loss": 12.1431,
      "step": 847
    },
    {
      "epoch": 0.6119429911600216,
      "grad_norm": 78.19994354248047,
      "learning_rate": 7.604401805869075e-05,
      "loss": 12.6175,
      "step": 848
    },
    {
      "epoch": 0.6126646220458236,
      "grad_norm": 66.14713287353516,
      "learning_rate": 7.590293453724604e-05,
      "loss": 11.1415,
      "step": 849
    },
    {
      "epoch": 0.6133862529316255,
      "grad_norm": 75.91639709472656,
      "learning_rate": 7.576185101580135e-05,
      "loss": 10.6691,
      "step": 850
    },
    {
      "epoch": 0.6141078838174274,
      "grad_norm": 80.78117370605469,
      "learning_rate": 7.562076749435665e-05,
      "loss": 11.2626,
      "step": 851
    },
    {
      "epoch": 0.6148295147032293,
      "grad_norm": 69.92677307128906,
      "learning_rate": 7.547968397291197e-05,
      "loss": 11.1851,
      "step": 852
    },
    {
      "epoch": 0.6155511455890312,
      "grad_norm": 72.64228057861328,
      "learning_rate": 7.533860045146728e-05,
      "loss": 11.3951,
      "step": 853
    },
    {
      "epoch": 0.6162727764748331,
      "grad_norm": 74.11946868896484,
      "learning_rate": 7.519751693002258e-05,
      "loss": 10.9838,
      "step": 854
    },
    {
      "epoch": 0.616994407360635,
      "grad_norm": 76.43141174316406,
      "learning_rate": 7.505643340857787e-05,
      "loss": 11.74,
      "step": 855
    },
    {
      "epoch": 0.617716038246437,
      "grad_norm": 83.75775909423828,
      "learning_rate": 7.491534988713318e-05,
      "loss": 10.1769,
      "step": 856
    },
    {
      "epoch": 0.6184376691322389,
      "grad_norm": 78.8893814086914,
      "learning_rate": 7.477426636568848e-05,
      "loss": 11.6222,
      "step": 857
    },
    {
      "epoch": 0.6191593000180408,
      "grad_norm": 84.0315170288086,
      "learning_rate": 7.463318284424379e-05,
      "loss": 11.2183,
      "step": 858
    },
    {
      "epoch": 0.6198809309038427,
      "grad_norm": 68.76261138916016,
      "learning_rate": 7.44920993227991e-05,
      "loss": 11.0483,
      "step": 859
    },
    {
      "epoch": 0.6206025617896446,
      "grad_norm": 96.69922637939453,
      "learning_rate": 7.435101580135441e-05,
      "loss": 10.8521,
      "step": 860
    },
    {
      "epoch": 0.6213241926754465,
      "grad_norm": 78.99853515625,
      "learning_rate": 7.420993227990972e-05,
      "loss": 11.0567,
      "step": 861
    },
    {
      "epoch": 0.6220458235612484,
      "grad_norm": 71.79222106933594,
      "learning_rate": 7.406884875846501e-05,
      "loss": 11.2182,
      "step": 862
    },
    {
      "epoch": 0.6227674544470503,
      "grad_norm": 94.8959732055664,
      "learning_rate": 7.392776523702031e-05,
      "loss": 12.0694,
      "step": 863
    },
    {
      "epoch": 0.6234890853328523,
      "grad_norm": 75.06117248535156,
      "learning_rate": 7.378668171557562e-05,
      "loss": 10.8091,
      "step": 864
    },
    {
      "epoch": 0.6242107162186542,
      "grad_norm": 83.72990417480469,
      "learning_rate": 7.364559819413093e-05,
      "loss": 10.5238,
      "step": 865
    },
    {
      "epoch": 0.6249323471044561,
      "grad_norm": 81.54924774169922,
      "learning_rate": 7.350451467268623e-05,
      "loss": 12.3268,
      "step": 866
    },
    {
      "epoch": 0.625653977990258,
      "grad_norm": 72.85284423828125,
      "learning_rate": 7.336343115124154e-05,
      "loss": 11.1034,
      "step": 867
    },
    {
      "epoch": 0.6263756088760599,
      "grad_norm": 86.04000091552734,
      "learning_rate": 7.322234762979684e-05,
      "loss": 11.5752,
      "step": 868
    },
    {
      "epoch": 0.6270972397618618,
      "grad_norm": 73.80105590820312,
      "learning_rate": 7.308126410835215e-05,
      "loss": 11.3813,
      "step": 869
    },
    {
      "epoch": 0.6278188706476637,
      "grad_norm": 76.00704956054688,
      "learning_rate": 7.294018058690745e-05,
      "loss": 11.3988,
      "step": 870
    },
    {
      "epoch": 0.6285405015334656,
      "grad_norm": 73.23538970947266,
      "learning_rate": 7.279909706546276e-05,
      "loss": 10.9043,
      "step": 871
    },
    {
      "epoch": 0.6292621324192675,
      "grad_norm": 100.14957427978516,
      "learning_rate": 7.265801354401806e-05,
      "loss": 12.1114,
      "step": 872
    },
    {
      "epoch": 0.6299837633050694,
      "grad_norm": 95.7875747680664,
      "learning_rate": 7.251693002257337e-05,
      "loss": 11.5552,
      "step": 873
    },
    {
      "epoch": 0.6307053941908713,
      "grad_norm": 71.71329498291016,
      "learning_rate": 7.237584650112867e-05,
      "loss": 11.2011,
      "step": 874
    },
    {
      "epoch": 0.6314270250766733,
      "grad_norm": 64.92316436767578,
      "learning_rate": 7.223476297968396e-05,
      "loss": 10.1735,
      "step": 875
    },
    {
      "epoch": 0.6321486559624752,
      "grad_norm": 74.07572174072266,
      "learning_rate": 7.209367945823928e-05,
      "loss": 11.3794,
      "step": 876
    },
    {
      "epoch": 0.6328702868482771,
      "grad_norm": 85.9410400390625,
      "learning_rate": 7.195259593679459e-05,
      "loss": 11.1827,
      "step": 877
    },
    {
      "epoch": 0.633591917734079,
      "grad_norm": 84.07064819335938,
      "learning_rate": 7.181151241534989e-05,
      "loss": 11.109,
      "step": 878
    },
    {
      "epoch": 0.634313548619881,
      "grad_norm": 75.50390625,
      "learning_rate": 7.16704288939052e-05,
      "loss": 10.8477,
      "step": 879
    },
    {
      "epoch": 0.6350351795056829,
      "grad_norm": 81.3931884765625,
      "learning_rate": 7.15293453724605e-05,
      "loss": 11.8384,
      "step": 880
    },
    {
      "epoch": 0.6357568103914848,
      "grad_norm": 90.30779266357422,
      "learning_rate": 7.138826185101581e-05,
      "loss": 11.6314,
      "step": 881
    },
    {
      "epoch": 0.6364784412772867,
      "grad_norm": 97.50724792480469,
      "learning_rate": 7.12471783295711e-05,
      "loss": 12.5955,
      "step": 882
    },
    {
      "epoch": 0.6372000721630886,
      "grad_norm": 74.0204086303711,
      "learning_rate": 7.11060948081264e-05,
      "loss": 10.8563,
      "step": 883
    },
    {
      "epoch": 0.6379217030488905,
      "grad_norm": 68.46395111083984,
      "learning_rate": 7.096501128668172e-05,
      "loss": 11.7575,
      "step": 884
    },
    {
      "epoch": 0.6386433339346924,
      "grad_norm": 80.86180114746094,
      "learning_rate": 7.082392776523703e-05,
      "loss": 10.89,
      "step": 885
    },
    {
      "epoch": 0.6393649648204943,
      "grad_norm": 87.23809051513672,
      "learning_rate": 7.068284424379233e-05,
      "loss": 10.2636,
      "step": 886
    },
    {
      "epoch": 0.6400865957062962,
      "grad_norm": 72.46028900146484,
      "learning_rate": 7.054176072234764e-05,
      "loss": 11.2072,
      "step": 887
    },
    {
      "epoch": 0.6408082265920981,
      "grad_norm": 71.97684478759766,
      "learning_rate": 7.040067720090293e-05,
      "loss": 12.803,
      "step": 888
    },
    {
      "epoch": 0.6415298574779,
      "grad_norm": 73.48847961425781,
      "learning_rate": 7.025959367945823e-05,
      "loss": 12.0004,
      "step": 889
    },
    {
      "epoch": 0.6422514883637019,
      "grad_norm": 91.62409973144531,
      "learning_rate": 7.011851015801354e-05,
      "loss": 11.2781,
      "step": 890
    },
    {
      "epoch": 0.6429731192495038,
      "grad_norm": 85.52568817138672,
      "learning_rate": 6.997742663656884e-05,
      "loss": 10.9711,
      "step": 891
    },
    {
      "epoch": 0.6436947501353057,
      "grad_norm": 76.95989990234375,
      "learning_rate": 6.983634311512416e-05,
      "loss": 12.1396,
      "step": 892
    },
    {
      "epoch": 0.6444163810211077,
      "grad_norm": 78.85674285888672,
      "learning_rate": 6.969525959367947e-05,
      "loss": 11.7718,
      "step": 893
    },
    {
      "epoch": 0.6451380119069097,
      "grad_norm": 80.66578674316406,
      "learning_rate": 6.955417607223477e-05,
      "loss": 11.6889,
      "step": 894
    },
    {
      "epoch": 0.6458596427927116,
      "grad_norm": 76.3918228149414,
      "learning_rate": 6.941309255079006e-05,
      "loss": 11.4968,
      "step": 895
    },
    {
      "epoch": 0.6465812736785135,
      "grad_norm": 69.34585571289062,
      "learning_rate": 6.927200902934537e-05,
      "loss": 11.2774,
      "step": 896
    },
    {
      "epoch": 0.6473029045643154,
      "grad_norm": 93.98994445800781,
      "learning_rate": 6.913092550790067e-05,
      "loss": 11.2752,
      "step": 897
    },
    {
      "epoch": 0.6480245354501173,
      "grad_norm": 80.29806518554688,
      "learning_rate": 6.898984198645598e-05,
      "loss": 11.424,
      "step": 898
    },
    {
      "epoch": 0.6487461663359192,
      "grad_norm": 74.88028717041016,
      "learning_rate": 6.884875846501129e-05,
      "loss": 10.879,
      "step": 899
    },
    {
      "epoch": 0.6494677972217211,
      "grad_norm": 72.43238067626953,
      "learning_rate": 6.87076749435666e-05,
      "loss": 10.8901,
      "step": 900
    },
    {
      "epoch": 0.650189428107523,
      "grad_norm": 68.75379180908203,
      "learning_rate": 6.856659142212191e-05,
      "loss": 10.6694,
      "step": 901
    },
    {
      "epoch": 0.6509110589933249,
      "grad_norm": 72.23468780517578,
      "learning_rate": 6.84255079006772e-05,
      "loss": 11.4161,
      "step": 902
    },
    {
      "epoch": 0.6516326898791268,
      "grad_norm": 83.67127227783203,
      "learning_rate": 6.82844243792325e-05,
      "loss": 10.22,
      "step": 903
    },
    {
      "epoch": 0.6523543207649287,
      "grad_norm": 81.6135025024414,
      "learning_rate": 6.814334085778781e-05,
      "loss": 10.5282,
      "step": 904
    },
    {
      "epoch": 0.6530759516507306,
      "grad_norm": 68.36775970458984,
      "learning_rate": 6.800225733634312e-05,
      "loss": 10.5914,
      "step": 905
    },
    {
      "epoch": 0.6537975825365325,
      "grad_norm": 64.84132385253906,
      "learning_rate": 6.786117381489842e-05,
      "loss": 10.664,
      "step": 906
    },
    {
      "epoch": 0.6545192134223344,
      "grad_norm": 75.02425384521484,
      "learning_rate": 6.772009029345373e-05,
      "loss": 11.2626,
      "step": 907
    },
    {
      "epoch": 0.6552408443081363,
      "grad_norm": 85.33810424804688,
      "learning_rate": 6.757900677200903e-05,
      "loss": 11.4774,
      "step": 908
    },
    {
      "epoch": 0.6559624751939384,
      "grad_norm": 77.10227966308594,
      "learning_rate": 6.743792325056434e-05,
      "loss": 11.0243,
      "step": 909
    },
    {
      "epoch": 0.6566841060797403,
      "grad_norm": 70.3681411743164,
      "learning_rate": 6.729683972911964e-05,
      "loss": 10.5299,
      "step": 910
    },
    {
      "epoch": 0.6574057369655422,
      "grad_norm": 69.568359375,
      "learning_rate": 6.715575620767495e-05,
      "loss": 12.3929,
      "step": 911
    },
    {
      "epoch": 0.6581273678513441,
      "grad_norm": 74.51453399658203,
      "learning_rate": 6.701467268623025e-05,
      "loss": 10.185,
      "step": 912
    },
    {
      "epoch": 0.658848998737146,
      "grad_norm": 69.10478973388672,
      "learning_rate": 6.687358916478556e-05,
      "loss": 11.4193,
      "step": 913
    },
    {
      "epoch": 0.6595706296229479,
      "grad_norm": 78.40303039550781,
      "learning_rate": 6.673250564334086e-05,
      "loss": 11.0312,
      "step": 914
    },
    {
      "epoch": 0.6602922605087498,
      "grad_norm": 77.45523834228516,
      "learning_rate": 6.659142212189615e-05,
      "loss": 11.9289,
      "step": 915
    },
    {
      "epoch": 0.6610138913945517,
      "grad_norm": 65.45199584960938,
      "learning_rate": 6.645033860045147e-05,
      "loss": 11.4672,
      "step": 916
    },
    {
      "epoch": 0.6617355222803536,
      "grad_norm": 72.79638671875,
      "learning_rate": 6.630925507900678e-05,
      "loss": 11.3435,
      "step": 917
    },
    {
      "epoch": 0.6624571531661555,
      "grad_norm": 77.92652130126953,
      "learning_rate": 6.616817155756208e-05,
      "loss": 10.9392,
      "step": 918
    },
    {
      "epoch": 0.6631787840519574,
      "grad_norm": 72.4222183227539,
      "learning_rate": 6.602708803611739e-05,
      "loss": 11.3068,
      "step": 919
    },
    {
      "epoch": 0.6639004149377593,
      "grad_norm": 73.09516143798828,
      "learning_rate": 6.588600451467269e-05,
      "loss": 11.6863,
      "step": 920
    },
    {
      "epoch": 0.6646220458235612,
      "grad_norm": 67.65135192871094,
      "learning_rate": 6.574492099322798e-05,
      "loss": 10.6104,
      "step": 921
    },
    {
      "epoch": 0.6653436767093631,
      "grad_norm": 70.94681549072266,
      "learning_rate": 6.560383747178329e-05,
      "loss": 10.2494,
      "step": 922
    },
    {
      "epoch": 0.666065307595165,
      "grad_norm": 67.74748992919922,
      "learning_rate": 6.54627539503386e-05,
      "loss": 11.5983,
      "step": 923
    },
    {
      "epoch": 0.666786938480967,
      "grad_norm": 69.10647583007812,
      "learning_rate": 6.532167042889391e-05,
      "loss": 11.1407,
      "step": 924
    },
    {
      "epoch": 0.667508569366769,
      "grad_norm": 74.4486083984375,
      "learning_rate": 6.518058690744922e-05,
      "loss": 10.1736,
      "step": 925
    },
    {
      "epoch": 0.6682302002525708,
      "grad_norm": 71.90455627441406,
      "learning_rate": 6.503950338600452e-05,
      "loss": 10.9068,
      "step": 926
    },
    {
      "epoch": 0.6689518311383728,
      "grad_norm": 78.623779296875,
      "learning_rate": 6.489841986455983e-05,
      "loss": 12.0213,
      "step": 927
    },
    {
      "epoch": 0.6696734620241747,
      "grad_norm": 72.70069122314453,
      "learning_rate": 6.475733634311512e-05,
      "loss": 11.47,
      "step": 928
    },
    {
      "epoch": 0.6703950929099766,
      "grad_norm": 71.3066177368164,
      "learning_rate": 6.461625282167042e-05,
      "loss": 11.0194,
      "step": 929
    },
    {
      "epoch": 0.6711167237957785,
      "grad_norm": 79.84371185302734,
      "learning_rate": 6.447516930022573e-05,
      "loss": 11.1303,
      "step": 930
    },
    {
      "epoch": 0.6718383546815804,
      "grad_norm": 88.22190856933594,
      "learning_rate": 6.433408577878103e-05,
      "loss": 11.6477,
      "step": 931
    },
    {
      "epoch": 0.6725599855673823,
      "grad_norm": 87.39871978759766,
      "learning_rate": 6.419300225733635e-05,
      "loss": 12.1746,
      "step": 932
    },
    {
      "epoch": 0.6732816164531842,
      "grad_norm": 76.9666976928711,
      "learning_rate": 6.405191873589166e-05,
      "loss": 11.347,
      "step": 933
    },
    {
      "epoch": 0.6740032473389861,
      "grad_norm": 70.67206573486328,
      "learning_rate": 6.391083521444696e-05,
      "loss": 11.3429,
      "step": 934
    },
    {
      "epoch": 0.674724878224788,
      "grad_norm": 74.2431640625,
      "learning_rate": 6.376975169300226e-05,
      "loss": 11.2544,
      "step": 935
    },
    {
      "epoch": 0.6754465091105899,
      "grad_norm": 81.38719177246094,
      "learning_rate": 6.362866817155756e-05,
      "loss": 11.1582,
      "step": 936
    },
    {
      "epoch": 0.6761681399963918,
      "grad_norm": 77.66092681884766,
      "learning_rate": 6.348758465011287e-05,
      "loss": 11.9462,
      "step": 937
    },
    {
      "epoch": 0.6768897708821937,
      "grad_norm": 77.50480651855469,
      "learning_rate": 6.334650112866817e-05,
      "loss": 11.6537,
      "step": 938
    },
    {
      "epoch": 0.6776114017679957,
      "grad_norm": 85.34708404541016,
      "learning_rate": 6.320541760722348e-05,
      "loss": 11.3309,
      "step": 939
    },
    {
      "epoch": 0.6783330326537976,
      "grad_norm": 95.79105377197266,
      "learning_rate": 6.30643340857788e-05,
      "loss": 10.8991,
      "step": 940
    },
    {
      "epoch": 0.6790546635395995,
      "grad_norm": 86.54027557373047,
      "learning_rate": 6.292325056433409e-05,
      "loss": 11.5587,
      "step": 941
    },
    {
      "epoch": 0.6797762944254014,
      "grad_norm": 73.36768341064453,
      "learning_rate": 6.278216704288939e-05,
      "loss": 10.6823,
      "step": 942
    },
    {
      "epoch": 0.6804979253112033,
      "grad_norm": 71.07283782958984,
      "learning_rate": 6.26410835214447e-05,
      "loss": 12.5326,
      "step": 943
    },
    {
      "epoch": 0.6812195561970052,
      "grad_norm": 72.5545654296875,
      "learning_rate": 6.25e-05,
      "loss": 11.4963,
      "step": 944
    },
    {
      "epoch": 0.6819411870828072,
      "grad_norm": 91.2335433959961,
      "learning_rate": 6.23589164785553e-05,
      "loss": 11.523,
      "step": 945
    },
    {
      "epoch": 0.6826628179686091,
      "grad_norm": 72.91858673095703,
      "learning_rate": 6.221783295711061e-05,
      "loss": 11.5355,
      "step": 946
    },
    {
      "epoch": 0.683384448854411,
      "grad_norm": 71.83000946044922,
      "learning_rate": 6.207674943566592e-05,
      "loss": 10.8558,
      "step": 947
    },
    {
      "epoch": 0.6841060797402129,
      "grad_norm": 68.3106918334961,
      "learning_rate": 6.193566591422122e-05,
      "loss": 9.3689,
      "step": 948
    },
    {
      "epoch": 0.6848277106260148,
      "grad_norm": 78.76461791992188,
      "learning_rate": 6.179458239277653e-05,
      "loss": 10.756,
      "step": 949
    },
    {
      "epoch": 0.6855493415118167,
      "grad_norm": 75.54198455810547,
      "learning_rate": 6.165349887133183e-05,
      "loss": 10.2766,
      "step": 950
    },
    {
      "epoch": 0.6862709723976186,
      "grad_norm": 74.09336853027344,
      "learning_rate": 6.151241534988714e-05,
      "loss": 11.4369,
      "step": 951
    },
    {
      "epoch": 0.6869926032834205,
      "grad_norm": 66.09758758544922,
      "learning_rate": 6.137133182844244e-05,
      "loss": 11.8413,
      "step": 952
    },
    {
      "epoch": 0.6877142341692224,
      "grad_norm": 86.4636001586914,
      "learning_rate": 6.123024830699775e-05,
      "loss": 11.845,
      "step": 953
    },
    {
      "epoch": 0.6884358650550244,
      "grad_norm": 71.55049896240234,
      "learning_rate": 6.108916478555305e-05,
      "loss": 12.3487,
      "step": 954
    },
    {
      "epoch": 0.6891574959408263,
      "grad_norm": 77.52100372314453,
      "learning_rate": 6.094808126410836e-05,
      "loss": 11.9511,
      "step": 955
    },
    {
      "epoch": 0.6898791268266282,
      "grad_norm": 61.093780517578125,
      "learning_rate": 6.0806997742663656e-05,
      "loss": 11.2385,
      "step": 956
    },
    {
      "epoch": 0.6906007577124301,
      "grad_norm": 69.688232421875,
      "learning_rate": 6.066591422121896e-05,
      "loss": 12.395,
      "step": 957
    },
    {
      "epoch": 0.691322388598232,
      "grad_norm": 81.45073699951172,
      "learning_rate": 6.0524830699774266e-05,
      "loss": 11.3992,
      "step": 958
    },
    {
      "epoch": 0.6920440194840339,
      "grad_norm": 72.67398834228516,
      "learning_rate": 6.038374717832957e-05,
      "loss": 11.1264,
      "step": 959
    },
    {
      "epoch": 0.6927656503698358,
      "grad_norm": 79.49584197998047,
      "learning_rate": 6.0242663656884876e-05,
      "loss": 11.158,
      "step": 960
    },
    {
      "epoch": 0.6934872812556377,
      "grad_norm": 74.16114044189453,
      "learning_rate": 6.010158013544018e-05,
      "loss": 12.1028,
      "step": 961
    },
    {
      "epoch": 0.6942089121414396,
      "grad_norm": 72.96704864501953,
      "learning_rate": 5.9960496613995486e-05,
      "loss": 11.8828,
      "step": 962
    },
    {
      "epoch": 0.6949305430272416,
      "grad_norm": 70.8276596069336,
      "learning_rate": 5.981941309255079e-05,
      "loss": 10.2951,
      "step": 963
    },
    {
      "epoch": 0.6956521739130435,
      "grad_norm": 90.036376953125,
      "learning_rate": 5.96783295711061e-05,
      "loss": 12.4509,
      "step": 964
    },
    {
      "epoch": 0.6963738047988454,
      "grad_norm": 72.85834503173828,
      "learning_rate": 5.95372460496614e-05,
      "loss": 10.7641,
      "step": 965
    },
    {
      "epoch": 0.6970954356846473,
      "grad_norm": 72.30858612060547,
      "learning_rate": 5.93961625282167e-05,
      "loss": 10.8402,
      "step": 966
    },
    {
      "epoch": 0.6978170665704492,
      "grad_norm": 68.70833587646484,
      "learning_rate": 5.925507900677201e-05,
      "loss": 10.6422,
      "step": 967
    },
    {
      "epoch": 0.6985386974562511,
      "grad_norm": 61.65890884399414,
      "learning_rate": 5.911399548532732e-05,
      "loss": 10.6495,
      "step": 968
    },
    {
      "epoch": 0.6992603283420531,
      "grad_norm": 68.64945220947266,
      "learning_rate": 5.8972911963882615e-05,
      "loss": 11.7688,
      "step": 969
    },
    {
      "epoch": 0.699981959227855,
      "grad_norm": 72.91327667236328,
      "learning_rate": 5.883182844243792e-05,
      "loss": 11.7903,
      "step": 970
    },
    {
      "epoch": 0.7007035901136569,
      "grad_norm": 65.14591217041016,
      "learning_rate": 5.869074492099323e-05,
      "loss": 11.8464,
      "step": 971
    },
    {
      "epoch": 0.7014252209994588,
      "grad_norm": 68.2854232788086,
      "learning_rate": 5.854966139954854e-05,
      "loss": 11.0818,
      "step": 972
    },
    {
      "epoch": 0.7021468518852607,
      "grad_norm": 72.53256225585938,
      "learning_rate": 5.8408577878103836e-05,
      "loss": 10.8717,
      "step": 973
    },
    {
      "epoch": 0.7028684827710626,
      "grad_norm": 75.91275024414062,
      "learning_rate": 5.826749435665914e-05,
      "loss": 11.8223,
      "step": 974
    },
    {
      "epoch": 0.7035901136568645,
      "grad_norm": 70.7285385131836,
      "learning_rate": 5.812641083521445e-05,
      "loss": 10.284,
      "step": 975
    },
    {
      "epoch": 0.7043117445426664,
      "grad_norm": 80.5879898071289,
      "learning_rate": 5.798532731376975e-05,
      "loss": 10.8028,
      "step": 976
    },
    {
      "epoch": 0.7050333754284683,
      "grad_norm": 76.08708953857422,
      "learning_rate": 5.7844243792325056e-05,
      "loss": 11.3156,
      "step": 977
    },
    {
      "epoch": 0.7057550063142702,
      "grad_norm": 71.95672607421875,
      "learning_rate": 5.770316027088036e-05,
      "loss": 11.0868,
      "step": 978
    },
    {
      "epoch": 0.7064766372000721,
      "grad_norm": 72.3509750366211,
      "learning_rate": 5.7562076749435666e-05,
      "loss": 11.1239,
      "step": 979
    },
    {
      "epoch": 0.707198268085874,
      "grad_norm": 63.46514892578125,
      "learning_rate": 5.742099322799097e-05,
      "loss": 10.6875,
      "step": 980
    },
    {
      "epoch": 0.707919898971676,
      "grad_norm": 58.02237319946289,
      "learning_rate": 5.727990970654628e-05,
      "loss": 10.5609,
      "step": 981
    },
    {
      "epoch": 0.7086415298574779,
      "grad_norm": 65.79769134521484,
      "learning_rate": 5.713882618510158e-05,
      "loss": 10.1506,
      "step": 982
    },
    {
      "epoch": 0.7093631607432798,
      "grad_norm": 78.30500030517578,
      "learning_rate": 5.699774266365689e-05,
      "loss": 12.1837,
      "step": 983
    },
    {
      "epoch": 0.7100847916290818,
      "grad_norm": 75.76207733154297,
      "learning_rate": 5.685665914221219e-05,
      "loss": 11.2889,
      "step": 984
    },
    {
      "epoch": 0.7108064225148837,
      "grad_norm": 67.66419982910156,
      "learning_rate": 5.67155756207675e-05,
      "loss": 10.7983,
      "step": 985
    },
    {
      "epoch": 0.7115280534006856,
      "grad_norm": 61.97681427001953,
      "learning_rate": 5.6574492099322795e-05,
      "loss": 10.2241,
      "step": 986
    },
    {
      "epoch": 0.7122496842864875,
      "grad_norm": 73.8862533569336,
      "learning_rate": 5.643340857787811e-05,
      "loss": 10.6462,
      "step": 987
    },
    {
      "epoch": 0.7129713151722894,
      "grad_norm": 78.03631591796875,
      "learning_rate": 5.629232505643341e-05,
      "loss": 11.2789,
      "step": 988
    },
    {
      "epoch": 0.7136929460580913,
      "grad_norm": 80.12479400634766,
      "learning_rate": 5.615124153498871e-05,
      "loss": 10.5745,
      "step": 989
    },
    {
      "epoch": 0.7144145769438932,
      "grad_norm": 75.99987030029297,
      "learning_rate": 5.6010158013544016e-05,
      "loss": 11.6616,
      "step": 990
    },
    {
      "epoch": 0.7151362078296951,
      "grad_norm": 74.00592803955078,
      "learning_rate": 5.586907449209933e-05,
      "loss": 11.0537,
      "step": 991
    },
    {
      "epoch": 0.715857838715497,
      "grad_norm": 72.6694564819336,
      "learning_rate": 5.5727990970654626e-05,
      "loss": 10.8566,
      "step": 992
    },
    {
      "epoch": 0.7165794696012989,
      "grad_norm": 78.59869384765625,
      "learning_rate": 5.558690744920993e-05,
      "loss": 11.1434,
      "step": 993
    },
    {
      "epoch": 0.7173011004871008,
      "grad_norm": 93.92654418945312,
      "learning_rate": 5.5445823927765236e-05,
      "loss": 11.7837,
      "step": 994
    },
    {
      "epoch": 0.7180227313729027,
      "grad_norm": 82.27252197265625,
      "learning_rate": 5.530474040632055e-05,
      "loss": 11.575,
      "step": 995
    },
    {
      "epoch": 0.7187443622587046,
      "grad_norm": 68.46973419189453,
      "learning_rate": 5.5163656884875846e-05,
      "loss": 11.0837,
      "step": 996
    },
    {
      "epoch": 0.7194659931445065,
      "grad_norm": 72.74105072021484,
      "learning_rate": 5.502257336343115e-05,
      "loss": 10.7287,
      "step": 997
    },
    {
      "epoch": 0.7201876240303084,
      "grad_norm": 69.17615509033203,
      "learning_rate": 5.488148984198646e-05,
      "loss": 10.4475,
      "step": 998
    },
    {
      "epoch": 0.7209092549161105,
      "grad_norm": 65.7987060546875,
      "learning_rate": 5.474040632054176e-05,
      "loss": 10.5542,
      "step": 999
    },
    {
      "epoch": 0.7216308858019124,
      "grad_norm": 76.22305297851562,
      "learning_rate": 5.459932279909707e-05,
      "loss": 12.0457,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 1386,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 50,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
